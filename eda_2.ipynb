{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f14207f-be28-4283-9314-1e02e715db8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fundamentos de Ciência de Dados\n",
    "## PPGI/UFRJ 2024.2\n",
    "### Profs Sergio Serra e Jorge Zavaleta\n",
    "### Aluno Ubirajara S. Santos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21fc0eda-8216-4ae5-ba67-9a060afea2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prov\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ac38fd8-c6aa-4138-ae54-2a103a3f52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './dados/saidas'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38838849-a486-40fb-a8c1-35e945ee947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fontes de Dados\n",
    "data_sources = {\n",
    "     \"amostras_rochas_fluidos\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-amostras-de-rochas-e-fluidos/acervo-de-amostras/consolidacao-2023.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"setores_sirgas\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/assuntos/exploracao-e-producao-de-oleo-e-gas/estudos-geologicos-e-geofisicos/arquivos-classificacao-de-modelos-exploratorios/setores-sirgas.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"blocos_exploratorios\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ABLOCOS_EXPLORATORIOS_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"campos_producao\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ACAMPOS_PRODUCAO_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"reservas_nacionais_hc\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-estatisticos/arquivos-reservas-nacionais-de-petroleo-e-gas-natural/tabela-dados-bar-2023.xlsx\",\n",
    "        \"type\": \"xlsx\"},\n",
    "     \"pocos_perfurados_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/pocos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_levantamentos_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-levantamentos-geoquimicos.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"levantamento_sismico_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/sismicos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_pocos_2024\": {\n",
    "        \"url\": \"./dados/entradas/Tabela_pocos_2024_Novembro_24.csv\",\n",
    "        \"type\": \"csv\", \"sep\": \";\" ,\"encoding\": \"ANSI\"},\n",
    "     \"tabela_dados_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-dados-geoquimicos.csv\",\n",
    "        \"type\": \"csv\",\n",
    "        \"header\": 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2c3e5eb-413c-433b-ad53-5bfb214de691",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, datetime\n",
    "from prov.model import ProvDocument, Namespace\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "import plotly\n",
    "import graphviz\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import importlib.metadata\n",
    "\n",
    "def gerar_prov_outputs(doc_prov):\n",
    "    entity = \"EDA-PROV\"\n",
    "    output_file = f\"{entity}.png\"\n",
    "    try:\n",
    "        dot = prov_to_dot(doc_prov)\n",
    "        # Write to PNG\n",
    "        dot.write_png(output_file)\n",
    "        print(f\"Provenance graph generated successfully: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating provenance graph: {e}\")\n",
    "        # Save the DOT file for debugging\n",
    "        with open(\"debug.dot\", \"w\") as f:\n",
    "            f.write(dot.to_string())\n",
    "        print(\"Saved DOT file for debugging as 'debug.dot'.\")\n",
    "   \n",
    "    # Serialização do documento\n",
    "    doc_prov.serialize(entity + \".xml\", format='xml') \n",
    "    doc_prov.serialize(entity + \".ttl\", format='rdf', rdf_format='ttl',encoding=\"utf-8\")\n",
    "    print(\"Provenance serialized as XML and TTL.\")\n",
    "    \n",
    "\n",
    "def adding_namespaces(document_prov):\n",
    "    # Adiciona namespaces ao documento de proveniência.\n",
    "    document_prov.add_namespace('void', 'http://vocab.deri.ie/void#')\n",
    "    document_prov.add_namespace('ufrj', 'https://www.ufrj.br')\n",
    "    document_prov.add_namespace('schema', 'http://schema.org/')    # Dados estruturados Schema.org\n",
    "    document_prov.add_namespace('prov', 'http://www.w3.org/ns/prov#')     # Padrões PROV\n",
    "    document_prov.add_namespace('foaf', 'http://xmlns.com/foaf/0.1/')     # Agentes FOAF\n",
    "    document_prov.add_namespace('ufrj-ppgi', 'http://www.ufrj.br/ppgi/')  # UFRJ PPGI\n",
    "    document_prov.add_namespace('anp', 'https://www.gov.br/anp/pt-br')    # ANP - Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\n",
    "    document_prov.add_namespace('anp-dados_tec','https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/acervo-de-dados-tecnicos') # ANP - Acervo de Dados Técnicos \n",
    "    document_prov.add_namespace('petrobras','https://petrobras.com.br/')  # PETROBRAS\n",
    "    document_prov.add_namespace('br','http://br.org/ns/')    # Organizações Brasileiras\n",
    "    return document_prov\n",
    "\n",
    "\n",
    "def escape_label(text):\n",
    "    \"\"\"\n",
    "    Escapes special characters for Graphviz.\n",
    "    Encodes text to ASCII with XML character references.\n",
    "    \"\"\"\n",
    "    return text.encode(\"ascii\", \"xmlcharrefreplace\").decode()\n",
    "\n",
    "def get_installed_packages():\n",
    "    #Retorna os pacotes instalados no ambiente com suas versões.\n",
    "    try:\n",
    "        return {pkg.metadata['Name']: pkg.version for pkg in importlib.metadata.distributions()}\n",
    "    except ImportError:\n",
    "        import pkg_resources\n",
    "        return {dist.project_name: dist.version for dist in pkg_resources.working_set}\n",
    "\n",
    "def get_system_info():\n",
    "    #Retorna informações do sistema.\n",
    "    return {\n",
    "        \"OS\": platform.system(),\n",
    "        \"OS Version\": platform.version(),\n",
    "        \"OS Release\": platform.release(),\n",
    "        \"Python Version\": sys.version,\n",
    "        \"Python Executable\": sys.executable,\n",
    "        \"Current Working Directory\": str(Path.cwd()),}\n",
    "\n",
    "def get_used_packages():\n",
    "    \n",
    "    #Retorna um dicionário dos pacotes usados explicitamente no projeto e suas versões.\n",
    "    \n",
    "    packages = ['numpy', 'pandas', 'matplotlib', 'seaborn', 'rdflib', 'prov', 'graphviz', \n",
    "                'openpyxl', 'folium', 'pydot', 'requests', 'geopandas']  # Adicione ou remova pacotes usados\n",
    "    package_versions = {}\n",
    "    for package in packages:\n",
    "        try:\n",
    "            import importlib.metadata\n",
    "            version = importlib.metadata.version(package)\n",
    "            package_versions[package] = version\n",
    "        except ImportError:\n",
    "            print(f\"Pacote {package} não encontrado.\")\n",
    "    return package_versions\n",
    "\n",
    "def add_system_and_package_provenance(doc_prov):\n",
    "    #Adiciona informações do sistema e pacotes ao documento de proveniência\n",
    "    \n",
    "    # Criar atividade para rastrear informações de sistema e pacotes\n",
    "    activity_id = \"ufrj:track_system_and_packages\"\n",
    "    tracking_activity = doc_prov.activity(activity_id, datetime.datetime.now(), None, {\"prov:label\": escape_label(\"Track system and package provenance\")})\n",
    "\n",
    "    # Associar a atividade ao agente do notebook\n",
    "    if \"ag-eda-ipynb\" in dict_agents:\n",
    "        doc_prov.wasAssociatedWith(tracking_activity, dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Adicionar informações do sistema como entidades\n",
    "    system_info = get_system_info()\n",
    "    for key, value in system_info.items():\n",
    "        sanitized_key = key.replace(\" \", \"_\")  # Substituir espaços por _\n",
    "        sys_entity = doc_prov.entity(f\"schema:{sanitized_key}\", {\"prov:value\": value})\n",
    "        doc_prov.wasGeneratedBy(sys_entity, tracking_activity)\n",
    "\n",
    "    # Adicionar pacotes usados como entidades\n",
    "    used_packages = get_used_packages()\n",
    "    for pkg, version in used_packages.items():\n",
    "        pkg_entity = doc_prov.entity(f\"schema:{pkg}\", {\"prov:value\": version})\n",
    "        doc_prov.wasGeneratedBy(pkg_entity, tracking_activity)\n",
    "\n",
    "    return doc_prov\n",
    "\n",
    "def create_agents(document_prov):\n",
    "    \n",
    "    #creating agents\n",
    "    dagnts={} #cria dic\n",
    "    dagnts[\"ag-orgbr\"] = document_prov.agent(\"br:orgBr\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Oraganizações Brasileiras\")})\n",
    "    dagnts[\"ag-anp\"] = document_prov.agent(\"anp:ANP\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\")})\n",
    "    dagnts[\"ag-ufrj\"] = document_prov.agent(\"ufrj:UFRJ\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Universidade Federal do Rio de Janeiro\")})\n",
    "    dagnts[\"ag-ppgi\"] = document_prov.agent(\"ufrj:PPGI\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Programa de Pós Graduação em Informática\")})\n",
    "    dagnts[\"ag-greco\"] = document_prov.agent(\"ufrj:GRECO\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Grupo de Engenharia do Conhecimento\")})\n",
    "    dagnts[\"ag-author-ubirajara\"] = document_prov.agent(\"ufrj:Ubirajara\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Ubirajara Simões Santos\"), \"foaf:mbox\":\"ubirajas@hotmail.com\"})\n",
    "    dagnts[\"ag-author-sergio\"] = document_prov.agent(\"ufrj:Sergio\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Sergio Serra\"), \"foaf:mbox\":\"serra@ppgi.ufrj.br\"})\n",
    "    dagnts[\"ag-author-jorge\"] = document_prov.agent(\"ufrj:Jorge\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Jorge Zavaleta\"), \"foaf:mbox\":\"zavaleta@pet-si.ufrrj.br\"})\n",
    "    dagnts[\"ag-petrobras\"] = document_prov.agent(\"petrobras:Petrobras\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Petróleo Brasiliero S.A\")})\n",
    "    dagnts[\"ag-eda-ipynb\"] = document_prov.agent(\"ufrj:eda.ipynb\", {\"prov:type\":\"prov:SoftwareAgent\", \"foaf:name\":escape_label(\"eda.ipynb\"), \"prov:label\":escape_label(\"Notebook Python utilizado no trabalho\")})\n",
    "    return dagnts\n",
    "\n",
    "def associate_ufrj_agents(agents_dictionary):\n",
    "    agents_dictionary[\"ag-anp\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-petrobras\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ufrj\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ppgi\"].actedOnBehalfOf(agents_dictionary[\"ag-ufrj\"])\n",
    "    agents_dictionary[\"ag-greco\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-greco\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-petrobras\"])\n",
    "    agents_dictionary[\"ag-author-sergio\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-jorge\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-eda-ipynb\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    return agents_dictionary\n",
    "\n",
    " \n",
    "\n",
    "def create_initial_activities(document_prov):\n",
    "    #creating activities\n",
    "    #dataDownloadDatasets = datetime.datetime.strptime('29/11/24', '%d/%m/%y')\n",
    "    \n",
    "    dativs={}\n",
    "    dativs[\"act-create-ds\"] = document_prov.activity(\"anp:create-dataset\", None, None, {\"prov:label\":escape_label( \"Criação de datasets pela ANP\")})\n",
    "    #dativs[\"act-extract-ds\"] = document_prov.activity(\"ufrj:extract-dataset\")\n",
    "    dativs[\"act-create-ds-eda\"] = document_prov.activity(\"ufrj:create-ds-eda\", None, None, {\"prov:label\":escape_label( \"Criação de datasets para EDA\")})\n",
    "    #dativs[\"act-load-ds-eda\"] = document_prov.activity(\"ufrj:load-ds-eda\")\n",
    "    dativs[\"act-save-ipynb\"] = document_prov.activity(\"ufrj:save-ipynb\", None, None, {\"prov:label\":escape_label(\"Salvar notebook EDA\")})\n",
    "    return dativs\n",
    "\n",
    "def cria_entidades_iniciais(document_prov):\n",
    "    #creating entidades\n",
    "    dents={}\n",
    "    \n",
    "    # Entidade para amostras de rochas e fluidos\n",
    "    dents[\"ent-amostras-rochas-fluidos\"] = document_prov.entity('anp:amostras_rochas_fluidos', {'prov:label':escape_label('Dataset com amostras de rochas e fluidos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Consolidado 2023 de amostras disponíveis.'), 'prov:format': 'zip' })\n",
    "    # Entidade para setores SIRGAS\n",
    "    dents[\"ent-setores-sirgas\"] = document_prov.entity('anp:setores_sirgas', {'prov:label':escape_label('Setores SIRGAS'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Modelos exploratórios em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para blocos exploratórios\n",
    "    dents[\"ent-blocos-exploratorios\"] = document_prov.entity('anp:blocos_exploratorios', {'prov:label':escape_label( 'Blocos exploratórios'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Blocos exploratórios com dados geoespaciais.'), 'prov:format': 'zip'})\n",
    "    # Entidade para campos de produção\n",
    "    dents[\"ent-campos-producao\"] = document_prov.entity('anp:campos_producao', {'prov:label':escape_label( 'Campos de Produção'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados dos campos de produção em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para reservas nacionais de hidrocarbonetos\n",
    "    dents[\"ent-reservas-nacionais-hc\"] = document_prov.entity('anp:reservas_nacionais_hc',{'prov:label':escape_label( 'Reservas Nacionais de Hidrocarbonetos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela com dados sobre reservas nacionais.'), 'prov:format': 'xlsx'})\n",
    "    # Entidade para poços perfurados (2023)\n",
    "    dents[\"ent-pocos-perfurados-2023\"] = document_prov.entity('anp:pocos_perfurados_2023',{'prov:label':escape_label( 'Poços perfurados - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com os poços perfurados no ano de 2023.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de levantamentos geoquímicos\n",
    "    dents[\"ent-tabela-levantamentos-geoquimica\"] = document_prov.entity('anp:tabela_levantamentos_geoquimica',{'prov:label':escape_label( 'Tabela de levantamentos geoquímicos 20/04/2022'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados sobre levantamentos geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para tabela de dados geoquímicos\n",
    "    dents[\"ent-tabela-dados-geoquimica\"] = document_prov.entity('anp:tabela_dados_geoquimica',{'prov:label':escape_label( 'Tabela_dados_geoquimica 06/08/2021'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para levantamento sísmico (2023)\n",
    "    dents[\"ent-levantamento-sismico-2023\"] = document_prov.entity('anp:levantamento_sismico_2023', {'prov:label':escape_label( 'Levantamento Sísmico - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com dados de levantamentos sísmicos públicos.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de poços (2024)\n",
    "    dents[\"ent-tabela-pocos-2024\"] = document_prov.entity('anp:tabela_pocos_2024', {'prov:label':escape_label( 'Tabela de Poços - 2024'.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela CSV com dados atualizados de poços para 2024.'), 'prov:format': 'csv'})\n",
    "     # Entidade para ANP dados técnicos\n",
    "    dents[\"ent-anp-dados_tec-ds\"] = document_prov.entity('anp-dados_tec:dataset', {'prov:label':escape_label( 'ANP Dataset de Dados Técnicos'.encode(\"ascii\", \"xmlcharrefreplace\").decode()),'prov:type': 'void:Dataset','prov:description':escape_label('Dataset com dados técnicos disponíveis publicamente.'),'prov:format': 'csv'})\n",
    "    \n",
    "    # Entidade script python\n",
    "    dents[\"ent-eda-ipynb\"] = document_prov.entity('ufrj:eda-ipyn', {'prov:label':escape_label( \"Notebook Python utilizado no trabalho\".encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'foaf:Document'})\n",
    "    # Entidade Git\n",
    "    dents[\"ent-git-eda\"] = document_prov.entity('anp:github-eda', {'prov:label':escape_label( 'Repositorio Eba da ANP'.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'prov:Collection'})\n",
    "    return dents\n",
    "  \n",
    "\n",
    "def initial_association_agents_activities_entities(document_prov, dictionary_agents, dictionary_activities, dictionary_entities):\n",
    "    \n",
    "    #Associate activity of generate dataset with ANP agent\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds\"], dictionary_agents[\"ag-anp\"])\n",
    "    \n",
    "    #Associating datasets with activities of generate eba datasets\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-amostras-rochas-fluidos\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-setores-sirgas\"], dictionary_activities[\"act-create-ds\"])    \n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-blocos-exploratorios\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-campos-producao\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-reservas-nacionais-hc\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-pocos-perfurados-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-levantamentos-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-dados-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-levantamento-sismico-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-pocos-2024\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-anp-dados_tec-ds\"], dictionary_activities[\"act-create-ds\"])\n",
    "    \n",
    "    \n",
    "    #Associating ZIPs, XLSX, CSV com entities do dataset genérico\n",
    "    #document_prov.wasDerivedFrom(dictionary_entities[\"ent-dredfp2021-zip\"], dictionary_entities[\"ent-dredfp\"])  \n",
    "       \n",
    "    #associate activity of eda, com autor\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-author-ubirajara\"])   \n",
    "\n",
    "    #associate notebook agent with eba dataset\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-eda-ipynb\"])    \n",
    "             \n",
    "    #associate eda github repository with store datasets activity\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-git-eda\"], dictionary_activities[\"act-save-ipynb\"])\n",
    "\n",
    "def associate_save_activity(doc_prov, dict_agents, dict_entities):\n",
    "    \n",
    "    #Associa a atividade de salvar notebook ao agente e à entidade relevante.\n",
    "   \n",
    "    activity_id = \"ufrj:save-ipynb\"\n",
    "    save_activity = doc_prov.activity(activity_id, datetime.datetime.now(), None, {\"prov:label\": escape_label(\"Salvar notebook EDA\")})\n",
    "    # Associar ao agente eda.ipynb\n",
    "    if \"ag-eda-ipynb\" in dict_agents:\n",
    "        doc_prov.wasAssociatedWith(save_activity, dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar à entidade eda-ipynb\n",
    "    if \"ent-eda-ipynb\" in dict_entities:\n",
    "        doc_prov.wasGeneratedBy(dict_entities[\"ent-eda-ipynb\"], save_activity)\n",
    "\n",
    "    return doc_prov\n",
    "    \n",
    "    \n",
    "def initProvenance():\n",
    "    #Inicializa o documento de proveniência com namespaces, agentes, atividades e entidades.\n",
    "    \n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Criando um documento vazio de proveniência\n",
    "    doc_prov = ProvDocument()\n",
    "\n",
    "    # Criar namespaces no documento de proveniência\n",
    "    doc_prov = adding_namespaces(doc_prov)\n",
    "\n",
    "    # Criar agentes\n",
    "    dict_agents = create_agents(doc_prov)\n",
    "\n",
    "    # Criar atividades iniciais\n",
    "    dict_activities = create_initial_activities(doc_prov)\n",
    "\n",
    "    # Criar entidades iniciais\n",
    "    dict_entities = cria_entidades_iniciais(doc_prov)\n",
    "\n",
    "    # Criar hierarquia de agentes\n",
    "    dict_agents = associate_ufrj_agents(dict_agents)\n",
    "\n",
    "    # Associar agentes, atividades e entidades\n",
    "    initial_association_agents_activities_entities(doc_prov, dict_agents, dict_activities, dict_entities)\n",
    "\n",
    "    # Adicionar proveniência do sistema e pacotes\n",
    "    doc_prov = add_system_and_package_provenance(doc_prov)\n",
    "\n",
    "    # Associar atividade ufrj:save-ipynb\n",
    "    doc_prov = associate_save_activity(doc_prov, dict_agents, dict_entities)\n",
    "\n",
    "    return doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66ffb913-8d8d-47e9-b7a3-3c2e9f98ed5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import openpyxl\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import zipfile\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e031f122-f76a-4894-be1a-cf55b4fdf9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_zip_content(url, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Analisa o conteúdo de um arquivo ZIP e categoriza os tipos de arquivos encontrados.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL para o arquivo ZIP.\n",
    "        temp_dir (str): Diretório temporário para extração dos arquivos.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicionário com categorias de arquivos (csv, xlsx, shp, others).\n",
    "    \"\"\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    file_types = {\"csv\": [], \"xlsx\": [], \"xls\": [], \"shp\": [], \"others\": []}\n",
    "\n",
    "    try:\n",
    "        # Baixar o arquivo ZIP\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Erro ao baixar o arquivo: {response.status_code}\")\n",
    "\n",
    "        # Abrir o ZIP em memória e extrair\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zf:\n",
    "            zf.extractall(temp_dir)\n",
    "            extracted_files = zf.namelist()\n",
    "\n",
    "        # Categorizar os arquivos extraídos\n",
    "        for file in extracted_files:\n",
    "            file_path = os.path.join(temp_dir, file)\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_types[\"csv\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".xlsx\"):\n",
    "                file_types[\"xlsx\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".xls\"):\n",
    "                file_types[\"xls\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".shp\"):\n",
    "                file_types[\"shp\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            else:\n",
    "                file_types[\"others\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "\n",
    "        print(f\"Conteúdo do ZIP analisado: {file_types}\")\n",
    "        return file_types\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"O arquivo fornecido não é um ZIP válido: {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar ZIP: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02b413d1-798a-4d64-8949-4446900b33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_csv(file_path, encodings=[\"utf-8\", \"ISO-8859-1\", \"utf-8-sig\"]):\n",
    "    \"\"\"\n",
    "    Diagnostica problemas em arquivos CSV: delimitador e encoding.\n",
    "    \"\"\"\n",
    "    print(f\"Diagnóstico do arquivo: {file_path}\")\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"Tentando com encoding: {encoding}\")\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                sample = f.read(2048)  # Lê os primeiros 2048 caracteres\n",
    "            print(f\"Primeiros caracteres ({encoding}):\")\n",
    "            print(sample[:500])  # Mostra apenas os primeiros 500 caracteres\n",
    "            print(\"\\n--- Fim da Amostra ---\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo com encoding {encoding}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b3eb38b-6ac4-47aa-a22d-71cad5bfeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_csv(file_path, encodings=[\"utf-8\", \"ISO-8859-1\", \"utf-8-sig\"], max_lines=5):\n",
    "    \"\"\"\n",
    "    Diagnostica problemas em arquivos CSV: delimitador e encoding.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Caminho do arquivo CSV a ser diagnosticado.\n",
    "        encodings (list): Lista de encodings para tentar.\n",
    "        max_lines (int): Número máximo de linhas a serem lidas para análise.\n",
    "\n",
    "    Returns:\n",
    "        dict: Informações diagnosticadas incluindo encoding funcional e conteúdo inicial.\n",
    "    \"\"\"\n",
    "    print(f\"=== Diagnóstico do arquivo: {file_path} ===\")\n",
    "    diagnostics = {\"file_path\": file_path, \"encoding\": None, \"sample\": None, \"error\": None}\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"Tentando com encoding: {encoding}\")\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                sample = [f.readline().strip() for _ in range(max_lines)]\n",
    "\n",
    "            diagnostics[\"encoding\"] = encoding\n",
    "            diagnostics[\"sample\"] = sample\n",
    "            print(f\"Arquivo lido com sucesso usando encoding '{encoding}'.\")\n",
    "            print(f\"Amostra das primeiras {max_lines} linhas:\")\n",
    "            print(\"\\n\".join(sample))\n",
    "            print(\"\\n--- Fim da Amostra ---\\n\")\n",
    "            return diagnostics  # Retorna na primeira tentativa bem-sucedida\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo com encoding {encoding}: {e}\")\n",
    "            diagnostics[\"error\"] = str(e)\n",
    "\n",
    "    # Retorna informações de erro se nenhum encoding funcionar\n",
    "    print(\"Falha ao diagnosticar o arquivo com os encodings fornecidos.\")\n",
    "    return diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21a4ae8d-1f68-4c4d-94de-699433f36b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xlsx_files(xlsx_files):\n",
    "    \"\"\"\n",
    "    Carrega arquivos XLSX em DataFrames e retorna um dicionário de DataFrames.\n",
    "\n",
    "    Args:\n",
    "        xlsx_files (list): Lista de dicionários com informações dos arquivos XLSX. Cada dicionário contém:\n",
    "            - 'name': Nome do arquivo.\n",
    "            - 'path': Caminho para o arquivo.\n",
    "\n",
    "    Returns:\n",
    "        dict: Um dicionário onde as chaves são os nomes dos arquivos e os valores são os DataFrames.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Para rastrear proveniência\n",
    "\n",
    "    dataframes = {}\n",
    "    for xlsx_info in xlsx_files:\n",
    "        try:\n",
    "            # Início da atividade de carga\n",
    "            exec_start = datetime.datetime.now()\n",
    "            \n",
    "            # Carregar o arquivo XLSX\n",
    "            df = pd.read_excel(xlsx_info[\"path\"])\n",
    "            dataframes[xlsx_info[\"name\"]] = df\n",
    "\n",
    "            # Rastrear proveniência\n",
    "            exec_end = datetime.datetime.now()\n",
    "            activity_key = f\"act-load-xlsx-{xlsx_info['name']}\"\n",
    "            dict_activities[activity_key] = doc_prov.activity(\n",
    "                f\"ufrj:load_xlsx_{xlsx_info['name']}\",\n",
    "                exec_start,\n",
    "                exec_end,\n",
    "                {\"prov:label\": escape_label(f\"Carregamento do arquivo XLSX: {xlsx_info['name']}\")}\n",
    "            )\n",
    "            doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "            entity_key = f\"ent-xlsx-{xlsx_info['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:{xlsx_info['name']}\",\n",
    "                {\"prov:label\": escape_label(f\"Arquivo XLSX carregado: {xlsx_info['name']}\"),\n",
    "                 \"prov:type\": \"void:Dataset\",\n",
    "                 \"prov:generatedAtTime\": exec_end.isoformat()}\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "            print(f\"Arquivo XLSX carregado: {xlsx_info['name']} com shape {df.shape}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar XLSX {xlsx_info['name']}: {e}\")\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c252bc4-2c43-4d95-a5df-32c997dc4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "def process_shp_files(shp_files):\n",
    "    \"\"\"\n",
    "    Carrega arquivos Shapefiles em GeoDataFrames e retorna um dicionário de GeoDataFrames.\n",
    "\n",
    "    Args:\n",
    "        shp_files (list): Lista de dicionários com informações dos arquivos Shapefile.\n",
    "            Cada dicionário contém:\n",
    "                - 'name': Nome do arquivo.\n",
    "                - 'path': Caminho completo para o arquivo.\n",
    "\n",
    "    Returns:\n",
    "        dict: Um dicionário onde as chaves são os nomes dos arquivos e os valores são os GeoDataFrames.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Para rastrear proveniência\n",
    "\n",
    "    geodataframes = {}\n",
    "    for shp_info in shp_files:\n",
    "        try:\n",
    "            # Início da atividade de carga\n",
    "            exec_start = datetime.datetime.now()\n",
    "            \n",
    "            # Carregar o arquivo Shapefile em um GeoDataFrame\n",
    "            gdf = gpd.read_file(shp_info[\"path\"])\n",
    "            geodataframes[shp_info[\"name\"]] = gdf\n",
    "\n",
    "            # Rastrear proveniência\n",
    "            exec_end = datetime.datetime.now()\n",
    "            activity_key = f\"act-load-shp-{shp_info['name']}\"\n",
    "            dict_activities[activity_key] = doc_prov.activity(\n",
    "                f\"ufrj:load_shp_{shp_info['name']}\",\n",
    "                exec_start,\n",
    "                exec_end,\n",
    "                {\"prov:label\": escape_label(f\"Carregamento do Shapefile: {shp_info['name']}\")}\n",
    "            )\n",
    "            doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "            entity_key = f\"ent-shp-{shp_info['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:{shp_info['name']}\",\n",
    "                {\"prov:label\": escape_label(f\"Shapefile carregado: {shp_info['name']}\"),\n",
    "                 \"prov:type\": \"void:Dataset\",\n",
    "                 \"prov:generatedAtTime\": exec_end.isoformat()}\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "            print(f\"Arquivo Shapefile carregado: {shp_info['name']} com shape {gdf.shape}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar Shapefile {shp_info['name']}: {e}\")\n",
    "\n",
    "    return geodataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f878a87-e607-4e44-86d2-3d39946695ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_unknown_files(unknown_files):\n",
    "    \"\"\"\n",
    "    Exibe uma mensagem com arquivos de formatos não reconhecidos e registra a proveniência.\n",
    "\n",
    "    Args:\n",
    "        unknown_files (list): Lista de dicionários com informações sobre arquivos não reconhecidos.\n",
    "            Cada dicionário contém:\n",
    "                - 'name': Nome do arquivo.\n",
    "                - 'path': Caminho completo para o arquivo.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\n",
    "\n",
    "    if unknown_files:\n",
    "        print(f\"Arquivos não reconhecidos encontrados: {[file['name'] for file in unknown_files]}\")\n",
    "\n",
    "        # Registrar proveniência da análise\n",
    "        exec_start = datetime.datetime.now()\n",
    "        activity_key = \"act-analyze-unknown-files\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(\n",
    "            \"ufrj:analyze_unknown_files\",\n",
    "            exec_start,\n",
    "            None,\n",
    "            {\"prov:label\": escape_label(\"Análise de arquivos desconhecidos\")}\n",
    "        )\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        # Criar entidades para cada arquivo desconhecido\n",
    "        for file_info in unknown_files:\n",
    "            entity_key = f\"ent-unknown-{file_info['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:unknown_{file_info['name']}\",\n",
    "                {\"prov:label\": escape_label(f\"Arquivo desconhecido: {file_info['name']}\"),\n",
    "                 \"prov:type\": \"void:Dataset\",\n",
    "                 \"prov:location\": file_info[\"path\"]}\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "    else:\n",
    "        print(\"Nenhum arquivo não reconhecido foi encontrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c115892-4b4b-4a05-aaba-d2e0971b4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zip_source(source_name, data_sources, temp_dir=\"./dados/temp\"):\n",
    "    # Proveniência\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {\"dataframes\": {}, \"geodataframes\": {}}\n",
    "\n",
    "    exec_start = datetime.datetime.now()\n",
    "\n",
    "    # Analisar conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao analisar o conteúdo do ZIP '{source_name}'.\")\n",
    "        return {\"dataframes\": {}, \"geodataframes\": {}}\n",
    "\n",
    "    # Criar atividade de processamento de ZIP\n",
    "    activity_key = f\"act-process-zip-{source_name}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        f\"ufrj:process_zip_{source_name}\",\n",
    "        exec_start,\n",
    "        None,\n",
    "        {\"prov:label\": escape_label(f\"Processamento do ZIP: {source_name}\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar a entidade do ZIP original à atividade\n",
    "    zip_entity_key = f\"ent-{source_name}\"\n",
    "    if zip_entity_key in dict_entities:\n",
    "        doc_prov.used(dict_activities[activity_key], dict_entities[zip_entity_key])\n",
    "\n",
    "    # Inicializar coleções\n",
    "    dataframes = {}\n",
    "    geodataframes = {}\n",
    "\n",
    "    # Processar CSVs\n",
    "    for csv_file in file_types[\"csv\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file[\"path\"], sep=\",\", encoding=\"utf-8\")\n",
    "            dataframes[csv_file[\"name\"]] = df\n",
    "\n",
    "            # Criar entidade para cada CSV\n",
    "            entity_key = f\"ent-csv-{csv_file['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:csv_{csv_file['name']}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"CSV carregado: {csv_file['name']}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "            print(f\"CSV carregado: {csv_file['name']} com shape {df.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar CSV {csv_file['name']}: {e}\")\n",
    "\n",
    "    # Processar XLSXs\n",
    "    for xlsx_file in file_types[\"xlsx\"]:\n",
    "        try:\n",
    "            df = pd.read_excel(xlsx_file[\"path\"])\n",
    "            dataframes[xlsx_file[\"name\"]] = df\n",
    "\n",
    "            # Criar entidade para cada XLSX\n",
    "            entity_key = f\"ent-xlsx-{xlsx_file['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:xlsx_{xlsx_file['name']}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"XLSX carregado: {xlsx_file['name']}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "            print(f\"XLSX carregado: {xlsx_file['name']} com shape {df.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar XLSX {xlsx_file['name']}: {e}\")\n",
    "\n",
    "    # Processar Shapefiles\n",
    "    for shp_file in file_types[\"shp\"]:\n",
    "        try:\n",
    "            gdf = gpd.read_file(shp_file[\"path\"])\n",
    "            geodataframes[shp_file[\"name\"]] = gdf\n",
    "\n",
    "            # Criar entidade para cada Shapefile\n",
    "            entity_key = f\"ent-shp-{shp_file['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:shp_{shp_file['name']}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"Shapefile carregado: {shp_file['name']}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "            print(f\"Shapefile carregado: {shp_file['name']} com shape {gdf.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar Shapefile {shp_file['name']}: {e}\")\n",
    "\n",
    "    # Relatar arquivos não reconhecidos\n",
    "    report_unknown_files(file_types[\"others\"])\n",
    "\n",
    "    # Finalizar atividade\n",
    "    exec_end = datetime.datetime.now()\n",
    "    dict_activities[activity_key].add_attributes({\"prov:endTime\": exec_end.isoformat()})\n",
    "\n",
    "    return {\"dataframes\": dataframes, \"geodataframes\": geodataframes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b6c917a-b7cb-40f6-bd89-29517d9622a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import datetime\n",
    "\n",
    "def extract_and_load_shapefile_from_zip(source_name, data_sources, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Extrai e carrega Shapefiles de um ZIP com rastreamento de proveniência.\n",
    "    Args:\n",
    "        source_name (str): Nome da fonte de dados no `data_sources`.\n",
    "        data_sources (dict): Dicionário contendo informações das fontes de dados.\n",
    "        temp_dir (str): Diretório temporário para extração dos arquivos.\n",
    "    Returns:\n",
    "        dict: Dicionário contendo informações sobre os Shapefiles e seus GeoDataFrames.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\n",
    "\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {}\n",
    "\n",
    "    exec_start = datetime.datetime.now()\n",
    "    shapefile_info = {}\n",
    "\n",
    "    try:\n",
    "        # Baixar o arquivo ZIP\n",
    "        response = requests.get(source[\"url\"])\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Erro ao baixar o arquivo: {response.status_code}\")\n",
    "\n",
    "        # Abrir o ZIP em memória e extrair\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zf:\n",
    "            zf.extractall(temp_dir)\n",
    "            extracted_files = zf.namelist()\n",
    "\n",
    "        # Procurar arquivos .shp\n",
    "        shp_files = [file for file in extracted_files if file.endswith(\".shp\")]\n",
    "\n",
    "        if not shp_files:\n",
    "            raise ValueError(\"Nenhum arquivo Shapefile (.shp) encontrado no ZIP.\")\n",
    "\n",
    "        # Criar atividade de extração no documento de proveniência\n",
    "        activity_key = f\"act-extract-shp-{source_name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(\n",
    "            f\"ufrj:extract_shp_{source_name}\",\n",
    "            exec_start,\n",
    "            None,\n",
    "            {\"prov:label\": escape_label(f\"Extração de Shapefile do ZIP: {source_name}\")}\n",
    "        )\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        # Associar atividade ao ZIP original\n",
    "        zip_entity_key = f\"ent-{source_name}\"\n",
    "        if zip_entity_key in dict_entities:\n",
    "            doc_prov.used(dict_activities[activity_key], dict_entities[zip_entity_key])\n",
    "\n",
    "        # Processar todos os Shapefiles encontrados\n",
    "        for shp_file in shp_files:\n",
    "            shapefile_path = os.path.join(temp_dir, shp_file)\n",
    "            gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "            # Criar entidade para cada Shapefile\n",
    "            entity_key = f\"ent-shp-{source_name}-{os.path.basename(shp_file)}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:{source_name}_{os.path.basename(shp_file)}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"Shapefile extraído: {os.path.basename(shp_file)}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "            # Armazenar informações do Shapefile\n",
    "            shapefile_info[os.path.basename(shp_file)] = {\n",
    "                \"path\": shapefile_path,\n",
    "                \"crs\": gdf.crs,\n",
    "                \"columns\": gdf.columns.tolist(),\n",
    "                \"geometry_type\": gdf.geom_type.unique(),\n",
    "                \"gdf\": gdf\n",
    "            }\n",
    "            print(f\"Shapefile carregado: {shapefile_path} com shape {gdf.shape}\")\n",
    "\n",
    "        exec_end = datetime.datetime.now()\n",
    "        dict_activities[activity_key].add_attributes({\"prov:endTime\": exec_end.isoformat()})\n",
    "\n",
    "        return shapefile_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar Shapefile no ZIP '{source_name}': {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "716a3e91-04d8-4964-b8cd-a3cbff355109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_sources_with_zip(data_sources, source_name, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Atualiza `data_sources` com os arquivos extraídos de um ZIP, criando uma relação hierárquica.\n",
    "    \"\"\"\n",
    "    # Obter fonte original\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Analisar o conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao processar ZIP '{source_name}'.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Criar relações de proveniência\n",
    "    for file_type, files in file_types.items():\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        for i, file_info in enumerate(files):\n",
    "            child_name = f\"{source_name}_{file_type}_{i+1}\"\n",
    "            data_sources[child_name] = {\n",
    "                \"url\": file_info[\"path\"],  # Caminho local do arquivo extraído\n",
    "                \"type\": file_type,        # Tipo do arquivo (csv, xlsx, shp, etc.)\n",
    "                \"parent\": source_name,    # Referência ao ZIP pai\n",
    "                \"sep\": source.get(\"sep\", \";\") if file_type == \"csv\" else None,\n",
    "                \"encoding\": source.get(\"encoding\", \"utf-8\") if file_type == \"csv\" else None,\n",
    "                \"header\": source.get(\"header\", 0) if file_type == \"csv\" else None,\n",
    "                \"date_columns\": source.get(\"date_columns\", []) if file_type == \"csv\" else None,\n",
    "            }\n",
    "            print(f\"Adicionado {child_name} como filho de {source_name}.\")\n",
    "    \n",
    "    print(f\"Data sources atualizados com arquivos de '{source_name}'.\")\n",
    "    return data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b8adbc9-5994-4efd-978d-69f183713f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_sources_with_zip(data_sources, source_name, temp_dir=\"./temp\"):\n",
    "    \"\"\"\n",
    "    Atualiza `data_sources` com os arquivos extraídos de um ZIP, criando uma relação hierárquica.\n",
    "    E registra a proveniência entre o ZIP (pai) e os arquivos extraídos (filhos).\n",
    "    \"\"\"\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Analisar o conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao processar ZIP '{source_name}'.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Lista de filhos (arquivos extraídos)\n",
    "    children_sources = []\n",
    "\n",
    "    for file_type, files in file_types.items():\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        for i, file_info in enumerate(files):\n",
    "            child_name = f\"{source_name}_{file_type}_{i+1}\"\n",
    "            data_sources[child_name] = {\n",
    "                \"url\": file_info[\"path\"],  # Caminho local do arquivo extraído\n",
    "                \"type\": file_type,        # Tipo do arquivo (csv, xlsx, shp, etc.)\n",
    "                \"parent\": source_name,    # Referência ao ZIP pai\n",
    "                \"sep\": source.get(\"sep\", \";\") if file_type == \"csv\" else None,\n",
    "                \"encoding\": source.get(\"encoding\", \"utf-8\") if file_type == \"csv\" else None,\n",
    "                \"header\": source.get(\"header\", 0) if file_type == \"csv\" else None,\n",
    "                \"date_columns\": source.get(\"date_columns\", []) if file_type == \"csv\" else None,\n",
    "            }\n",
    "            children_sources.append(child_name)\n",
    "            print(f\"Adicionado {child_name} como filho de {source_name}.\")\n",
    "\n",
    "    # Registrar a proveniência para o ZIP e seus filhos\n",
    "    register_provenance_for_zip_and_children(source_name, children_sources)\n",
    "    \n",
    "    print(f\"Data sources atualizados com arquivos de '{source_name}'.\")\n",
    "    return data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0462131f-eba2-428c-a2d5-e56fe66c0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_provenance_for_zip_and_children(parent_source, children_sources, activity_prefix=\"process-zip\"):\n",
    "    \"\"\"\n",
    "    Registra a proveniência entre o ZIP pai e os arquivos extraídos (filhos).\n",
    "    A atividade é associada ao agente 'ag-eda-ipynb' para todos os filhos.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Criar atividade para o processamento do ZIP\n",
    "    exec_start = datetime.datetime.now()\n",
    "    activity_key = f\"{activity_prefix}-{parent_source}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        f\"ufrj:{activity_prefix}_{parent_source}\", exec_start, None,\n",
    "        {\"prov:label\": escape_label(f\"Processamento do ZIP {parent_source}\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Registrar cada filho como derivado do pai\n",
    "    for child_source in children_sources:\n",
    "        entity_key = f\"ent-{child_source}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{child_source}\", {\n",
    "            \"prov:label\": escape_label(f\"Arquivo derivado de {parent_source}\"),\n",
    "            \"prov:type\": \"void:Dataset\"\n",
    "        })\n",
    "        \n",
    "        # Relacionar pai e filho\n",
    "        doc_prov.wasDerivedFrom(\n",
    "            dict_entities[entity_key], dict_entities.get(f\"ent-{parent_source}\")\n",
    "        )\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "        # Adiciona a atividade de proveniência do arquivo como \"gerado por\" o agente IPYNB\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "    print(f\"Proveniência registrada para arquivos derivados de {parent_source}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df4ff600-8332-4879-b792-63dcc13dc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_source_csv(source_name, data_sources):\n",
    "    \"\"\"\n",
    "    Carrega dados com base no nome da fonte e na configuração em data_sources.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com os dados carregados, ou None se houver erro.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents,  dict_activities, dict_entities  # Declare global variables\n",
    "    #save execution start time\n",
    "    execStartTime = datetime.datetime.now()\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "   \n",
    "    if not source: #or source.get(\"type\") != \"csv\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é do tipo CSV.\")\n",
    "        return None\n",
    "\n",
    "    file_type = source.get(\"type\")\n",
    "    url = source.get(\"url\")\n",
    "    sep = source.get(\"sep\", \";\")  # Valor padrão para CSV\n",
    "    encoding = source.get(\"encoding\", \"utf-8\")  # Valor padrão para codificação\n",
    "    date_columns = source.get(\"date_columns\", [])  \n",
    "\n",
    "    try:\n",
    "        if file_type == \"csv\":\n",
    "             # Caso específico para tabela_pocos_2024\n",
    "            if source_name == \"tabela_pocos_2024\":\n",
    "                df = pd.read_csv(url, encoding=\"ANSI\", sep=sep)\n",
    "            # Caso específico para tabela_dados_geoquimica\n",
    "            elif source_name == \"tabela_dados_geoquimica\":\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, header=1)  # Cabeçalho na segunda linha\n",
    "            else:\n",
    "                #header_line = detect_csv_header(url, delimiter=sep)\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, parse_dates=date_columns)\n",
    "        elif file_type == \"xlsx\":\n",
    "            df = pd.read_excel(url)\n",
    "        else:\n",
    "            print(f\"Tipo de arquivo '{file_type}' não suportado.\")\n",
    "            return None\n",
    "        print(f\"Dados carregados com sucesso para '{source_name}'.\")\n",
    "    \n",
    "        # End execution time for provenance tracking\n",
    "        execEndTime = datetime.datetime.now()\n",
    "    \n",
    "        # Criar atividade com horário de término da execução\n",
    "        activity_key = f\"act-carga-{source_name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}\", execStartTime, execEndTime)\n",
    "    \n",
    "        # Associar a atividade ao agente\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "        \n",
    "        # Associar a atividade com os dados carregados\n",
    "        entity_key = f\"ent-{source_name}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{source_name}\", {\n",
    "            \"prov:label\": escape_label(f\"Dataset carregado: {source_name}\"),\"prov:type\": \"void:Dataset\", \"prov:generatedAtTime\": execEndTime.isoformat(),})\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "    \n",
    "        # Associar a atividade ufrj:carga à entidade correspondente criada pela ANP\n",
    "        anp_entity_key = f\"ent-{source_name.replace('_', '-')}\"  # Convert to ANP format (e.g., `tabela_pocos_2024` -> `ent-tabela-pocos-2024`)\n",
    "        if anp_entity_key in dict_entities:\n",
    "            # Establish the prov:used relationship\n",
    "            doc_prov.used(dict_activities[activity_key], dict_entities[anp_entity_key])\n",
    "        else:\n",
    "            print(f\"Warning: ANP entity '{anp_entity_key}' not found for activity '{activity_key}'.\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados de '{source_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b00d974a-e712-421e-9511-a00d2ff9563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zip_source_shp(source_name, data_sources, temp_dir=\"./dados/temp\"):\n",
    "    # Processa um ZIP contendo Shapefiles com rastreamento de proveniência.\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {\"geodataframes\": {}}\n",
    "\n",
    "    exec_start = datetime.datetime.now()\n",
    "\n",
    "    # Analisar conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao analisar o conteúdo do ZIP '{source_name}'.\")\n",
    "        return {\"geodataframes\": {}}\n",
    "\n",
    "    # Criar atividade de processamento de ZIP\n",
    "    activity_key = f\"act-process-zip-{source_name}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        f\"ufrj:carga_{source_name}\",\n",
    "        exec_start,\n",
    "        None,\n",
    "        {\"prov:label\": escape_label(f\"Processamento do ZIP: {source_name}\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar a entidade do ZIP original à atividade\n",
    "    zip_entity_key = f\"ent-{source_name}\"\n",
    "    if zip_entity_key in dict_entities:\n",
    "        doc_prov.used(dict_activities[activity_key], dict_entities[zip_entity_key])\n",
    "\n",
    "    # Criar a entidade derivada `ufrj:<source_name>`\n",
    "    derived_entity_key = f\"ent-{source_name}-ufrj\"\n",
    "    dict_entities[derived_entity_key] = doc_prov.entity(\n",
    "        f\"ufrj:{source_name}\",\n",
    "        {\n",
    "            \"prov:label\": escape_label(f\"Dataset processado: {source_name}\"),\n",
    "            \"prov:type\": \"void:Dataset\",\n",
    "            \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "        }\n",
    "    )\n",
    "    doc_prov.wasGeneratedBy(dict_entities[derived_entity_key], dict_activities[activity_key])\n",
    "\n",
    "    # Processar Shapefiles\n",
    "    geodataframes = {}\n",
    "    for shp_file in file_types[\"shp\"]:\n",
    "        try:\n",
    "            gdf = gpd.read_file(shp_file[\"path\"])\n",
    "            geodataframes[shp_file[\"name\"]] = gdf\n",
    "\n",
    "            # Criar entidade para cada Shapefile\n",
    "            entity_key = f\"ent-shp-{shp_file['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:shp_{shp_file['name']}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"Shapefile carregado: {shp_file['name']}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "            print(f\"Shapefile carregado: {shp_file['name']} com shape {gdf.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar Shapefile {shp_file['name']}: {e}\")\n",
    "\n",
    "    # Relatar arquivos não reconhecidos\n",
    "    report_unknown_files(file_types[\"others\"])\n",
    "\n",
    "    exec_end = datetime.datetime.now()\n",
    "    dict_activities[activity_key].add_attributes({\"prov:endTime\": exec_end.isoformat()})\n",
    "\n",
    "    return {\"geodataframes\": geodataframes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c50c5fc-afd6-4ddc-93a2-d1f30c3cb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframes(globals_dict, output_dir=\"./dados/saidas\"):\n",
    "    \"\"\"\n",
    "    Salva todos os DataFrames e GeoDataFrames do escopo global no diretório de saída.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_objects = []\n",
    "\n",
    "    for obj_name, obj in globals_dict.items():\n",
    "        try:\n",
    "            if isinstance(obj, pd.DataFrame):\n",
    "                file_path = os.path.join(output_dir, f\"{obj_name}.csv\")\n",
    "                obj.to_csv(file_path, index=False)\n",
    "                print(f\"DataFrame '{obj_name}' salvo em '{file_path}'.\")\n",
    "                saved_objects.append(obj_name)\n",
    "\n",
    "            elif isinstance(obj, gpd.GeoDataFrame):\n",
    "                file_path = os.path.join(output_dir, f\"{obj_name}.geojson\")\n",
    "                obj.to_file(file_path, driver=\"GeoJSON\")\n",
    "                print(f\"GeoDataFrame '{obj_name}' salvo em '{file_path}'.\")\n",
    "                saved_objects.append(obj_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar '{obj_name}': {e}\")\n",
    "\n",
    "    return saved_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e693405-7a3f-4184-8ef1-37e0bff11297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eda_dataset():\n",
    "    global df_sismica_2023_orig, df_pocos_orig, df_lev_geoq_2022_orig, df_geoq_2021_orig, df_reservas_orig, df_poco_2023_orig\n",
    "    global gdf_setores_sirgas, gdf_blocos_exploratorios, gdf_campos_producao\n",
    "\n",
    "    # Carregar datasets CSV/XLSX\n",
    "    df_sismica_2023_orig = load_data_from_source_csv(\"levantamento_sismico_2023\", data_sources)\n",
    "    df_pocos_orig = load_data_from_source_csv(\"tabela_pocos_2024\", data_sources)\n",
    "    df_lev_geoq_2022_orig = load_data_from_source_csv(\"tabela_levantamentos_geoquimica\", data_sources)\n",
    "    df_geoq_2021_orig = load_data_from_source_csv(\"tabela_dados_geoquimica\", data_sources)\n",
    "    df_reservas_orig = load_data_from_source_csv(\"reservas_nacionais_hc\", data_sources)\n",
    "    df_poco_2023_orig = load_data_from_source_csv(\"pocos_perfurados_2023\", data_sources)\n",
    "\n",
    "    # Processar shapefiles do ZIP\n",
    "    gdf_setores_sirgas = process_zip_source_shp(\"setores_sirgas\", data_sources).get(\"geodataframes\", {}).get(\"SETORES_TODOS_SIRGAS.shp\")\n",
    "    gdf_blocos_exploratorios = process_zip_source_shp(\"blocos_exploratorios\", data_sources).get(\"geodataframes\", {}).get(\"BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp\")\n",
    "    gdf_campos_producao = process_zip_source_shp(\"campos_producao\", data_sources).get(\"geodataframes\", {}).get(\"CAMPOS_PRODUCAO_SIRGASPolygon.shp\")\n",
    "\n",
    "    print(\"\\nDatasets prontos para análise:\")\n",
    "    datasets = {\n",
    "        \"df_sismica_2023_orig\": df_sismica_2023_orig,\n",
    "        \"df_pocos_orig\": df_pocos_orig,\n",
    "        \"df_lev_geoq_2022_orig\": df_lev_geoq_2022_orig,\n",
    "        \"df_geoq_2021_orig\": df_geoq_2021_orig,\n",
    "        \"df_reservas_orig\": df_reservas_orig,\n",
    "        \"df_poco_2023_orig\": df_poco_2023_orig,\n",
    "        \"gdf_setores_sirgas\": gdf_setores_sirgas,\n",
    "        \"gdf_blocos_exploratorios\": gdf_blocos_exploratorios,\n",
    "        \"gdf_campos_producao\": gdf_campos_producao,\n",
    "    }\n",
    "    for name, dataset in datasets.items():\n",
    "        if dataset is not None:\n",
    "            print(f\"- {name}: Shape {dataset.shape}\")\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "23400fd1-92ee-4264-99be-a71789622927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Inicializar proveniência\n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "    # Carregar datasets e GeoDataFrames\n",
    "    datasets = create_eda_dataset()\n",
    "\n",
    "    # Salvar os DataFrames e GeoDataFrames no diretório de saída\n",
    "    saved_objects = save_dataframes(globals())\n",
    "\n",
    "    # Listar objetos salvos\n",
    "    if saved_objects:\n",
    "        print(\"\\nObjetos salvos:\")\n",
    "        for obj_name in saved_objects:\n",
    "            print(f\"- {obj_name}\")\n",
    "    else:\n",
    "        print(\"\\nNenhum objeto foi salvo.\")\n",
    "\n",
    "    # Serializar e gerar saídas de proveniência\n",
    "    gerar_prov_outputs(doc_prov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a9788113-3467-422f-bc7d-6162031ceb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso para 'levantamento_sismico_2023'.\n",
      "Dados carregados com sucesso para 'tabela_pocos_2024'.\n",
      "Dados carregados com sucesso para 'tabela_levantamentos_geoquimica'.\n",
      "Dados carregados com sucesso para 'tabela_dados_geoquimica'.\n",
      "Dados carregados com sucesso para 'reservas_nacionais_hc'.\n",
      "Dados carregados com sucesso para 'pocos_perfurados_2023'.\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'SETORES_TODOS_SIRGAS.shp', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.shp'}], 'others': [{'name': 'SETORES_TODOS_SIRGAS.dbf', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.dbf'}, {'name': 'SETORES_TODOS_SIRGAS.prj', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.prj'}, {'name': 'SETORES_TODOS_SIRGAS.sbn', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.sbn'}, {'name': 'SETORES_TODOS_SIRGAS.sbx', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.sbx'}, {'name': 'SETORES_TODOS_SIRGAS.shx', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.shx'}, {'name': 'SETORES_TODOS_SIRGAS.xml', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.xml'}]}\n",
      "Shapefile carregado: SETORES_TODOS_SIRGAS.shp com shape (188, 4)\n",
      "Arquivos não reconhecidos encontrados: ['SETORES_TODOS_SIRGAS.dbf', 'SETORES_TODOS_SIRGAS.prj', 'SETORES_TODOS_SIRGAS.sbn', 'SETORES_TODOS_SIRGAS.sbx', 'SETORES_TODOS_SIRGAS.shx', 'SETORES_TODOS_SIRGAS.xml']\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp'}], 'others': [{'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.cst', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.cst'}, {'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.prj', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.prj'}, {'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.dbf', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.dbf'}, {'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.shx', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.shx'}, {'name': 'wfsrequest.txt', 'path': './dados/temp\\\\wfsrequest.txt'}]}\n",
      "Shapefile carregado: BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp com shape (424, 15)\n",
      "Arquivos não reconhecidos encontrados: ['BLOCOS_EXPLORATORIOS_SIRGASPolygon.cst', 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.prj', 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.dbf', 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.shx', 'wfsrequest.txt']\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.shp', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.shp'}], 'others': [{'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.cst', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.cst'}, {'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.prj', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.prj'}, {'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.dbf', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.dbf'}, {'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.shx', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.shx'}, {'name': 'wfsrequest.txt', 'path': './dados/temp\\\\wfsrequest.txt'}]}\n",
      "Shapefile carregado: CAMPOS_PRODUCAO_SIRGASPolygon.shp com shape (432, 18)\n",
      "Arquivos não reconhecidos encontrados: ['CAMPOS_PRODUCAO_SIRGASPolygon.cst', 'CAMPOS_PRODUCAO_SIRGASPolygon.prj', 'CAMPOS_PRODUCAO_SIRGASPolygon.dbf', 'CAMPOS_PRODUCAO_SIRGASPolygon.shx', 'wfsrequest.txt']\n",
      "\n",
      "Datasets prontos para análise:\n",
      "- df_sismica_2023_orig: Shape (52, 15)\n",
      "- df_pocos_orig: Shape (30827, 60)\n",
      "- df_lev_geoq_2022_orig: Shape (69, 8)\n",
      "- df_geoq_2021_orig: Shape (4665, 38)\n",
      "- df_reservas_orig: Shape (419, 10)\n",
      "- df_poco_2023_orig: Shape (106, 59)\n",
      "- gdf_setores_sirgas: Shape (188, 4)\n",
      "- gdf_blocos_exploratorios: Shape (424, 15)\n",
      "- gdf_campos_producao: Shape (432, 18)\n",
      "DataFrame 'df_sismica_2023_orig' salvo em './dados/saidas\\df_sismica_2023_orig.csv'.\n",
      "DataFrame 'df_pocos_orig' salvo em './dados/saidas\\df_pocos_orig.csv'.\n",
      "DataFrame 'df_lev_geoq_2022_orig' salvo em './dados/saidas\\df_lev_geoq_2022_orig.csv'.\n",
      "DataFrame 'df_geoq_2021_orig' salvo em './dados/saidas\\df_geoq_2021_orig.csv'.\n",
      "DataFrame 'df_reservas_orig' salvo em './dados/saidas\\df_reservas_orig.csv'.\n",
      "DataFrame 'df_poco_2023_orig' salvo em './dados/saidas\\df_poco_2023_orig.csv'.\n",
      "DataFrame 'gdf_setores_sirgas' salvo em './dados/saidas\\gdf_setores_sirgas.csv'.\n",
      "DataFrame 'gdf_blocos_exploratorios' salvo em './dados/saidas\\gdf_blocos_exploratorios.csv'.\n",
      "DataFrame 'gdf_campos_producao' salvo em './dados/saidas\\gdf_campos_producao.csv'.\n",
      "\n",
      "Objetos salvos:\n",
      "- df_sismica_2023_orig\n",
      "- df_pocos_orig\n",
      "- df_lev_geoq_2022_orig\n",
      "- df_geoq_2021_orig\n",
      "- df_reservas_orig\n",
      "- df_poco_2023_orig\n",
      "- gdf_setores_sirgas\n",
      "- gdf_blocos_exploratorios\n",
      "- gdf_campos_producao\n",
      "Provenance graph generated successfully: EDA-PROV.png\n",
      "Provenance serialized as XML and TTL.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89eeb93-8f51-4df8-8347-a62f513fc38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b5afe-5acb-413a-a317-b99a66bbf4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc35606-8115-42c0-bd99-940774e9690f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e8b1f-4e54-4ef1-ab20-c2e471f72123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6819c-aab9-46b9-a8fa-27eebe8a99c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "498bbbcd-bf7d-465c-97dc-0e853178826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sismica_2023_orig = load_data_from_source(\"levantamento_sismico_2023\", data_sources)\n",
    "#df_sismica_2023_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc74c138-fbb6-4b29-9aad-cc19bc0dd3a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"def inspect_dataframes(dataframes):\n",
    "    \n",
    "    #Inspeciona um dicionário de DataFrames, exibindo colunas, shape e estatísticas descritivas.\n",
    "   \n",
    "    for name, df in dataframes.items():\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(\"\\nHead:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nDescribe:\")\n",
    "        print(df.describe(include='all', datetime_is_numeric=True))\n",
    "\n",
    "    def clean_dataframe_columns(dataframes):\n",
    "  \n",
    "    #Realiza o saneamento e a renomeação de colunas em um conjunto de DataFrames.\n",
    "\n",
    "    cleaned_dataframes = {}\n",
    "    for name, df in dataframes.items():\n",
    "        # Renomear colunas removendo espaços e convertendo para letras minúsculas\n",
    "        df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "        # Registrar atividade de saneamento na proveniência\n",
    "        execStartTime = datetime.datetime.now()\n",
    "        activity_key = f\"act-saneamento-{name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:saneamento_{name}\", execStartTime, None, {\n",
    "            \"prov:label\": escape_label(f\"Saneamento de colunas: {name}\")\n",
    "        })\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        entity_key = f\"ent-saneado-{name}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{name}_saneado\", {\n",
    "            \"prov:label\": escape_label(f\"Dataset saneado: {name}\"),\n",
    "            \"prov:type\": \"void:Dataset\",\n",
    "        })\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "        cleaned_dataframes[name] = df\n",
    "    return cleaned_dataframes\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f14207f-be28-4283-9314-1e02e715db8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fundamentos de Ciência de Dados\n",
    "## PPGI/UFRJ 2024.2\n",
    "### Profs Sergio Serra e Jorge Zavaleta\n",
    "### Aluno Ubirajara S. Santos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21fc0eda-8216-4ae5-ba67-9a060afea2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prov\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import plotly\n",
    "import graphviz\n",
    "import unicodedata\n",
    "import platform\n",
    "import importlib.metadata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import openpyxl\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from prov.model import ProvDocument, Namespace\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from difflib import get_close_matches\n",
    "from folium import GeoJson\n",
    "from shapely.geometry import mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac38fd8-c6aa-4138-ae54-2a103a3f52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './dados/saidas'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38838849-a486-40fb-a8c1-35e945ee947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fontes de Dados\n",
    "data_sources = {\n",
    "     \"amostras_rochas_fluidos\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-amostras-de-rochas-e-fluidos/acervo-de-amostras/consolidacao-2023.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"setores_sirgas\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/assuntos/exploracao-e-producao-de-oleo-e-gas/estudos-geologicos-e-geofisicos/arquivos-classificacao-de-modelos-exploratorios/setores-sirgas.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"blocos_exploratorios\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ABLOCOS_EXPLORATORIOS_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"campos_producao\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ACAMPOS_PRODUCAO_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"reservas_nacionais_hc\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-estatisticos/arquivos-reservas-nacionais-de-petroleo-e-gas-natural/tabela-dados-bar-2023.xlsx\",\n",
    "        \"type\": \"xlsx\"},\n",
    "     \"pocos_perfurados_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/pocos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_levantamentos_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-levantamentos-geoquimicos.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"levantamento_sismico_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/sismicos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_pocos_2024\": {\n",
    "        \"url\": \"./dados/entradas/Tabela_pocos_2024_Novembro_24.csv\",\n",
    "        \"type\": \"csv\", \"sep\": \";\" ,\"encoding\": \"ANSI\"},\n",
    "     \"tabela_dados_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-dados-geoquimicos.csv\",\n",
    "        \"type\": \"csv\",\n",
    "        \"header\": 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c3e5eb-413c-433b-ad53-5bfb214de691",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FUNÇÂO EM USO\n",
    "\n",
    "def gerar_prov_outputs(doc_prov):\n",
    "    entity = \"EDA-PROV\"\n",
    "    output_file = f\"{entity}.png\"\n",
    "    try:\n",
    "        dot = prov_to_dot(doc_prov)\n",
    "        # Write to PNG\n",
    "        dot.write_png(output_file)\n",
    "        print(f\"Provenance graph generated successfully: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating provenance graph: {e}\")\n",
    "        # Save the DOT file for debugging\n",
    "        with open(\"debug.dot\", \"w\") as f:\n",
    "            f.write(dot.to_string())\n",
    "        print(\"Saved DOT file for debugging as 'debug.dot'.\")\n",
    "   \n",
    "    # Serialização do documento\n",
    "    doc_prov.serialize(entity + \".xml\", format='xml') \n",
    "    doc_prov.serialize(entity + \".ttl\", format='rdf', rdf_format='ttl',encoding=\"utf-8\")\n",
    "    print(\"Provenance serialized as XML and TTL.\")\n",
    "    \n",
    "\n",
    "def adding_namespaces(document_prov):\n",
    "    # Adiciona namespaces ao documento de proveniência.\n",
    "    document_prov.add_namespace('void', 'http://vocab.deri.ie/void#')\n",
    "    document_prov.add_namespace('ufrj', 'https://www.ufrj.br')\n",
    "    document_prov.add_namespace('schema', 'http://schema.org/')    # Dados estruturados Schema.org\n",
    "    document_prov.add_namespace('prov', 'http://www.w3.org/ns/prov#')     # Padrões PROV\n",
    "    document_prov.add_namespace('foaf', 'http://xmlns.com/foaf/0.1/')     # Agentes FOAF\n",
    "    document_prov.add_namespace('ufrj-ppgi', 'http://www.ufrj.br/ppgi/')  # UFRJ PPGI\n",
    "    document_prov.add_namespace('anp', 'https://www.gov.br/anp/pt-br')    # ANP - Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\n",
    "    document_prov.add_namespace('anp-dados_tec','https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/acervo-de-dados-tecnicos') # ANP - Acervo de Dados Técnicos \n",
    "    document_prov.add_namespace('petrobras','https://petrobras.com.br/')  # PETROBRAS\n",
    "    document_prov.add_namespace('br','http://br.org/ns/')    # Organizações Brasileiras\n",
    "    document_prov.add_namespace('git','https://github.com/ubirajara-s/eda') # Githut do repositório projeto EDA\n",
    "    return document_prov\n",
    "\n",
    "\n",
    "def escape_label(text):\n",
    "    \"\"\"\n",
    "    Escapes special characters for Graphviz.\n",
    "    Encodes text to ASCII with XML character references.\n",
    "    \"\"\"\n",
    "    return text.encode(\"ascii\", \"xmlcharrefreplace\").decode()\n",
    "\n",
    "def get_installed_packages():\n",
    "    #Retorna os pacotes instalados no ambiente com suas versões.\n",
    "    try:\n",
    "        return {pkg.metadata['Name']: pkg.version for pkg in importlib.metadata.distributions()}\n",
    "    except ImportError:\n",
    "        import pkg_resources\n",
    "        return {dist.project_name: dist.version for dist in pkg_resources.working_set}\n",
    "\n",
    "def get_system_info():\n",
    "    #Retorna informações do sistema.\n",
    "    return {\n",
    "        \"OS\": platform.system(),\n",
    "        \"OS Version\": platform.version(),\n",
    "        \"OS Release\": platform.release(),\n",
    "        \"Python Version\": sys.version,\n",
    "        \"Python Executable\": sys.executable,\n",
    "        \"Current Working Directory\": str(Path.cwd()),}\n",
    "\n",
    "def get_used_packages():\n",
    "    \n",
    "    #Retorna um dicionário dos pacotes usados explicitamente no projeto e suas versões.\n",
    "    \n",
    "    packages = ['numpy', 'pandas', 'matplotlib', 'seaborn', 'rdflib', 'prov', 'graphviz', \n",
    "                'openpyxl', 'folium', 'pydot', 'requests', 'geopandas', 'plotly','ipython']  # Adicione ou remova pacotes usados\n",
    "    package_versions = {}\n",
    "    for package in packages:\n",
    "        try:\n",
    "            import importlib.metadata\n",
    "            version = importlib.metadata.version(package)\n",
    "            package_versions[package] = version\n",
    "        except ImportError:\n",
    "            print(f\"Pacote {package} não encontrado.\")\n",
    "    return package_versions\n",
    "\n",
    "def add_system_and_package_provenance(doc_prov):\n",
    "    #Adiciona informações do sistema e pacotes ao documento de proveniência\n",
    "    \n",
    "    # Criar atividade para rastrear informações de sistema e pacotes\n",
    "    activity_id = \"ufrj:track_system_and_packages\"\n",
    "    tracking_activity = doc_prov.activity(activity_id, datetime.datetime.now(), None, {\"prov:label\": escape_label(\"Track system and package provenance\")})\n",
    "\n",
    "    # Associar a atividade ao agente do notebook\n",
    "    if \"ag-eda-ipynb\" in dict_agents:\n",
    "        doc_prov.wasAssociatedWith(tracking_activity, dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Adicionar informações do sistema como entidades\n",
    "    system_info = get_system_info()\n",
    "    for key, value in system_info.items():\n",
    "        sanitized_key = key.replace(\" \", \"_\")  # Substituir espaços por _\n",
    "        sys_entity = doc_prov.entity(f\"schema:{sanitized_key}\", {\"prov:value\": value})\n",
    "        doc_prov.wasGeneratedBy(sys_entity, tracking_activity)\n",
    "\n",
    "    # Adicionar pacotes usados como entidades\n",
    "    used_packages = get_used_packages()\n",
    "    for pkg, version in used_packages.items():\n",
    "        pkg_entity = doc_prov.entity(f\"schema:{pkg}\", {\"prov:value\": version})\n",
    "        doc_prov.wasGeneratedBy(pkg_entity, tracking_activity)\n",
    "\n",
    "    return doc_prov\n",
    "\n",
    "def create_agents(document_prov):\n",
    "    \n",
    "    #creating agents\n",
    "    dagnts={} #cria dic\n",
    "    dagnts[\"ag-orgbr\"] = document_prov.agent(\"br:orgBr\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Oraganizações Brasileiras\")})\n",
    "    dagnts[\"ag-anp\"] = document_prov.agent(\"anp:ANP\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\")})\n",
    "    dagnts[\"ag-ufrj\"] = document_prov.agent(\"ufrj:UFRJ\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Universidade Federal do Rio de Janeiro\")})\n",
    "    dagnts[\"ag-ppgi\"] = document_prov.agent(\"ufrj:PPGI\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Programa de Pós Graduação em Informática\")})\n",
    "    dagnts[\"ag-greco\"] = document_prov.agent(\"ufrj:GRECO\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Grupo de Engenharia do Conhecimento\")})\n",
    "    dagnts[\"ag-author-ubirajara\"] = document_prov.agent(\"ufrj:Ubirajara\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Ubirajara Simões Santos\"), \"foaf:mbox\":\"ubirajas@hotmail.com\"})\n",
    "    dagnts[\"ag-author-sergio\"] = document_prov.agent(\"ufrj:Sergio\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Sergio Serra\"), \"foaf:mbox\":\"serra@ppgi.ufrj.br\"})\n",
    "    dagnts[\"ag-author-jorge\"] = document_prov.agent(\"ufrj:Jorge\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Jorge Zavaleta\"), \"foaf:mbox\":\"zavaleta@pet-si.ufrrj.br\"})\n",
    "    dagnts[\"ag-petrobras\"] = document_prov.agent(\"petrobras:Petrobras\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Petróleo Brasiliero S.A\")})\n",
    "    dagnts[\"ag-eda-ipynb\"] = document_prov.agent(\"ufrj:eda.ipynb\", {\"prov:type\":\"prov:SoftwareAgent\", \"foaf:name\":escape_label(\"eda.ipynb\"), \"prov:label\":escape_label(\"Notebook Python utilizado no trabalho\")})\n",
    "    return dagnts\n",
    "\n",
    "def associate_ufrj_agents(agents_dictionary):\n",
    "    agents_dictionary[\"ag-anp\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-petrobras\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ufrj\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ppgi\"].actedOnBehalfOf(agents_dictionary[\"ag-ufrj\"])\n",
    "    agents_dictionary[\"ag-greco\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-greco\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-petrobras\"])\n",
    "    agents_dictionary[\"ag-author-sergio\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-jorge\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-eda-ipynb\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    return agents_dictionary\n",
    "\n",
    " \n",
    "\n",
    "def create_initial_activities(document_prov):\n",
    "    #creating activities\n",
    "    #dataDownloadDatasets = datetime.datetime.strptime('29/11/24', '%d/%m/%y')\n",
    "    \n",
    "    dativs={}\n",
    "    dativs[\"act-create-ds\"] = document_prov.activity(\"anp:create-dataset\", None, None, {\"prov:label\":escape_label( \"Criação de datasets pela ANP\")})\n",
    "    #dativs[\"act-extract-ds\"] = document_prov.activity(\"ufrj:extract-dataset\")\n",
    "    dativs[\"act-create-ds-eda\"] = document_prov.activity(\"ufrj:create-ds-eda\", None, None, {\"prov:label\":escape_label( \"Criação de datasets para EDA\")})\n",
    "    #dativs[\"act-load-ds-eda\"] = document_prov.activity(\"ufrj:load-ds-eda\")\n",
    "    dativs[\"act-save-ipynb\"] = document_prov.activity(\"ufrj:save-ipynb\", None, None, {\"prov:label\":escape_label(\"Salvar notebook EDA\")})\n",
    "    return dativs\n",
    "\n",
    "def cria_entidades_iniciais(document_prov):\n",
    "    global dict_entities  # Adicionar entidades ao dicionário global\n",
    "    #creating entidades\n",
    "    dents={}\n",
    "    \n",
    "    # Entidade para amostras de rochas e fluidos\n",
    "    dents[\"ent-amostras-rochas-fluidos\"] = document_prov.entity('anp:amostras_rochas_fluidos', {'prov:label':escape_label('Dataset com amostras de rochas e fluidos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Consolidado 2023 de amostras disponíveis.'), 'prov:format': 'zip' })\n",
    "    # Entidade para setores SIRGAS\n",
    "    dents[\"ent-setores-sirgas\"] = document_prov.entity('anp:setores_sirgas', {'prov:label':escape_label('Setores SIRGAS'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Modelos exploratórios em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para blocos exploratórios\n",
    "    dents[\"ent-blocos-exploratorios\"] = document_prov.entity('anp:blocos_exploratorios', {'prov:label':escape_label( 'Blocos exploratórios'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Blocos exploratórios com dados geoespaciais.'), 'prov:format': 'zip'})\n",
    "    # Entidade para campos de produção\n",
    "    dents[\"ent-campos-producao\"] = document_prov.entity('anp:campos_producao', {'prov:label':escape_label( 'Campos de Produção'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados dos campos de produção em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para reservas nacionais de hidrocarbonetos\n",
    "    dents[\"ent-reservas-nacionais-hc\"] = document_prov.entity('anp:reservas_nacionais_hc',{'prov:label':escape_label( 'Reservas Nacionais de Hidrocarbonetos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela com dados sobre reservas nacionais.'), 'prov:format': 'xlsx'})\n",
    "    # Entidade para poços perfurados (2023)\n",
    "    dents[\"ent-pocos-perfurados-2023\"] = document_prov.entity('anp:pocos_perfurados_2023',{'prov:label':escape_label( 'Poços perfurados - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com os poços perfurados no ano de 2023.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de levantamentos geoquímicos\n",
    "    dents[\"ent-tabela-levantamentos-geoquimica\"] = document_prov.entity('anp:tabela_levantamentos_geoquimica',{'prov:label':escape_label( 'Tabela de levantamentos geoquímicos 20/04/2022'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados sobre levantamentos geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para tabela de dados geoquímicos\n",
    "    dents[\"ent-tabela-dados-geoquimica\"] = document_prov.entity('anp:tabela_dados_geoquimica',{'prov:label':escape_label( 'Tabela_dados_geoquimica 06/08/2021'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para levantamento sísmico (2023)\n",
    "    dents[\"ent-levantamento-sismico-2023\"] = document_prov.entity('anp:levantamento_sismico_2023', {'prov:label':escape_label( 'Levantamento Sísmico - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com dados de levantamentos sísmicos públicos.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de poços (2024)\n",
    "    dents[\"ent-tabela-pocos-2024\"] = document_prov.entity('anp:tabela_pocos_2024', {'prov:label':escape_label( 'Tabela de Poços - 2024'.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela CSV com dados atualizados de poços para 2024.'), 'prov:format': 'csv'})\n",
    "     # Entidade para ANP dados técnicos\n",
    "    dents[\"ent-anp-dados_tec-ds\"] = document_prov.entity('anp-dados_tec:dataset', {'prov:label':escape_label( 'ANP Dataset de Dados Técnicos'.encode(\"ascii\", \"xmlcharrefreplace\").decode()),'prov:type': 'void:Dataset','prov:description':escape_label('Dataset com dados técnicos disponíveis publicamente.'),'prov:format': 'csv'})\n",
    "    \n",
    "    # Entidade script python\n",
    "    dents[\"ent-eda-ipynb\"] = document_prov.entity('ufrj:eda-ipyn', {'prov:label':escape_label( \"Notebook Python utilizado no trabalho\".encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'foaf:Document'})\n",
    "    # Entidade Git\n",
    "    dents[\"ent-git-eda\"] = document_prov.entity('git:github-eda', {'prov:label':escape_label( 'Repositorio GIT projeto EDA '.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'prov:Collection'})\n",
    "    return dents\n",
    "\n",
    "\n",
    "    dict_entities.update({\n",
    "        \"ent-setores-sirgas\": document_prov.entity(\n",
    "            'anp:setores_sirgas',{'prov:label': escape_label('Setores SIRGAS'),'prov:type': 'void:Dataset','prov:description': escape_label('Modelos exploratórios em formato SIRGAS.'),\n",
    "                'prov:format': 'zip'}),\n",
    "        \"ent-blocos-exploratorios\": document_prov.entity(\n",
    "            'anp:blocos_exploratorios',{'prov:label': escape_label('Blocos exploratórios'),'prov:type': 'void:Dataset','prov:description': escape_label('Blocos exploratórios com dados geoespaciais.'),\n",
    "                'prov:format': 'zip'}),\n",
    "        \"ent-campos-producao\": document_prov.entity(\n",
    "            'anp:campos_producao',{'prov:label': escape_label('Campos de Produção'),'prov:type': 'void:Dataset','prov:description': escape_label('Dados dos campos de produção em formato SIRGAS.'),\n",
    "                'prov:format': 'zip'}),\n",
    "    })\n",
    "\n",
    "def initial_association_agents_activities_entities(document_prov, dictionary_agents, dictionary_activities, dictionary_entities):\n",
    "    \n",
    "    #Associate activity of generate dataset with ANP agent\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds\"], dictionary_agents[\"ag-anp\"])\n",
    "    \n",
    "    #Associating datasets with activities of generate eba datasets\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-amostras-rochas-fluidos\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-setores-sirgas\"], dictionary_activities[\"act-create-ds\"])    \n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-blocos-exploratorios\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-campos-producao\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-reservas-nacionais-hc\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-pocos-perfurados-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-levantamentos-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-dados-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-levantamento-sismico-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-pocos-2024\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-anp-dados_tec-ds\"], dictionary_activities[\"act-create-ds\"])\n",
    "    \n",
    "    \n",
    "    #Associating ZIPs, XLSX, CSV com entities do dataset genérico\n",
    "    #document_prov.wasDerivedFrom(dictionary_entities[\"ent-dredfp2021-zip\"], dictionary_entities[\"ent-dredfp\"])  \n",
    "       \n",
    "    #associate activity of eda, com autor\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-author-ubirajara\"])   \n",
    "\n",
    "    #associate notebook agent with eba dataset\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-eda-ipynb\"])    \n",
    "             \n",
    "    #associate eda github repository with store datasets activity\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-git-eda\"], dictionary_activities[\"act-save-ipynb\"])\n",
    "\n",
    "def associate_save_activity(doc_prov, dict_agents, dict_entities):\n",
    "    \n",
    "    #Associa a atividade de salvar notebook ao agente e à entidade relevante.\n",
    "   \n",
    "    activity_id = \"ufrj:save-ipynb\"\n",
    "    save_activity = doc_prov.activity(activity_id, datetime.datetime.now(), None, {\"prov:label\": escape_label(\"Salvar notebook EDA\")})\n",
    "    # Associar ao agente eda.ipynb\n",
    "    if \"ag-eda-ipynb\" in dict_agents:\n",
    "        doc_prov.wasAssociatedWith(save_activity, dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar à entidade eda-ipynb\n",
    "    if \"ent-eda-ipynb\" in dict_entities:\n",
    "        doc_prov.wasGeneratedBy(dict_entities[\"ent-eda-ipynb\"], save_activity)\n",
    "\n",
    "    return doc_prov\n",
    "    \n",
    "    \n",
    "def initProvenance():\n",
    "    #Inicializa o documento de proveniência com namespaces, agentes, atividades e entidades.\n",
    "    \n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Criando um documento vazio de proveniência\n",
    "    doc_prov = ProvDocument()\n",
    "    dict_agents = {}\n",
    "    dict_activities = {}\n",
    "    dict_entities = {}\n",
    "\n",
    "    # Criar namespaces no documento de proveniência\n",
    "    doc_prov = adding_namespaces(doc_prov)\n",
    "\n",
    "    # Criar agentes\n",
    "    dict_agents = create_agents(doc_prov)\n",
    "\n",
    "    # Criar atividades iniciais\n",
    "    dict_activities = create_initial_activities(doc_prov)\n",
    "\n",
    "    # Criar entidades iniciais e atualizar o dicionário global\n",
    "    cria_entidades_iniciais(doc_prov)\n",
    "\n",
    "    # Criar entidades iniciais\n",
    "    dict_entities = cria_entidades_iniciais(doc_prov)\n",
    "\n",
    "    # Criar hierarquia de agentes\n",
    "    dict_agents = associate_ufrj_agents(dict_agents)\n",
    "\n",
    "    # Associar agentes, atividades e entidades\n",
    "    initial_association_agents_activities_entities(doc_prov, dict_agents, dict_activities, dict_entities)\n",
    "\n",
    "    # Adicionar proveniência do sistema e pacotes\n",
    "    doc_prov = add_system_and_package_provenance(doc_prov)\n",
    "\n",
    "    # Associar atividade ufrj:save-ipynb\n",
    "    doc_prov = associate_save_activity(doc_prov, dict_agents, dict_entities)\n",
    "\n",
    "    return doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e031f122-f76a-4894-be1a-cf55b4fdf9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÂO EM USO\n",
    "def analyze_zip_content(url, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Analisa o conteúdo de um arquivo ZIP e categoriza os tipos de arquivos encontrados.\n",
    "    Args:\n",
    "        url (str): URL para o arquivo ZIP.\n",
    "        temp_dir (str): Diretório temporário para extração dos arquivos.\n",
    "    Returns:\n",
    "        dict: Dicionário com categorias de arquivos (csv, xlsx, shp, others).\n",
    "    \"\"\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    file_types = {\"csv\": [], \"xlsx\": [], \"xls\": [], \"shp\": [], \"others\": []}\n",
    "\n",
    "    try:\n",
    "        # Baixar o arquivo ZIP\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Erro ao baixar o arquivo: {response.status_code}\")\n",
    "\n",
    "        # Abrir o ZIP em memória e extrair\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zf:\n",
    "            zf.extractall(temp_dir)\n",
    "            extracted_files = zf.namelist()\n",
    "\n",
    "        # Categorizar os arquivos extraídos\n",
    "        for file in extracted_files:\n",
    "            file_path = os.path.join(temp_dir, file)\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_types[\"csv\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".xlsx\"):\n",
    "                file_types[\"xlsx\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".xls\"):\n",
    "                file_types[\"xls\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".shp\"):\n",
    "                file_types[\"shp\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            else:\n",
    "                file_types[\"others\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "\n",
    "        print(f\"Conteúdo do ZIP analisado: {file_types}\")\n",
    "        return file_types\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"O arquivo fornecido não é um ZIP válido: {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar ZIP: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b413d1-798a-4d64-8949-4446900b33da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def diagnose_csv(file_path, encodings=[\"utf-8\", \"ISO-8859-1\", \"utf-8-sig\"]):\\n    \\n    #Diagnostica problemas em arquivos CSV: delimitador e encoding.\\n    \\n    print(f\"Diagnóstico do arquivo: {file_path}\")\\n    for encoding in encodings:\\n        try:\\n            print(f\"Tentando com encoding: {encoding}\")\\n            with open(file_path, \"r\", encoding=encoding) as f:\\n                sample = f.read(2048)  # Lê os primeiros 2048 caracteres\\n            print(f\"Primeiros caracteres ({encoding}):\")\\n            print(sample[:500])  # Mostra apenas os primeiros 500 caracteres\\n            print(\"\\n--- Fim da Amostra ---\\n\")\\n        except Exception as e:\\n            print(f\"Erro ao ler o arquivo com encoding {encoding}: {e}\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def diagnose_csv(file_path, encodings=[\"utf-8\", \"ISO-8859-1\", \"utf-8-sig\"]):\n",
    "    \n",
    "    #Diagnostica problemas em arquivos CSV: delimitador e encoding.\n",
    "    \n",
    "    print(f\"Diagnóstico do arquivo: {file_path}\")\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"Tentando com encoding: {encoding}\")\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                sample = f.read(2048)  # Lê os primeiros 2048 caracteres\n",
    "            print(f\"Primeiros caracteres ({encoding}):\")\n",
    "            print(sample[:500])  # Mostra apenas os primeiros 500 caracteres\n",
    "            print(\"\\n--- Fim da Amostra ---\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo com encoding {encoding}: {e}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f878a87-e607-4e44-86d2-3d39946695ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def report_unknown_files(unknown_files):\\n    \\n    #Exibe uma mensagem com arquivos de formatos não reconhecidos e registra a proveniência.\\n    #Args:\\n    #    unknown_files (list): Lista de dicionários com informações sobre arquivos não reconhecidos.\\n    #        Cada dicionário contém:\\n    #            - \\'name\\': Nome do arquivo.\\n    #            - \\'path\\': Caminho completo para o arquivo.\\n\\n    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\\n\\n    if unknown_files:\\n        print(f\"Arquivos não reconhecidos encontrados: {[file[\\'name\\'] for file in unknown_files]}\")\\n\\n        # Registrar proveniência da análise\\n        exec_start = datetime.datetime.now()\\n        activity_key = \"act-analyze-unknown-files\"\\n        dict_activities[activity_key] = doc_prov.activity(\\n            \"ufrj:analyze_unknown_files\",\\n            exec_start,\\n            None,\\n            {\"prov:label\": escape_label(\"Análise de arquivos desconhecidos\")}\\n        )\\n        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\\n\\n        # Criar entidades para cada arquivo desconhecido\\n        for file_info in unknown_files:\\n            entity_key = f\"ent-unknown-{file_info[\\'name\\']}\"\\n            dict_entities[entity_key] = doc_prov.entity(\\n                f\"ufrj:unknown_{file_info[\\'name\\']}\",\\n                {\"prov:label\": escape_label(f\"Arquivo desconhecido: {file_info[\\'name\\']}\"),\\n                 \"prov:type\": \"void:Dataset\",\\n                 \"prov:location\": file_info[\"path\"]}\\n            )\\n            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\\n\\n    else:\\n        print(\"Nenhum arquivo não reconhecido foi encontrado.\")    '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def report_unknown_files(unknown_files):\n",
    "    \n",
    "    #Exibe uma mensagem com arquivos de formatos não reconhecidos e registra a proveniência.\n",
    "    #Args:\n",
    "    #    unknown_files (list): Lista de dicionários com informações sobre arquivos não reconhecidos.\n",
    "    #        Cada dicionário contém:\n",
    "    #            - 'name': Nome do arquivo.\n",
    "    #            - 'path': Caminho completo para o arquivo.\n",
    "\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\n",
    "\n",
    "    if unknown_files:\n",
    "        print(f\"Arquivos não reconhecidos encontrados: {[file['name'] for file in unknown_files]}\")\n",
    "\n",
    "        # Registrar proveniência da análise\n",
    "        exec_start = datetime.datetime.now()\n",
    "        activity_key = \"act-analyze-unknown-files\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(\n",
    "            \"ufrj:analyze_unknown_files\",\n",
    "            exec_start,\n",
    "            None,\n",
    "            {\"prov:label\": escape_label(\"Análise de arquivos desconhecidos\")}\n",
    "        )\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        # Criar entidades para cada arquivo desconhecido\n",
    "        for file_info in unknown_files:\n",
    "            entity_key = f\"ent-unknown-{file_info['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:unknown_{file_info['name']}\",\n",
    "                {\"prov:label\": escape_label(f\"Arquivo desconhecido: {file_info['name']}\"),\n",
    "                 \"prov:type\": \"void:Dataset\",\n",
    "                 \"prov:location\": file_info[\"path\"]}\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "    else:\n",
    "        print(\"Nenhum arquivo não reconhecido foi encontrado.\")    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716a3e91-04d8-4964-b8cd-a3cbff355109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_sources_with_zip(data_sources, source_name, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Atualiza `data_sources` com os arquivos extraídos de um ZIP, criando uma relação hierárquica.\n",
    "    \"\"\"\n",
    "    # Obter fonte original\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Analisar o conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao processar ZIP '{source_name}'.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Criar relações de proveniência\n",
    "    for file_type, files in file_types.items():\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        for i, file_info in enumerate(files):\n",
    "            child_name = f\"{source_name}_{file_type}_{i+1}\"\n",
    "            data_sources[child_name] = {\n",
    "                \"url\": file_info[\"path\"],  # Caminho local do arquivo extraído\n",
    "                \"type\": file_type,        # Tipo do arquivo (csv, xlsx, shp, etc.)\n",
    "                \"parent\": source_name,    # Referência ao ZIP pai\n",
    "                \"sep\": source.get(\"sep\", \";\") if file_type == \"csv\" else None,\n",
    "                \"encoding\": source.get(\"encoding\", \"utf-8\") if file_type == \"csv\" else None,\n",
    "                \"header\": source.get(\"header\", 0) if file_type == \"csv\" else None,\n",
    "                \"date_columns\": source.get(\"date_columns\", []) if file_type == \"csv\" else None,\n",
    "            }\n",
    "            print(f\"Adicionado {child_name} como filho de {source_name}.\")\n",
    "    \n",
    "    print(f\"Data sources atualizados com arquivos de '{source_name}'.\")\n",
    "    return data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0462131f-eba2-428c-a2d5-e56fe66c0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_provenance_for_zip_and_children(parent_source, children_sources, activity_prefix=\"process-zip\"):\n",
    "    \"\"\"\n",
    "    Registra a proveniência entre o ZIP pai e os arquivos extraídos (filhos).\n",
    "    A atividade é associada ao agente 'ag-eda-ipynb' para todos os filhos.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Criar atividade para o processamento do ZIP\n",
    "    exec_start = datetime.datetime.now()\n",
    "    activity_key = f\"{activity_prefix}-{parent_source}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        f\"ufrj:{activity_prefix}_{parent_source}\", exec_start, None,\n",
    "        {\"prov:label\": escape_label(f\"Processamento do ZIP {parent_source}\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Registrar cada filho como derivado do pai\n",
    "    for child_source in children_sources:\n",
    "        entity_key = f\"ent-{child_source}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{child_source}\", {\n",
    "            \"prov:label\": escape_label(f\"Arquivo derivado de {parent_source}\"),\n",
    "            \"prov:type\": \"void:Dataset\"\n",
    "        })\n",
    "        \n",
    "        # Relacionar pai e filho\n",
    "        doc_prov.wasDerivedFrom(\n",
    "            dict_entities[entity_key], dict_entities.get(f\"ent-{parent_source}\")\n",
    "        )\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "        # Adiciona a atividade de proveniência do arquivo como \"gerado por\" o agente IPYNB\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "    print(f\"Proveniência registrada para arquivos derivados de {parent_source}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df4ff600-8332-4879-b792-63dcc13dc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÂO EM USO\n",
    "def load_data_from_source_csv(source_name, data_sources):\n",
    "    \"\"\"\n",
    "    Carrega dados com base no nome da fonte e na configuração em data_sources.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com os dados carregados, ou None se houver erro.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents,  dict_activities, dict_entities  # Declare global variables\n",
    "    #save execution start time\n",
    "    execStartTime = datetime.datetime.now()\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "   \n",
    "    if not source: #or source.get(\"type\") != \"csv\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é do tipo CSV.\")\n",
    "        return None\n",
    "\n",
    "    file_type = source.get(\"type\")\n",
    "    url = source.get(\"url\")\n",
    "    sep = source.get(\"sep\", \";\")  # Valor padrão para CSV\n",
    "    encoding = source.get(\"encoding\", \"utf-8\")  # Valor padrão para codificação\n",
    "    date_columns = source.get(\"date_columns\", [])  \n",
    "\n",
    "    try:\n",
    "        if file_type == \"csv\":\n",
    "             # Caso específico para tabela_pocos_2024\n",
    "            if source_name == \"tabela_pocos_2024\":\n",
    "                df = pd.read_csv(url, encoding=\"ANSI\", sep=sep)\n",
    "            # Caso específico para tabela_dados_geoquimica\n",
    "            elif source_name == \"tabela_dados_geoquimica\":\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, header=1)  # Cabeçalho na segunda linha\n",
    "            else:\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, parse_dates=date_columns)\n",
    "        elif file_type == \"xlsx\":\n",
    "            df = pd.read_excel(url)\n",
    "        else:\n",
    "            print(f\"Tipo de arquivo '{file_type}' não suportado.\")\n",
    "            return None\n",
    "        print(f\"Dados carregados com sucesso para '{source_name}'.\")\n",
    "    \n",
    "        # End execution time for provenance tracking\n",
    "        execEndTime = datetime.datetime.now()\n",
    "    \n",
    "        # Criar atividade com horário de término da execução\n",
    "        activity_key = f\"act-carga-{source_name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}\", execStartTime, execEndTime)\n",
    "    \n",
    "        # Associar a atividade ao agente\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "        \n",
    "        # Associar a atividade com os dados carregados\n",
    "        entity_key = f\"ent-{source_name}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{source_name}\", {\n",
    "            \"prov:label\": escape_label(f\"Dataset carregado: {source_name}\"),\"prov:type\": \"void:Dataset\", \"prov:generatedAtTime\": execEndTime.isoformat(),})\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "    \n",
    "        # Associar a atividade ufrj:carga à entidade correspondente criada pela ANP\n",
    "        anp_entity_key = f\"ent-{source_name.replace('_', '-')}\"  # Convert to ANP format (e.g., `tabela_pocos_2024` -> `ent-tabela-pocos-2024`)\n",
    "        if anp_entity_key in dict_entities:\n",
    "            # Establish the prov:used relationship\n",
    "            doc_prov.used(dict_activities[activity_key], dict_entities[anp_entity_key])\n",
    "        else:\n",
    "            print(f\"Warning: ANP entity '{anp_entity_key}' not found for activity '{activity_key}'.\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados de '{source_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8225242c-86be-4ec0-a1a8-e43a5740930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÂO EM USO\n",
    "def process_zip_source_shp(source_name, data_sources, temp_dir=\"./dados/temp\", output_dir=\"./dados/shp_salvos\"):\n",
    "    \"\"\"\n",
    "    Processa um ZIP contendo arquivos SHP, descompacta e carrega os arquivos como GeoDataFrames.\n",
    "    Também salva os arquivos SHP extraídos em um diretório de saída.\n",
    "\n",
    "    Args:\n",
    "        source_name (str): Nome da fonte de dados.\n",
    "        data_sources (dict): Dicionário com informações das fontes de dados.\n",
    "        temp_dir (str): Diretório temporário para descompactar os arquivos.\n",
    "        output_dir (str): Diretório para salvar os arquivos SHP extraídos.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicionário contendo os GeoDataFrames carregados.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    exec_start = datetime.datetime.now()\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {\"dataframes\": {}, \"geodataframes\": {}}\n",
    "\n",
    "    # Analisar conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao analisar o conteúdo do ZIP '{source_name}'.\")\n",
    "        return {\"dataframes\": {}, \"geodataframes\": {}}\n",
    "\n",
    "    # Criar atividade de processamento\n",
    "    activity_key = f\"act-carga-{source_name}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}\", exec_start)\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar atividade à entidade ANP\n",
    "    anp_entity_key = f\"ent-{source_name.replace('_', '-')}\"\n",
    "    if anp_entity_key in dict_entities:\n",
    "        doc_prov.used(dict_activities[activity_key], dict_entities[anp_entity_key])\n",
    "    else:\n",
    "        print(f\"Warning: ANP entity '{anp_entity_key}' not found for activity '{activity_key}'.\")\n",
    "\n",
    "    # Criar entidade derivada\n",
    "    derived_entity_key = f\"ent-processed-{source_name}\"\n",
    "    dict_entities[derived_entity_key] = doc_prov.entity(\n",
    "        f\"ufrj:{source_name}\",\n",
    "        {\n",
    "            \"prov:label\": escape_label(f\"Dataset processado: {source_name}\"),\n",
    "            \"prov:type\": \"void:Dataset\",\n",
    "            \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "        }\n",
    "    )\n",
    "    doc_prov.wasGeneratedBy(dict_entities[derived_entity_key], dict_activities[activity_key])\n",
    "\n",
    "    # Processar Shapefiles\n",
    "    geodataframes = {}\n",
    "    for shp_file in file_types[\"shp\"]:\n",
    "        try:\n",
    "            gdf = gpd.read_file(shp_file[\"path\"])\n",
    "            geodataframes[shp_file[\"name\"]] = gdf\n",
    "\n",
    "            # Salvar o shapefile no diretório de saída\n",
    "            shp_output_path = os.path.join(output_dir, shp_file[\"name\"])\n",
    "            gdf.to_file(shp_output_path)\n",
    "            print(f\"Shapefile '{shp_file['name']}' salvo em {shp_output_path}\")\n",
    "\n",
    "            # Atualizar proveniência para o arquivo salvo\n",
    "            shp_entity_key = f\"ent-shp-{shp_file['name']}\"\n",
    "            dict_entities[shp_entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:shp_{shp_file['name']}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"Shapefile carregado e salvo: {shp_file['name']}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "                    \"prov:location\": escape_label(shp_output_path)\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasDerivedFrom(dict_entities[shp_entity_key], dict_entities[derived_entity_key])\n",
    "            doc_prov.wasGeneratedBy(dict_entities[shp_entity_key], dict_activities[activity_key])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar ou salvar Shapefile {shp_file['name']}: {e}\")\n",
    "\n",
    "    return {\"dataframes\": {}, \"geodataframes\": geodataframes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb7daf3-3688-4e04-b58c-77ba1d5f58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÃO EM USO\n",
    "def process_csv_with_dtype_handling(file_path, header_row, initial_dtype=None):\n",
    "    \"\"\"\n",
    "    Carrega um arquivo CSV, detecta colunas com tipos mistos e força essas colunas para string.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Caminho para o arquivo CSV.\n",
    "        header_row (int): Linha onde o cabeçalho está localizado (baseado em zero).\n",
    "        initial_dtype (dict, optional): Tipos de dados esperados para as colunas. Padrão: None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame carregado com colunas mistas forçadas para string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar com os tipos iniciais fornecidos\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            sep=\";\",\n",
    "            skiprows=header_row,\n",
    "            dtype=initial_dtype,\n",
    "            encoding=\"utf-8\",\n",
    "            on_bad_lines=\"skip\",\n",
    "            low_memory=False  # Evita warnings de chunks\n",
    "        )\n",
    "        print(f\"Arquivo {file_path} carregado com sucesso. Shape: {df.shape}\")\n",
    "    except DtypeWarning as warning:\n",
    "        print(f\"Detectados tipos mistos ao carregar {file_path}. Tentando corrigir...\")\n",
    "        mixed_columns = []\n",
    "        try:\n",
    "            # Identificar colunas com tipos mistos ao carregar sem forçar tipos\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=\";\",\n",
    "                skiprows=header_row,\n",
    "                encoding=\"utf-8\",\n",
    "                on_bad_lines=\"skip\",\n",
    "                low_memory=False\n",
    "            )\n",
    "            for col in df.columns:\n",
    "                if df[col].apply(type).nunique() > 1:  # Se mais de um tipo presente\n",
    "                    mixed_columns.append(col)\n",
    "            print(f\"Colunas com tipos mistos detectadas: {mixed_columns}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao identificar colunas com tipos mistos: {e}\")\n",
    "            return pd.DataFrame()  # Retorna DataFrame vazio em caso de erro\n",
    "\n",
    "        # Forçar colunas detectadas como strings\n",
    "        force_dtype = {col: 'string' for col in mixed_columns}\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=\";\",\n",
    "                skiprows=header_row,\n",
    "                dtype=force_dtype,\n",
    "                encoding=\"utf-8\",\n",
    "                on_bad_lines=\"skip\",\n",
    "                low_memory=False\n",
    "            )\n",
    "            print(f\"Colunas mistas corrigidas e forçadas para string no arquivo {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar {file_path} com colunas forçadas para string: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o arquivo {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c07275ee-e4fd-49f2-b7e1-349408b288ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zip_source_csv_with_headers(source_name, data_sources, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Processa um ZIP contendo CSVs, descompacta e carrega os CSVs como DataFrames.\n",
    "    Suporta arquivos com cabeçalhos em linhas diferentes e corrige tipos mistos.\n",
    "    Args:\n",
    "        source_name (str): Nome da fonte de dados no dicionário `data_sources`.\n",
    "        data_sources (dict): Dicionário contendo informações das fontes de dados.\n",
    "        temp_dir (str): Diretório temporário para extração dos arquivos.\n",
    "    Returns:\n",
    "        dict: Dicionário com DataFrames carregados.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {}\n",
    "\n",
    "    exec_start = datetime.datetime.now()\n",
    "\n",
    "    # Analisar conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types or not file_types[\"csv\"]:\n",
    "        print(f\"Erro: Nenhum CSV encontrado no ZIP '{source_name}'.\")\n",
    "        return {}\n",
    "\n",
    "    # Criar atividade de processamento do ZIP\n",
    "    activity_key = f\"act-carga-{source_name}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}\", exec_start)\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar a atividade ao ZIP original (entidade ANP)\n",
    "    anp_entity_key = f\"ent-{source_name.replace('_', '-')}\"\n",
    "    if anp_entity_key in dict_entities:\n",
    "        doc_prov.used(dict_activities[activity_key], dict_entities[anp_entity_key])\n",
    "    else:\n",
    "        print(f\"Warning: ANP entity '{anp_entity_key}' not found for activity '{activity_key}'.\")\n",
    "\n",
    "    # Criar entidade derivada para o ZIP processado\n",
    "    derived_entity_key = f\"ent-processed-{source_name}\"\n",
    "    dict_entities[derived_entity_key] = doc_prov.entity(\n",
    "        f\"ufrj:{source_name}\",\n",
    "        {\n",
    "            \"prov:label\": escape_label(f\"Dataset processado: {source_name}\"),\n",
    "            \"prov:type\": \"void:Dataset\",\n",
    "            \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "        }\n",
    "    )\n",
    "    doc_prov.wasGeneratedBy(dict_entities[derived_entity_key], dict_activities[activity_key])\n",
    "\n",
    "    # Configuração de cabeçalhos conhecidos e dtypes\n",
    "    header_config = {\n",
    "        \"consolidacao-daa-2023-calha.csv\": 3,  # Cabeçalho na linha 4\n",
    "        \"consolidacao-daa-2023-fluidos.csv\": 2,  # Cabeçalho na linha 3\n",
    "        \"consolidacao-daa-2023-laminas.csv\": 3,  # Cabeçalho na linha 4\n",
    "        \"consolidacao-daa-2023-laterais.csv\": 3,\n",
    "        \"consolidacao-daa-2023-plugues.csv\": 3,\n",
    "        \"consolidacao-daa-2023-testemunhos.csv\": 3\n",
    "    }\n",
    "    dtype_config = {\n",
    "        \"consolidacao-daa-2023-calha.csv\": {\n",
    "            'Código do poço (API)': 'string',\n",
    "            'Identificador da caixa': 'string',\n",
    "            'Total de caixas': 'string',\n",
    "        },\n",
    "        # Outros `dtypes` podem ser adicionados aqui\n",
    "    }\n",
    "\n",
    "    # Processar os arquivos CSV extraídos\n",
    "    dataframes = {}\n",
    "    for csv_info in file_types[\"csv\"]:\n",
    "        try:\n",
    "            file_name = csv_info[\"name\"]\n",
    "            header_row = header_config.get(file_name, 0)  # Padrão é a primeira linha\n",
    "            dtype = dtype_config.get(file_name, None)\n",
    "\n",
    "            # Carregar CSV com tentativa de detecção de tipos mistos\n",
    "            df = pd.read_csv(\n",
    "                csv_info[\"path\"],\n",
    "                sep=\";\",\n",
    "                skiprows=header_row,\n",
    "                dtype=dtype,\n",
    "                encoding=\"utf-8\",\n",
    "                on_bad_lines=\"skip\",\n",
    "                low_memory=False  # Processar grandes arquivos\n",
    "            )\n",
    "\n",
    "            # Detectar colunas com tipos mistos e forçar para string\n",
    "            mixed_columns = []\n",
    "            for col in df.columns:\n",
    "                if df[col].apply(type).nunique() > 1:  # Mais de um tipo detectado\n",
    "                    mixed_columns.append(col)\n",
    "            if mixed_columns:\n",
    "                print(f\"Colunas com tipos mistos detectadas no arquivo '{file_name}': {mixed_columns}\")\n",
    "                dtype_update = {col: 'string' for col in mixed_columns}\n",
    "                df = pd.read_csv(\n",
    "                    csv_info[\"path\"],\n",
    "                    sep=\";\",\n",
    "                    skiprows=header_row,\n",
    "                    dtype={**dtype, **dtype_update} if dtype else dtype_update,\n",
    "                    encoding=\"utf-8\",\n",
    "                    on_bad_lines=\"skip\",\n",
    "                    low_memory=False\n",
    "                )\n",
    "                print(f\"Tipos mistos corrigidos no arquivo '{file_name}'.\")\n",
    "\n",
    "            # Validar cabeçalho\n",
    "            if df.columns[0].startswith(\"Todas as Informações\") or df.columns[0].startswith(\";;;;;;;;\"):\n",
    "                print(f\"Ajustando cabeçalho para '{file_name}'...\")\n",
    "                df = pd.read_csv(\n",
    "                    csv_info[\"path\"],\n",
    "                    sep=\";\",\n",
    "                    skiprows=header_row + 1,  # Ajustar o cabeçalho\n",
    "                    dtype=dtype,\n",
    "                    encoding=\"utf-8\",\n",
    "                    on_bad_lines=\"skip\",\n",
    "                    low_memory=False\n",
    "                )\n",
    "\n",
    "            dataframes[f\"df_{file_name}\"] = df\n",
    "\n",
    "            # Criar entidade para o CSV\n",
    "            csv_entity_key = f\"ent-csv-{file_name}\"\n",
    "            dict_entities[csv_entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:csv_{file_name}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"CSV carregado: {file_name}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[csv_entity_key], dict_activities[activity_key])\n",
    "            doc_prov.wasDerivedFrom(dict_entities[csv_entity_key], dict_entities[derived_entity_key])\n",
    "            print(f\"CSV carregado: {file_name} com shape {df.shape}\")\n",
    "        except Exception as e:\n",
    "            # Logar o erro e registrar a entidade como \"não processada\"\n",
    "            print(f\"Erro ao carregar CSV {file_name}: {e}\")\n",
    "            error_entity_key = f\"ent-error-{file_name}\"\n",
    "            dict_entities[error_entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:error_{file_name}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"Erro ao carregar CSV: {file_name}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:description\": escape_label(str(e)),\n",
    "                    \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[error_entity_key], dict_activities[activity_key])\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0c2861d-5b7d-4ecc-82be-126430426f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO EM USO\n",
    "def create_eda_dataset():\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "    \n",
    "    # Inicializar o dicionário de datasets\n",
    "    datasets = {}\n",
    "\n",
    "    # Carregar datasets CSV/XLSX diretamente da URL\n",
    "    datasets[\"df_sismica_2023_orig\"] = load_data_from_source_csv(\"levantamento_sismico_2023\", data_sources)\n",
    "    datasets[\"df_pocos_orig\"] = load_data_from_source_csv(\"tabela_pocos_2024\", data_sources)\n",
    "    datasets[\"df_lev_geoq_2022_orig\"] = load_data_from_source_csv(\"tabela_levantamentos_geoquimica\", data_sources)\n",
    "    datasets[\"df_geoq_2021_orig\"] = load_data_from_source_csv(\"tabela_dados_geoquimica\", data_sources)\n",
    "    datasets[\"df_reservas_orig\"] = load_data_from_source_csv(\"reservas_nacionais_hc\", data_sources)\n",
    "    datasets[\"df_poco_2023_orig\"] = load_data_from_source_csv(\"pocos_perfurados_2023\", data_sources)\n",
    "\n",
    "    # Carregar shapefiles do ZIP\n",
    "    gdf_setores_sirgas = process_zip_source_shp(\"setores_sirgas\", data_sources).get(\"geodataframes\", {}).get(\"SETORES_TODOS_SIRGAS.shp\")\n",
    "    datasets[\"gdf_setores_sirgas\"] = gdf_setores_sirgas\n",
    "\n",
    "    gdf_blocos_exploratorios = process_zip_source_shp(\"blocos_exploratorios\", data_sources).get(\"geodataframes\", {}).get(\"BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp\")\n",
    "    datasets[\"gdf_blocos_exploratorios\"] = gdf_blocos_exploratorios\n",
    "\n",
    "    gdf_campos_producao = process_zip_source_shp(\"campos_producao\", data_sources).get(\"geodataframes\", {}).get(\"CAMPOS_PRODUCAO_SIRGASPolygon.shp\")\n",
    "    datasets[\"gdf_campos_producao\"] = gdf_campos_producao\n",
    "\n",
    "    # Carregar CSVs do ZIP 'amostras_rochas_fluidos' com tratamento de cabeçalhos\n",
    "    dfs_amostras_rochas_fluidos = process_zip_source_csv_with_headers(\"amostras_rochas_fluidos\", data_sources)\n",
    "    datasets.update(dfs_amostras_rochas_fluidos)\n",
    "\n",
    "    print(\"\\nDatasets prontos para análise:\")\n",
    "    for name, dataset in datasets.items():\n",
    "        if dataset is not None:\n",
    "            print(f\"- {name}: Shape {dataset.shape}\")\n",
    "\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "647fa83e-b154-4319-9a35-ece43d76aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframes_with_provenance(datasets, output_dir=\"./dados/saidas\"):\n",
    "    \"\"\"\n",
    "    Salva os DataFrames e GeoDataFrames no diretório especificado e registra a proveniência.\n",
    "    Args:\n",
    "        datasets (dict): Dicionário contendo os DataFrames e GeoDataFrames.\n",
    "        output_dir (str): Diretório de saída.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_activities, dict_entities, dict_agents\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_objects = []\n",
    "\n",
    "    # Mapeamento explícito de nomes\n",
    "    entity_mapping = {\n",
    "        \"df_sismica_2023_orig\": \"levantamento_sismico_2023\",\n",
    "        \"df_pocos_orig\": \"tabela_pocos_2024\",\n",
    "        \"df_lev_geoq_2022_orig\": \"tabela_levantamentos_geoquimica\",\n",
    "        \"df_geoq_2021_orig\": \"tabela_dados_geoquimica\",\n",
    "        \"df_reservas_orig\": \"reservas_nacionais_hc\",\n",
    "        \"df_poco_2023_orig\": \"pocos_perfurados_2023\",\n",
    "        \"gdf_setores_sirgas\": \"setores_sirgas\",\n",
    "        \"gdf_blocos_exploratorios\": \"blocos_exploratorios\",\n",
    "        \"gdf_campos_producao\": \"campos_producao\",\n",
    "        # Mapear datasets derivados\n",
    "        \"df_consolidacao-daa-2023-calha.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-fluidos.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-laminas.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-laterais.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-plugues.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-testemunhos.csv\": \"amostras_rochas_fluidos\",\n",
    "    }\n",
    "\n",
    "    # Garantir que todas as entidades originais estão registradas\n",
    "    for original_entity in set(entity_mapping.values()):\n",
    "        original_entity_key = f\"ent-{original_entity}\"\n",
    "        if original_entity_key not in dict_entities:\n",
    "            dict_entities[original_entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:{original_entity}\",\n",
    "                {\n",
    "                    \"prov:label\": f\"Entidade original: {original_entity}\",\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    for name, obj in datasets.items():\n",
    "        if obj is None:\n",
    "            continue\n",
    "\n",
    "        exec_start = datetime.datetime.now()\n",
    "        try:\n",
    "            if isinstance(obj, pd.DataFrame):\n",
    "                output_file = os.path.join(output_dir, f\"{name}.csv\")\n",
    "                obj.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "                file_type = \"csv\"\n",
    "            elif isinstance(obj, gpd.GeoDataFrame):\n",
    "                output_file = os.path.join(output_dir, f\"{name}.geojson\")\n",
    "                obj.to_file(output_file, driver=\"GeoJSON\")\n",
    "                file_type = \"geojson\"\n",
    "            else:\n",
    "                print(f\"Objeto '{name}' não é um DataFrame nem um GeoDataFrame e será ignorado.\")\n",
    "                continue\n",
    "\n",
    "            saved_objects.append(output_file)\n",
    "            print(f\"Objeto '{name}' salvo em '{output_file}'.\")\n",
    "\n",
    "            # Criar atividade de salvamento\n",
    "            exec_end = datetime.datetime.now()\n",
    "            activity_key = f\"act-save-{name}\"\n",
    "            dict_activities[activity_key] = doc_prov.activity(\n",
    "                f\"ufrj:save_{name}\", exec_start, exec_end, {\"prov:label\": f\"Salvamento do arquivo {name}\"}\n",
    "            )\n",
    "            doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "            # Criar entidade do arquivo salvo\n",
    "            entity_key = f\"ent-saved-{name}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:saved_{name}\",\n",
    "                {\n",
    "                    \"prov:label\": f\"Arquivo salvo: {name}\",\n",
    "                    \"prov:type\": f\"void:Dataset/{file_type}\",\n",
    "                    \"prov:generatedAtTime\": exec_end.isoformat(),\n",
    "                },\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "            # Relacionar com a entidade original\n",
    "            original_entity_name = entity_mapping.get(name)\n",
    "            if original_entity_name:\n",
    "                original_entity_key = f\"ent-{original_entity_name}\"\n",
    "                doc_prov.wasDerivedFrom(dict_entities[entity_key], dict_entities[original_entity_key])\n",
    "                doc_prov.used(dict_activities[activity_key], dict_entities[original_entity_key])\n",
    "            else:\n",
    "                print(f\"Warning: Sem mapeamento para a entidade original de '{name}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar '{name}': {e}\")\n",
    "\n",
    "    if saved_objects:\n",
    "        print(\"\\nObjetos saidas:\")\n",
    "        for obj in saved_objects:\n",
    "            print(f\"- {obj}\")\n",
    "    else:\n",
    "        print(\"Nenhum objeto foi salvo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2313b6cf-b51e-45b8-9d04-30e15c07ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities, datasets\n",
    "    \n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "    # Carregar datasets\n",
    "    datasets = create_eda_dataset()\n",
    "\n",
    "    # Salvar datasets com rastreamento de proveniência\n",
    "    save_dataframes_with_provenance(datasets)\n",
    "\n",
    "    # Listar objetos carregados\n",
    "    print(\"\\nObjetos carregados e analisados:\")\n",
    "    for name, dataset in datasets.items():\n",
    "        if dataset is not None:\n",
    "            print(f\"- {name}: Shape {dataset.shape}\")\n",
    "\n",
    "    # Gerar saídas de proveniência\n",
    "    gerar_prov_outputs(doc_prov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "161613fe-8af3-47aa-bfe4-77609b30096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso para 'levantamento_sismico_2023'.\n",
      "Dados carregados com sucesso para 'tabela_pocos_2024'.\n",
      "Dados carregados com sucesso para 'tabela_levantamentos_geoquimica'.\n",
      "Dados carregados com sucesso para 'tabela_dados_geoquimica'.\n",
      "Dados carregados com sucesso para 'reservas_nacionais_hc'.\n",
      "Dados carregados com sucesso para 'pocos_perfurados_2023'.\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'SETORES_TODOS_SIRGAS.shp', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.shp'}], 'others': [{'name': 'SETORES_TODOS_SIRGAS.dbf', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.dbf'}, {'name': 'SETORES_TODOS_SIRGAS.prj', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.prj'}, {'name': 'SETORES_TODOS_SIRGAS.sbn', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.sbn'}, {'name': 'SETORES_TODOS_SIRGAS.sbx', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.sbx'}, {'name': 'SETORES_TODOS_SIRGAS.shx', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.shx'}, {'name': 'SETORES_TODOS_SIRGAS.xml', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.xml'}]}\n",
      "Shapefile 'SETORES_TODOS_SIRGAS.shp' salvo em ./dados/shp_salvos\\SETORES_TODOS_SIRGAS.shp\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp'}], 'others': [{'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.cst', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.cst'}, {'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.prj', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.prj'}, {'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.dbf', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.dbf'}, {'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.shx', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.shx'}, {'name': 'wfsrequest.txt', 'path': './dados/temp\\\\wfsrequest.txt'}]}\n",
      "Shapefile 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp' salvo em ./dados/shp_salvos\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.shp', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.shp'}], 'others': [{'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.cst', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.cst'}, {'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.prj', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.prj'}, {'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.dbf', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.dbf'}, {'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.shx', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.shx'}, {'name': 'wfsrequest.txt', 'path': './dados/temp\\\\wfsrequest.txt'}]}\n",
      "Shapefile 'CAMPOS_PRODUCAO_SIRGASPolygon.shp' salvo em ./dados/shp_salvos\\CAMPOS_PRODUCAO_SIRGASPolygon.shp\n",
      "Conteúdo do ZIP analisado: {'csv': [{'name': 'consolidacao-daa-2023-calha.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-calha.csv'}, {'name': 'consolidacao-daa-2023-fluidos.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-fluidos.csv'}, {'name': 'consolidacao-daa-2023-laminas.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-laminas.csv'}, {'name': 'consolidacao-daa-2023-laterais.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-laterais.csv'}, {'name': 'consolidacao-daa-2023-plugues.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-plugues.csv'}, {'name': 'consolidacao-daa-2023-testemunhos.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-testemunhos.csv'}], 'xlsx': [], 'xls': [], 'shp': [], 'others': []}\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-calha.csv': ['Código do poço (API)', 'Identificador da caixa', 'Total de caixas', 'Profundidade topo (m)', 'Profundidade base (m)', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-calha.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-calha.csv com shape (193949, 12)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-fluidos.csv': ['Tipo de fluido', 'Profundidade topo (m)', 'Profundidade base (m)', 'Volume (L)', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-fluidos.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-fluidos.csv com shape (3697, 12)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-laminas.csv': ['Código do poço (API)', 'Tipo de amostra de origem', 'Quantidade de lâminas delgadas', 'Quantidade de lâminas bioestratigráficas', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-laminas.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-laminas.csv com shape (401955, 10)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-laterais.csv': ['Identificador da caixa', 'Total de caixas', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-laterais.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-laterais.csv com shape (75284, 9)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-plugues.csv': ['Número do testemunho', 'Profundidade do plugue (m)', 'Orientação do plugue (H ou V)', 'Identificador da caixa', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-plugues.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-plugues.csv com shape (182167, 11)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-testemunhos.csv': ['Número do testemunho', 'Profundidade topo (m)', 'Profundidade base (m)', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-testemunhos.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-testemunhos.csv com shape (59425, 10)\n",
      "\n",
      "Datasets prontos para análise:\n",
      "- df_sismica_2023_orig: Shape (52, 15)\n",
      "- df_pocos_orig: Shape (30827, 60)\n",
      "- df_lev_geoq_2022_orig: Shape (69, 8)\n",
      "- df_geoq_2021_orig: Shape (4665, 38)\n",
      "- df_reservas_orig: Shape (419, 10)\n",
      "- df_poco_2023_orig: Shape (106, 59)\n",
      "- gdf_setores_sirgas: Shape (188, 4)\n",
      "- gdf_blocos_exploratorios: Shape (424, 15)\n",
      "- gdf_campos_producao: Shape (432, 18)\n",
      "- df_consolidacao-daa-2023-calha.csv: Shape (193949, 12)\n",
      "- df_consolidacao-daa-2023-fluidos.csv: Shape (3697, 12)\n",
      "- df_consolidacao-daa-2023-laminas.csv: Shape (401955, 10)\n",
      "- df_consolidacao-daa-2023-laterais.csv: Shape (75284, 9)\n",
      "- df_consolidacao-daa-2023-plugues.csv: Shape (182167, 11)\n",
      "- df_consolidacao-daa-2023-testemunhos.csv: Shape (59425, 10)\n",
      "Objeto 'df_sismica_2023_orig' salvo em './dados/saidas\\df_sismica_2023_orig.csv'.\n",
      "Objeto 'df_pocos_orig' salvo em './dados/saidas\\df_pocos_orig.csv'.\n",
      "Objeto 'df_lev_geoq_2022_orig' salvo em './dados/saidas\\df_lev_geoq_2022_orig.csv'.\n",
      "Objeto 'df_geoq_2021_orig' salvo em './dados/saidas\\df_geoq_2021_orig.csv'.\n",
      "Objeto 'df_reservas_orig' salvo em './dados/saidas\\df_reservas_orig.csv'.\n",
      "Objeto 'df_poco_2023_orig' salvo em './dados/saidas\\df_poco_2023_orig.csv'.\n",
      "Objeto 'gdf_setores_sirgas' salvo em './dados/saidas\\gdf_setores_sirgas.csv'.\n",
      "Objeto 'gdf_blocos_exploratorios' salvo em './dados/saidas\\gdf_blocos_exploratorios.csv'.\n",
      "Objeto 'gdf_campos_producao' salvo em './dados/saidas\\gdf_campos_producao.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-calha.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-calha.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-fluidos.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-fluidos.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-laminas.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-laminas.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-laterais.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-laterais.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-plugues.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-plugues.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-testemunhos.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-testemunhos.csv.csv'.\n",
      "\n",
      "Objetos saidas:\n",
      "- ./dados/saidas\\df_sismica_2023_orig.csv\n",
      "- ./dados/saidas\\df_pocos_orig.csv\n",
      "- ./dados/saidas\\df_lev_geoq_2022_orig.csv\n",
      "- ./dados/saidas\\df_geoq_2021_orig.csv\n",
      "- ./dados/saidas\\df_reservas_orig.csv\n",
      "- ./dados/saidas\\df_poco_2023_orig.csv\n",
      "- ./dados/saidas\\gdf_setores_sirgas.csv\n",
      "- ./dados/saidas\\gdf_blocos_exploratorios.csv\n",
      "- ./dados/saidas\\gdf_campos_producao.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-calha.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-fluidos.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-laminas.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-laterais.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-plugues.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-testemunhos.csv.csv\n",
      "\n",
      "Objetos carregados e analisados:\n",
      "- df_sismica_2023_orig: Shape (52, 15)\n",
      "- df_pocos_orig: Shape (30827, 60)\n",
      "- df_lev_geoq_2022_orig: Shape (69, 8)\n",
      "- df_geoq_2021_orig: Shape (4665, 38)\n",
      "- df_reservas_orig: Shape (419, 10)\n",
      "- df_poco_2023_orig: Shape (106, 59)\n",
      "- gdf_setores_sirgas: Shape (188, 4)\n",
      "- gdf_blocos_exploratorios: Shape (424, 15)\n",
      "- gdf_campos_producao: Shape (432, 18)\n",
      "- df_consolidacao-daa-2023-calha.csv: Shape (193949, 12)\n",
      "- df_consolidacao-daa-2023-fluidos.csv: Shape (3697, 12)\n",
      "- df_consolidacao-daa-2023-laminas.csv: Shape (401955, 10)\n",
      "- df_consolidacao-daa-2023-laterais.csv: Shape (75284, 9)\n",
      "- df_consolidacao-daa-2023-plugues.csv: Shape (182167, 11)\n",
      "- df_consolidacao-daa-2023-testemunhos.csv: Shape (59425, 10)\n",
      "Provenance graph generated successfully: EDA-PROV.png\n",
      "Provenance serialized as XML and TTL.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "306b5afe-5acb-413a-a317-b99a66bbf4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#BLOCO DE ANÁLISE DE DADOS e VISUALIZAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cdf3cc7-420c-484c-bf04-88e8e134fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise Básica dos Datasets:\n",
      "DataFrame: df_sismica_2023_orig\n",
      "Columns:\n",
      "['Nome', 'Categoria', 'Tipo', 'Autorização', 'Inicio', 'Término Real', 'Confidencialidade', 'Público em', 'Ato Normativo', 'Tecnologia', 'Bloco', 'Campo', 'Bacia', 'E.A.D', 'Operadora']\n",
      "Dtypes:\n",
      "Nome                  object\n",
      "Categoria             object\n",
      "Tipo                  object\n",
      "Autorização          float64\n",
      "Inicio                object\n",
      "Término Real          object\n",
      "Confidencialidade     object\n",
      "Público em            object\n",
      "Ato Normativo         object\n",
      "Tecnologia            object\n",
      "Bloco                 object\n",
      "Campo                 object\n",
      "Bacia                 object\n",
      "E.A.D                 object\n",
      "Operadora             object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_pocos_orig\n",
      "Columns:\n",
      "['POCO', 'CADASTRO', 'OPERADOR', 'POCO_OPERADOR', 'ESTADO', 'BACIA', 'BLOCO', 'SIG_CAMPO', 'CAMPO', 'TERRA_MAR', 'POCO_POS_ANP', 'TIPO', 'CATEGORIA', 'RECLASSIFICACAO', 'SITUACAO', 'INÍCIO', 'TÉRMINO', 'CONCLUSAO', 'TITULARIDADE', 'LATITUDE_BASE_4C', 'LONGITUDE_BASE_4C', 'LATITUDE_BASE_DD', 'LONGITUDE_BASE_DD', 'DATUM_HORIZONTAL', 'TIPO_DE_COORDENADA_DE_BASE', 'DIRECAO', 'PROFUNDIDADE_VERTICAL_M', 'PROFUNDIDADE_SONDADOR_M', 'PROFUNDIDADE_MEDIDA_M', 'REFERENCIA_DE_PROFUNDIDADE', 'MESA_ROTATIVA', 'COTA_ALTIMETRICA_M', 'LAMINA_D_AGUA_M', 'DATUM_VERTICAL', 'UNIDADE_ESTRATIGRAFICA', 'GEOLOGIA_GRUPO_FINAL', 'GEOLOGIA_FORMACAO_FINAL', 'GEOLOGIA_MEMBRO_FINAL', 'CDPE', 'AGP', 'PC', 'PAG', 'PERFIS_CONVENCIONAIS', 'DURANTE_PERFURACAO', 'PERFIS_DIGITAIS', 'PERFIS_PROCESSADOS', 'PERFIS_ESPECIAIS', 'AMOSTRA_LATERAL', 'SISMICA', 'TABELA_TEMPO_PROFUNDIDADE', 'DADOS_DIRECIONAIS', 'TESTE_A_CABO', 'TESTE_DE_FORMACAO', 'CANHONEIO', 'TESTEMUNHO', 'GEOQUIMICA', 'SIG_SONDA', 'NOM_SONDA', 'ATINGIU_PRESAL', 'DHA_ATUALIZACAO']\n",
      "Dtypes:\n",
      "POCO                           object\n",
      "CADASTRO                        int64\n",
      "OPERADOR                       object\n",
      "POCO_OPERADOR                  object\n",
      "ESTADO                         object\n",
      "BACIA                          object\n",
      "BLOCO                          object\n",
      "SIG_CAMPO                      object\n",
      "CAMPO                          object\n",
      "TERRA_MAR                      object\n",
      "POCO_POS_ANP                   object\n",
      "TIPO                            int64\n",
      "CATEGORIA                      object\n",
      "RECLASSIFICACAO                object\n",
      "SITUACAO                       object\n",
      "INÍCIO                         object\n",
      "TÉRMINO                        object\n",
      "CONCLUSAO                      object\n",
      "TITULARIDADE                   object\n",
      "LATITUDE_BASE_4C               object\n",
      "LONGITUDE_BASE_4C              object\n",
      "LATITUDE_BASE_DD               object\n",
      "LONGITUDE_BASE_DD              object\n",
      "DATUM_HORIZONTAL               object\n",
      "TIPO_DE_COORDENADA_DE_BASE     object\n",
      "DIRECAO                        object\n",
      "PROFUNDIDADE_VERTICAL_M        object\n",
      "PROFUNDIDADE_SONDADOR_M        object\n",
      "PROFUNDIDADE_MEDIDA_M          object\n",
      "REFERENCIA_DE_PROFUNDIDADE     object\n",
      "MESA_ROTATIVA                  object\n",
      "COTA_ALTIMETRICA_M             object\n",
      "LAMINA_D_AGUA_M                object\n",
      "DATUM_VERTICAL                 object\n",
      "UNIDADE_ESTRATIGRAFICA        float64\n",
      "GEOLOGIA_GRUPO_FINAL           object\n",
      "GEOLOGIA_FORMACAO_FINAL        object\n",
      "GEOLOGIA_MEMBRO_FINAL          object\n",
      "CDPE                           object\n",
      "AGP                            object\n",
      "PC                             object\n",
      "PAG                            object\n",
      "PERFIS_CONVENCIONAIS           object\n",
      "DURANTE_PERFURACAO             object\n",
      "PERFIS_DIGITAIS                object\n",
      "PERFIS_PROCESSADOS             object\n",
      "PERFIS_ESPECIAIS               object\n",
      "AMOSTRA_LATERAL                object\n",
      "SISMICA                        object\n",
      "TABELA_TEMPO_PROFUNDIDADE      object\n",
      "DADOS_DIRECIONAIS              object\n",
      "TESTE_A_CABO                   object\n",
      "TESTE_DE_FORMACAO              object\n",
      "CANHONEIO                      object\n",
      "TESTEMUNHO                     object\n",
      "GEOQUIMICA                     object\n",
      "SIG_SONDA                      object\n",
      "NOM_SONDA                      object\n",
      "ATINGIU_PRESAL                 object\n",
      "DHA_ATUALIZACAO                object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_lev_geoq_2022_orig\n",
      "Columns:\n",
      "['Nome', 'Bacia', 'Natureza da atividade', 'Início Aquisição', 'Término Aquisição', 'Ambiente', 'Confidencialidade', 'Shape']\n",
      "Dtypes:\n",
      "Nome                     object\n",
      "Bacia                    object\n",
      "Natureza da atividade    object\n",
      "Início Aquisição         object\n",
      "Término Aquisição        object\n",
      "Ambiente                 object\n",
      "Confidencialidade        object\n",
      "Shape                    object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_geoq_2021_orig\n",
      "Columns:\n",
      "['Poço', 'Extensão do Arquivo', 'Fonte dos Dados', 'Tipo de Geoquímica', 'Topo (m)', 'Base (m)', 'Cromatografia Gasosa', 'Cromatografia Líquida', 'Relatório', 'Análise de Óleo', 'Análise de Gás', 'Análise de Fluidos', 'Análise de Água', 'Biomarcadores', 'Aromáticos', 'Saturados', 'Isótopos', 'Espectrometria de Massas', 'Diamantoides', 'Carbono Orgânico Total', 'Pirólise', 'Inclusões Fluidas', 'Sumário de Óleo Cru', 'PVT', 'Re/Os de Asfaltenos', 'Litogeoquímica', 'Reflectância de Vitrinita', 'Viscosidade', 'CADASTRO', 'OPERADOR', 'ESTADO', 'BACIA', 'CAMPO', 'TITULARIDADE', 'LATITUDE', 'LONGITUDE', 'DATUM', 'DIRECAO']\n",
      "Dtypes:\n",
      "Poço                         object\n",
      "Extensão do Arquivo          object\n",
      "Fonte dos Dados              object\n",
      "Tipo de Geoquímica           object\n",
      "Topo (m)                     object\n",
      "Base (m)                     object\n",
      "Cromatografia Gasosa         object\n",
      "Cromatografia Líquida        object\n",
      "Relatório                    object\n",
      "Análise de Óleo              object\n",
      "Análise de Gás               object\n",
      "Análise de Fluidos           object\n",
      "Análise de Água              object\n",
      "Biomarcadores                object\n",
      "Aromáticos                   object\n",
      "Saturados                    object\n",
      "Isótopos                     object\n",
      "Espectrometria de Massas     object\n",
      "Diamantoides                 object\n",
      "Carbono Orgânico Total       object\n",
      "Pirólise                     object\n",
      "Inclusões Fluidas            object\n",
      "Sumário de Óleo Cru          object\n",
      "PVT                          object\n",
      "Re/Os de Asfaltenos          object\n",
      "Litogeoquímica               object\n",
      "Reflectância de Vitrinita    object\n",
      "Viscosidade                  object\n",
      "CADASTRO                      int64\n",
      "OPERADOR                     object\n",
      "ESTADO                       object\n",
      "BACIA                        object\n",
      "CAMPO                        object\n",
      "TITULARIDADE                 object\n",
      "LATITUDE                     object\n",
      "LONGITUDE                    object\n",
      "DATUM                        object\n",
      "DIRECAO                      object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_reservas_orig\n",
      "Columns:\n",
      "['Ano', 'Campo/Área de desenvolvimento', 'Bacia', 'Estado', 'VOIP (bbl)', 'VGIP (m³)', 'Petróleo Acumulado (bbl)', 'Gás Natural Acumulado (m³)', 'Fração Recuperada de Petróleo', 'Situação']\n",
      "Dtypes:\n",
      "Ano                                int64\n",
      "Campo/Área de desenvolvimento     object\n",
      "Bacia                             object\n",
      "Estado                            object\n",
      "VOIP (bbl)                       float64\n",
      "VGIP (m³)                        float64\n",
      "Petróleo Acumulado (bbl)         float64\n",
      "Gás Natural Acumulado (m³)       float64\n",
      "Fração Recuperada de Petróleo    float64\n",
      "Situação                          object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_poco_2023_orig\n",
      "Columns:\n",
      "['POCO', 'CADASTRO', 'OPERADOR', 'POCO_OPERADOR', 'ESTADO', 'BACIA', 'BLOCO', 'SIG_CAMPO', 'CAMPO', 'TERRA_MAR', 'POCO_POS_ANP', 'TIPO', 'CATEGORIA', 'RECLASSIFICACAO', 'SITUACAO', 'INICIO', 'TERMINO', 'CONCLUSAO', 'LATITUDE_BASE_4C', 'LONGITUDE_BASE_4C', 'LATITUDE_BASE_DD', 'LONGITUDE_BASE_DD', 'DATUM_HORIZONTAL', 'TIPO_DE_COORDENADA_DE_BASE', 'DIRECAO', 'PROFUNDIDADE_VERTICAL_M', 'PROFUNDIDADE_SONDADOR_M', 'PROFUNDIDADE_MEDIDA_M', 'REFERENCIA_DE_PROFUNDIDADE', 'MESA_ROTATIVA', 'COTA_ALTIMETRICA_M', 'LAMINA_D_AGUA_M', 'DATUM_VERTICAL', 'UNIDADE_ESTRATIGRAFICA', 'GEOLOGIA_GRUPO_FINAL', 'GEOLOGIA_FORMACAO_FINAL', 'GEOLOGIA_MEMBRO_FINAL', 'CDPE', 'AGP', 'PC', 'PAG', 'PERFIS_CONVENCIONAIS', 'DURANTE_PERFURACAO', 'PERFIS_DIGITAIS', 'PERFIS_PROCESSADOS', 'PERFIS_ESPECIAIS', 'AMOSTRA_LATERAL', 'SISMICA', 'TABELA_TEMPO_PROFUNDIDADE', 'DADOS_DIRECIONAIS', 'TESTE_A_CABO', 'TESTE_DE_FORMACAO', 'CANHONEIO', 'TESTEMUNHO', 'GEOQUIMICA', 'SIG_SONDA', 'NOM_SONDA', 'ATINGIU_PRESAL', 'DHA_ATUALIZACAO']\n",
      "Dtypes:\n",
      "POCO                           object\n",
      "CADASTRO                        int64\n",
      "OPERADOR                       object\n",
      "POCO_OPERADOR                  object\n",
      "ESTADO                         object\n",
      "BACIA                          object\n",
      "BLOCO                          object\n",
      "SIG_CAMPO                      object\n",
      "CAMPO                          object\n",
      "TERRA_MAR                      object\n",
      "POCO_POS_ANP                   object\n",
      "TIPO                            int64\n",
      "CATEGORIA                      object\n",
      "RECLASSIFICACAO                object\n",
      "SITUACAO                       object\n",
      "INICIO                         object\n",
      "TERMINO                        object\n",
      "CONCLUSAO                      object\n",
      "LATITUDE_BASE_4C               object\n",
      "LONGITUDE_BASE_4C              object\n",
      "LATITUDE_BASE_DD               object\n",
      "LONGITUDE_BASE_DD              object\n",
      "DATUM_HORIZONTAL               object\n",
      "TIPO_DE_COORDENADA_DE_BASE     object\n",
      "DIRECAO                        object\n",
      "PROFUNDIDADE_VERTICAL_M        object\n",
      "PROFUNDIDADE_SONDADOR_M        object\n",
      "PROFUNDIDADE_MEDIDA_M          object\n",
      "REFERENCIA_DE_PROFUNDIDADE     object\n",
      "MESA_ROTATIVA                  object\n",
      "COTA_ALTIMETRICA_M             object\n",
      "LAMINA_D_AGUA_M               float64\n",
      "DATUM_VERTICAL                 object\n",
      "UNIDADE_ESTRATIGRAFICA        float64\n",
      "GEOLOGIA_GRUPO_FINAL           object\n",
      "GEOLOGIA_FORMACAO_FINAL        object\n",
      "GEOLOGIA_MEMBRO_FINAL          object\n",
      "CDPE                           object\n",
      "AGP                           float64\n",
      "PC                             object\n",
      "PAG                            object\n",
      "PERFIS_CONVENCIONAIS           object\n",
      "DURANTE_PERFURACAO             object\n",
      "PERFIS_DIGITAIS               float64\n",
      "PERFIS_PROCESSADOS             object\n",
      "PERFIS_ESPECIAIS               object\n",
      "AMOSTRA_LATERAL                object\n",
      "SISMICA                        object\n",
      "TABELA_TEMPO_PROFUNDIDADE      object\n",
      "DADOS_DIRECIONAIS              object\n",
      "TESTE_A_CABO                   object\n",
      "TESTE_DE_FORMACAO              object\n",
      "CANHONEIO                      object\n",
      "TESTEMUNHO                    float64\n",
      "GEOQUIMICA                    float64\n",
      "SIG_SONDA                      object\n",
      "NOM_SONDA                      object\n",
      "ATINGIU_PRESAL                 object\n",
      "DHA_ATUALIZACAO                object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: gdf_setores_sirgas\n",
      "Columns:\n",
      "['ID1', 'BACIA', 'NOME_SETOR', 'geometry']\n",
      "Dtypes:\n",
      "ID1              int64\n",
      "BACIA           object\n",
      "NOME_SETOR      object\n",
      "geometry      geometry\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: gdf_blocos_exploratorios\n",
      "Columns:\n",
      "['COD_BLOCO', 'COD_FASE_C', 'DAT_ASSINA', 'DAT_TERMIN', 'NOM_BACIA', 'NOM_BLOCO', 'NOM_FANTAS', 'NUM_CONTRA', 'NUM_DESCOB', 'OPERADOR_C', 'RODADA', 'AREA_TOTAL', 'AMBIENTE', 'BLOCOS', 'geometry']\n",
      "Dtypes:\n",
      "COD_BLOCO       object\n",
      "COD_FASE_C      object\n",
      "DAT_ASSINA      object\n",
      "DAT_TERMIN      object\n",
      "NOM_BACIA       object\n",
      "NOM_BLOCO       object\n",
      "NOM_FANTAS      object\n",
      "NUM_CONTRA      object\n",
      "NUM_DESCOB     float64\n",
      "OPERADOR_C      object\n",
      "RODADA          object\n",
      "AREA_TOTAL     float64\n",
      "AMBIENTE        object\n",
      "BLOCOS          object\n",
      "geometry      geometry\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: gdf_campos_producao\n",
      "Columns:\n",
      "['NUM_RODADA', 'NOM_CAMPO', 'AREA', 'OPERADOR_C', 'NUM_CONTRA', 'DAT_ASSINA', 'DAT_TERMIN', 'NOM_BACIA', 'COD_CAMPO', 'SIG_CAMPO', 'DAT_DESCOB', 'DAT_INICIO', 'ETAPA', 'MED_LAMINA', 'FLUIDO_PRI', 'ID', 'AMBIENTE', 'geometry']\n",
      "Dtypes:\n",
      "NUM_RODADA      object\n",
      "NOM_CAMPO       object\n",
      "AREA           float64\n",
      "OPERADOR_C      object\n",
      "NUM_CONTRA      object\n",
      "DAT_ASSINA      object\n",
      "DAT_TERMIN      object\n",
      "NOM_BACIA       object\n",
      "COD_CAMPO      float64\n",
      "SIG_CAMPO       object\n",
      "DAT_DESCOB      object\n",
      "DAT_INICIO      object\n",
      "ETAPA           object\n",
      "MED_LAMINA     float64\n",
      "FLUIDO_PRI      object\n",
      "ID             float64\n",
      "AMBIENTE        object\n",
      "geometry      geometry\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_consolidacao-daa-2023-calha.csv\n",
      "Columns:\n",
      "['Bacia sedimentar', 'Código do poço (API)', 'Nome do poço (ANP)', 'Identificador da caixa', 'Total de caixas', 'Profundidade topo (m)', 'Profundidade base (m)', 'Data de conclusão do poço', 'Município/Estado', 'Depositária', 'Unnamed: 10', 'Unnamed: 11']\n",
      "Dtypes:\n",
      "Bacia sedimentar                     object\n",
      "Código do poço (API)         string[python]\n",
      "Nome do poço (ANP)                   object\n",
      "Identificador da caixa       string[python]\n",
      "Total de caixas              string[python]\n",
      "Profundidade topo (m)        string[python]\n",
      "Profundidade base (m)        string[python]\n",
      "Data de conclusão do poço    string[python]\n",
      "Município/Estado                     object\n",
      "Depositária                          object\n",
      "Unnamed: 10                         float64\n",
      "Unnamed: 11                         float64\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_consolidacao-daa-2023-fluidos.csv\n",
      "Columns:\n",
      "['Bacia sedimentar', 'Código do poço (API)', 'Nome do poço (ANP)', 'Tipo de fluido', 'Profundidade topo (m)', 'Profundidade base (m)', 'Quantidade de amostra', 'Tipo de container', 'Volume (L)', 'Data de conclusão do poço', 'Município/Estado', 'Depositária']\n",
      "Dtypes:\n",
      "Bacia sedimentar                     object\n",
      "Código do poço (API)                 object\n",
      "Nome do poço (ANP)                   object\n",
      "Tipo de fluido               string[python]\n",
      "Profundidade topo (m)        string[python]\n",
      "Profundidade base (m)        string[python]\n",
      "Quantidade de amostra                object\n",
      "Tipo de container                    object\n",
      "Volume (L)                   string[python]\n",
      "Data de conclusão do poço    string[python]\n",
      "Município/Estado                     object\n",
      "Depositária                          object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_consolidacao-daa-2023-laminas.csv\n",
      "Columns:\n",
      "['Bacia sedimentar', 'Código do poço (API)', 'Nome do poço (ANP)', 'Profundidade da lâmina (m)', 'Tipo de amostra de origem', 'Quantidade de lâminas delgadas', 'Quantidade de lâminas bioestratigráficas', 'Data de conclusão do poço', 'Município/Estado', 'Depositária']\n",
      "Dtypes:\n",
      "Bacia sedimentar                                    object\n",
      "Código do poço (API)                        string[python]\n",
      "Nome do poço (ANP)                                  object\n",
      "Profundidade da lâmina (m)                          object\n",
      "Tipo de amostra de origem                   string[python]\n",
      "Quantidade de lâminas delgadas              string[python]\n",
      "Quantidade de lâminas bioestratigráficas    string[python]\n",
      "Data de conclusão do poço                   string[python]\n",
      "Município/Estado                                    object\n",
      "Depositária                                         object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_consolidacao-daa-2023-laterais.csv\n",
      "Columns:\n",
      "['Bacia sedimentar', 'Código do poço (API)', 'Nome do poço (ANP)', 'Profundidade (m)', 'Identificador da caixa', 'Total de caixas', 'Data de conclusão do poço', 'Município/Estado', 'Depositária']\n",
      "Dtypes:\n",
      "Bacia sedimentar                     object\n",
      "Código do poço (API)                 object\n",
      "Nome do poço (ANP)                   object\n",
      "Profundidade (m)                     object\n",
      "Identificador da caixa       string[python]\n",
      "Total de caixas              string[python]\n",
      "Data de conclusão do poço    string[python]\n",
      "Município/Estado                     object\n",
      "Depositária                          object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_consolidacao-daa-2023-plugues.csv\n",
      "Columns:\n",
      "['Bacia sedimentar', 'Código do poço (API)', 'Nome do poço (ANP)', 'Número do testemunho', 'Profundidade do plugue (m)', 'Orientação do plugue (H ou V)', 'Identificador da caixa', 'Total de caixas', 'Data de conclusão do poço', 'Município/Estado', 'Depositária']\n",
      "Dtypes:\n",
      "Bacia sedimentar                         object\n",
      "Código do poço (API)                     object\n",
      "Nome do poço (ANP)                       object\n",
      "Número do testemunho             string[python]\n",
      "Profundidade do plugue (m)       string[python]\n",
      "Orientação do plugue (H ou V)    string[python]\n",
      "Identificador da caixa           string[python]\n",
      "Total de caixas                         float64\n",
      "Data de conclusão do poço        string[python]\n",
      "Município/Estado                         object\n",
      "Depositária                              object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "DataFrame: df_consolidacao-daa-2023-testemunhos.csv\n",
      "Columns:\n",
      "['Bacia sedimentar', 'Código do poço (API)', 'Nome do poço (ANP)', 'Número do testemunho', 'Total de caixas', 'Profundidade topo (m)', 'Profundidade base (m)', 'Data de conclusão do poço', 'Município/Estado', 'Depositária']\n",
      "Dtypes:\n",
      "Bacia sedimentar                     object\n",
      "Código do poço (API)                 object\n",
      "Nome do poço (ANP)                   object\n",
      "Número do testemunho         string[python]\n",
      "Total de caixas                      object\n",
      "Profundidade topo (m)        string[python]\n",
      "Profundidade base (m)        string[python]\n",
      "Data de conclusão do poço    string[python]\n",
      "Município/Estado                     object\n",
      "Depositária                          object\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_datasets(datasets):\n",
    "    \n",
    "    #Itera sobre todos os DataFrames e GeoDataFrames do conjunto de datasets,\n",
    "    #imprimindo as colunas e os tipos de dados.\n",
    "    \n",
    "    for name, dataset in datasets.items():\n",
    "        if isinstance(dataset, pd.DataFrame):\n",
    "            print(f\"DataFrame: {name}\")\n",
    "            print(\"Columns:\")\n",
    "            print(dataset.columns.tolist())\n",
    "            print(\"Dtypes:\")\n",
    "            print(dataset.dtypes)\n",
    "            print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "        elif isinstance(dataset, gpd.GeoDataFrame):\n",
    "            print(f\"GeoDataFrame: {name}\")\n",
    "            print(\"Columns:\")\n",
    "            print(dataset.columns.tolist())\n",
    "            print(\"Dtypes:\")\n",
    "            print(dataset.dtypes)\n",
    "            print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Objeto '{name}' não é um DataFrame nem um GeoDataFrame. Ignorando...\\n\")\n",
    "\n",
    "\n",
    "# Chamar a função após carregar os datasets na função `main`\n",
    "# Isso carrega os datasets usando a lógica implementada no `main`\n",
    "\n",
    "# Analisar os datasets carregados\n",
    "print(\"Análise Básica dos Datasets:\")\n",
    "analyze_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9ceb29c-7c33-43a2-b265-91c43ac7cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gdf_with_folium(global_vars):\n",
    "    \"\"\"\n",
    "    Plota os GeoDataFrames disponíveis no ambiente global usando Folium.\n",
    "    \n",
    "    Args:\n",
    "        global_vars (dict): Dicionário contendo as variáveis globais para varredura.\n",
    "    \"\"\"\n",
    "    # Filtrar GeoDataFrames\n",
    "    gdfs = {name: obj for name, obj in global_vars.items() if isinstance(obj, gpd.GeoDataFrame)}\n",
    "\n",
    "    for name, gdf in gdfs.items():\n",
    "        print(f\"Plotando GeoDataFrame: {name}...\")\n",
    "        \n",
    "        # Criar um mapa centrado na geometria do GeoDataFrame\n",
    "        centroid = gdf.geometry.unary_union.centroid\n",
    "        m = folium.Map(location=[centroid.y, centroid.x], zoom_start=10)\n",
    "        \n",
    "        # Adicionar os dados ao mapa\n",
    "        GeoJson(data=mapping(gdf)).add_to(m)\n",
    "        \n",
    "        # Exibir o mapa no Jupyter\n",
    "        display(m)\n",
    "\n",
    "# Chamada da função usando as variáveis globais\n",
    "plot_gdf_with_folium(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6b511-0820-4441-9e09-e58a8e2dbcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d406e0c0-1b35-468d-841f-3e08bfec7676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440471a8-cc2e-421e-9171-d296659cdb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bda194-b2b5-4fbd-aa29-9ed90f93c87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970ee26-8203-4436-b8cf-f4cb5213e0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bc7ba-96a5-4045-aeef-6d2e40631b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d1dad-241f-4963-9b9b-366ac685d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6819c-aab9-46b9-a8fa-27eebe8a99c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "498bbbcd-bf7d-465c-97dc-0e853178826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_sismica_2023_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc74c138-fbb6-4b29-9aad-cc19bc0dd3a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def inspect_dataframes(dataframes):\\n    \\n    #Inspeciona um dicionário de DataFrames, exibindo colunas, shape e estatísticas descritivas.\\n   \\n    for name, df in dataframes.items():\\n        print(f\"\\n=== {name} ===\")\\n        print(f\"Shape: {df.shape}\")\\n        print(f\"Columns: {df.columns.tolist()}\")\\n        print(\"\\nHead:\")\\n        print(df.head())\\n        print(\"\\nDescribe:\")\\n        print(df.describe(include=\\'all\\', datetime_is_numeric=True))\\n\\n    def clean_dataframe_columns(dataframes):\\n  \\n    #Realiza o saneamento e a renomeação de colunas em um conjunto de DataFrames.\\n\\n    cleaned_dataframes = {}\\n    for name, df in dataframes.items():\\n        # Renomear colunas removendo espaços e convertendo para letras minúsculas\\n        df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\\n\\n        # Registrar atividade de saneamento na proveniência\\n        execStartTime = datetime.datetime.now()\\n        activity_key = f\"act-saneamento-{name}\"\\n        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:saneamento_{name}\", execStartTime, None, {\\n            \"prov:label\": escape_label(f\"Saneamento de colunas: {name}\")\\n        })\\n        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\\n\\n        entity_key = f\"ent-saneado-{name}\"\\n        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{name}_saneado\", {\\n            \"prov:label\": escape_label(f\"Dataset saneado: {name}\"),\\n            \"prov:type\": \"void:Dataset\",\\n        })\\n        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\\n\\n        cleaned_dataframes[name] = df\\n    return cleaned_dataframes\\n        '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def inspect_dataframes(dataframes):\n",
    "    \n",
    "    #Inspeciona um dicionário de DataFrames, exibindo colunas, shape e estatísticas descritivas.\n",
    "   \n",
    "    for name, df in dataframes.items():\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(\"\\nHead:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nDescribe:\")\n",
    "        print(df.describe(include='all', datetime_is_numeric=True))\n",
    "\n",
    "    def clean_dataframe_columns(dataframes):\n",
    "  \n",
    "    #Realiza o saneamento e a renomeação de colunas em um conjunto de DataFrames.\n",
    "\n",
    "    cleaned_dataframes = {}\n",
    "    for name, df in dataframes.items():\n",
    "        # Renomear colunas removendo espaços e convertendo para letras minúsculas\n",
    "        df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "        # Registrar atividade de saneamento na proveniência\n",
    "        execStartTime = datetime.datetime.now()\n",
    "        activity_key = f\"act-saneamento-{name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:saneamento_{name}\", execStartTime, None, {\n",
    "            \"prov:label\": escape_label(f\"Saneamento de colunas: {name}\")\n",
    "        })\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        entity_key = f\"ent-saneado-{name}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{name}_saneado\", {\n",
    "            \"prov:label\": escape_label(f\"Dataset saneado: {name}\"),\n",
    "            \"prov:type\": \"void:Dataset\",\n",
    "        })\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "        cleaned_dataframes[name] = df\n",
    "    return cleaned_dataframes\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f14207f-be28-4283-9314-1e02e715db8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fundamentos de Ciência de Dados\n",
    "## PPGI/UFRJ 2024.2\n",
    "### Profs Sergio Serra e Jorge Zavaleta\n",
    "### Aluno Ubirajara S. Santos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "21fc0eda-8216-4ae5-ba67-9a060afea2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prov\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import plotly\n",
    "import graphviz\n",
    "import unicodedata\n",
    "import platform\n",
    "import importlib.metadata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import openpyxl\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from prov.model import ProvDocument, Namespace\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from difflib import get_close_matches\n",
    "from folium import GeoJson\n",
    "from shapely.geometry import mapping\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4ac38fd8-c6aa-4138-ae54-2a103a3f52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './dados/saidas'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "38838849-a486-40fb-a8c1-35e945ee947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fontes de Dados\n",
    "data_sources = {\n",
    "     \"amostras_rochas_fluidos\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-amostras-de-rochas-e-fluidos/acervo-de-amostras/consolidacao-2023.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"setores_sirgas\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/assuntos/exploracao-e-producao-de-oleo-e-gas/estudos-geologicos-e-geofisicos/arquivos-classificacao-de-modelos-exploratorios/setores-sirgas.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"bacias\":{\n",
    "        \"url\":\"https://geomaps.anp.gov.br/geoserver/wfs?service=wfs&version=1.0.0&request=GetFeature&typeName=sisroc:bacias_gishub_db&outputFormat=SHAPE-ZIP&format_options=filename:Bacias-zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"blocos_exploratorios\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ABLOCOS_EXPLORATORIOS_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"campos_producao\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ACAMPOS_PRODUCAO_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"reservas_nacionais_hc\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-estatisticos/arquivos-reservas-nacionais-de-petroleo-e-gas-natural/tabela-dados-bar-2023.xlsx\",\n",
    "        \"type\": \"xlsx\"},\n",
    "     \"pocos_perfurados_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/pocos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_levantamentos_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-levantamentos-geoquimicos.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"levantamento_sismico_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/sismicos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_pocos_2024\": {\n",
    "        \"url\": \"./dados/entradas/Tabela_pocos_2024_Novembro_24.csv\",\n",
    "        \"type\": \"csv\", \"sep\": \";\" ,\"encoding\": \"ANSI\"},\n",
    "     \"tabela_dados_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-dados-geoquimicos.csv\",\n",
    "        \"type\": \"csv\",\n",
    "        \"header\": 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d2c3e5eb-413c-433b-ad53-5bfb214de691",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FUNÇÂO EM USO\n",
    "\n",
    "def gerar_prov_outputs(doc_prov):\n",
    "    entity = \"EDA-PROV\"\n",
    "    output_file = f\"{entity}.png\"\n",
    "    try:\n",
    "        dot = prov_to_dot(doc_prov)\n",
    "        # Write to PNG\n",
    "        dot.write_png(output_file)\n",
    "        print(f\"Provenance graph generated successfully: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating provenance graph: {e}\")\n",
    "        # Save the DOT file for debugging\n",
    "        with open(\"debug.dot\", \"w\") as f:\n",
    "            f.write(dot.to_string())\n",
    "        print(\"Saved DOT file for debugging as 'debug.dot'.\")\n",
    "\n",
    "    # Serialização do documento\n",
    "    doc_prov.serialize(entity + \".xml\", format='xml') \n",
    "    doc_prov.serialize(entity + \".ttl\", format='rdf', rdf_format='ttl',encoding=\"utf-8\")\n",
    "    print(\"Provenance serialized as XML and TTL.\")\n",
    "\n",
    "\n",
    "def adding_namespaces(document_prov):\n",
    "    # Adiciona namespaces ao documento de proveniência.\n",
    "    document_prov.add_namespace('void', 'http://vocab.deri.ie/void#')\n",
    "    document_prov.add_namespace('ufrj', 'https://www.ufrj.br')\n",
    "    document_prov.add_namespace('schema', 'http://schema.org/')    # Dados estruturados Schema.org\n",
    "    document_prov.add_namespace('prov', 'http://www.w3.org/ns/prov#')     # Padrões PROV\n",
    "    document_prov.add_namespace('foaf', 'http://xmlns.com/foaf/0.1/')     # Agentes FOAF\n",
    "    document_prov.add_namespace('ufrj-ppgi', 'http://www.ufrj.br/ppgi/')  # UFRJ PPGI\n",
    "    document_prov.add_namespace('anp', 'https://www.gov.br/anp/pt-br')    # ANP - Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\n",
    "    document_prov.add_namespace('anp-dados_tec','https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/acervo-de-dados-tecnicos') # ANP - Acervo de Dados Técnicos \n",
    "    document_prov.add_namespace('petrobras','https://petrobras.com.br/')  # PETROBRAS\n",
    "    document_prov.add_namespace('br','http://br.org/ns/')    # Organizações Brasileiras\n",
    "    document_prov.add_namespace('git','https://github.com/ubirajara-s/eda') # Githut do repositório projeto EDA\n",
    "    return document_prov\n",
    "\n",
    "\n",
    "def escape_label(text):\n",
    "    \"\"\"\n",
    "    Escapes special characters for Graphviz.\n",
    "    Encodes text to ASCII with XML character references.\n",
    "    \"\"\"\n",
    "    return text.encode(\"ascii\", \"xmlcharrefreplace\").decode()\n",
    "\n",
    "def get_installed_packages():\n",
    "    #Retorna os pacotes instalados no ambiente com suas versões.\n",
    "    try:\n",
    "        return {pkg.metadata['Name']: pkg.version for pkg in importlib.metadata.distributions()}\n",
    "    except ImportError:\n",
    "        import pkg_resources\n",
    "        return {dist.project_name: dist.version for dist in pkg_resources.working_set}\n",
    "\n",
    "def get_system_info():\n",
    "    #Retorna informações do sistema.\n",
    "    return {\n",
    "        \"OS\": platform.system(),\n",
    "        \"OS Version\": platform.version(),\n",
    "        \"OS Release\": platform.release(),\n",
    "        \"Python Version\": sys.version,\n",
    "        \"Python Executable\": sys.executable,\n",
    "        \"Current Working Directory\": str(Path.cwd()),}\n",
    "\n",
    "def get_used_packages():\n",
    "\n",
    "    #Retorna um dicionário dos pacotes usados explicitamente no projeto e suas versões.\n",
    "\n",
    "    packages = ['numpy', 'pandas', 'matplotlib', 'seaborn', 'rdflib', 'prov', 'graphviz', \n",
    "                'openpyxl', 'folium', 'pydot', 'requests', 'geopandas', 'plotly','ipython', 'contextily']  # Adicione ou remova pacotes usados\n",
    "    package_versions = {}\n",
    "    for package in packages:\n",
    "        try:\n",
    "            import importlib.metadata\n",
    "            version = importlib.metadata.version(package)\n",
    "            package_versions[package] = version\n",
    "        except ImportError:\n",
    "            print(f\"Pacote {package} não encontrado.\")\n",
    "    return package_versions\n",
    "\n",
    "def add_system_and_package_provenance(doc_prov):\n",
    "    #Adiciona informações do sistema e pacotes ao documento de proveniência\n",
    "\n",
    "    # Criar atividade para rastrear informações de sistema e pacotes\n",
    "    activity_id = \"ufrj:track_system_and_packages\"\n",
    "    tracking_activity = doc_prov.activity(activity_id, datetime.now(), None, {\"prov:label\": escape_label(\"Track system and package provenance\")})\n",
    "\n",
    "    # Associar a atividade ao agente do notebook\n",
    "    if \"ag-eda-ipynb\" in dict_agents:\n",
    "        doc_prov.wasAssociatedWith(tracking_activity, dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Adicionar informações do sistema como entidades\n",
    "    system_info = get_system_info()\n",
    "    for key, value in system_info.items():\n",
    "        sanitized_key = key.replace(\" \", \"_\")  # Substituir espaços por _\n",
    "        sys_entity = doc_prov.entity(f\"schema:{sanitized_key}\", {\"prov:value\": value})\n",
    "        doc_prov.wasGeneratedBy(sys_entity, tracking_activity)\n",
    "\n",
    "    # Adicionar pacotes usados como entidades\n",
    "    used_packages = get_used_packages()\n",
    "    for pkg, version in used_packages.items():\n",
    "        pkg_entity = doc_prov.entity(f\"schema:{pkg}\", {\"prov:value\": version})\n",
    "        doc_prov.wasGeneratedBy(pkg_entity, tracking_activity)\n",
    "\n",
    "    return doc_prov\n",
    "\n",
    "def create_agents(document_prov):\n",
    "\n",
    "    #creating agents\n",
    "    dagnts={} #cria dic\n",
    "    dagnts[\"ag-orgbr\"] = document_prov.agent(\"br:orgBr\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Oraganizações Brasileiras\")})\n",
    "    dagnts[\"ag-anp\"] = document_prov.agent(\"anp:ANP\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\")})\n",
    "    dagnts[\"ag-ufrj\"] = document_prov.agent(\"ufrj:UFRJ\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Universidade Federal do Rio de Janeiro\")})\n",
    "    dagnts[\"ag-ppgi\"] = document_prov.agent(\"ufrj:PPGI\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Programa de Pós Graduação em Informática\")})\n",
    "    dagnts[\"ag-greco\"] = document_prov.agent(\"ufrj:GRECO\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Grupo de Engenharia do Conhecimento\")})\n",
    "    dagnts[\"ag-author-ubirajara\"] = document_prov.agent(\"ufrj:Ubirajara\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Ubirajara Simões Santos\"), \"foaf:mbox\":\"ubirajas@hotmail.com\"})\n",
    "    dagnts[\"ag-author-sergio\"] = document_prov.agent(\"ufrj:Sergio\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Sergio Serra\"), \"foaf:mbox\":\"serra@ppgi.ufrj.br\"})\n",
    "    dagnts[\"ag-author-jorge\"] = document_prov.agent(\"ufrj:Jorge\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Jorge Zavaleta\"), \"foaf:mbox\":\"zavaleta@pet-si.ufrrj.br\"})\n",
    "    dagnts[\"ag-petrobras\"] = document_prov.agent(\"petrobras:Petrobras\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Petróleo Brasiliero S.A\")})\n",
    "    dagnts[\"ag-eda-ipynb\"] = document_prov.agent(\"ufrj:eda.ipynb\", {\"prov:type\":\"prov:SoftwareAgent\", \"foaf:name\":escape_label(\"eda.ipynb\"), \"prov:label\":escape_label(\"Notebook Python utilizado no trabalho\")})\n",
    "    return dagnts\n",
    "\n",
    "def associate_ufrj_agents(agents_dictionary):\n",
    "    agents_dictionary[\"ag-anp\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-petrobras\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ufrj\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ppgi\"].actedOnBehalfOf(agents_dictionary[\"ag-ufrj\"])\n",
    "    agents_dictionary[\"ag-greco\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-greco\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-petrobras\"])\n",
    "    agents_dictionary[\"ag-author-sergio\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-jorge\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-eda-ipynb\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    return agents_dictionary\n",
    "\n",
    "\n",
    "def create_initial_activities(document_prov):\n",
    "    #creating activities\n",
    "    #dataDownloadDatasets = datetime.datetime.strptime('29/11/24', '%d/%m/%y')\n",
    "\n",
    "    dativs={}\n",
    "    dativs[\"act-create-ds\"] = document_prov.activity(\"anp:create-dataset\", None, None, {\"prov:label\":escape_label( \"Criação de datasets pela ANP\")})\n",
    "    #dativs[\"act-extract-ds\"] = document_prov.activity(\"ufrj:extract-dataset\")\n",
    "    dativs[\"act-create-ds-eda\"] = document_prov.activity(\"ufrj:create-ds-eda\", None, None, {\"prov:label\":escape_label( \"Criação de datasets para EDA\")})\n",
    "    #dativs[\"act-load-ds-eda\"] = document_prov.activity(\"ufrj:load-ds-eda\")\n",
    "    dativs[\"act-save-ipynb\"] = document_prov.activity(\"ufrj:save-ipynb\", None, None, {\"prov:label\":escape_label(\"Salvar notebook EDA\")})\n",
    "    return dativs\n",
    "\n",
    "def cria_entidades_iniciais(document_prov):\n",
    "    global dict_entities  # Adicionar entidades ao dicionário global\n",
    "    #creating entidades\n",
    "    dents={}\n",
    "\n",
    "    # Entidade para amostras de rochas e fluidos\n",
    "    dents[\"ent-amostras-rochas-fluidos\"] = document_prov.entity('anp:amostras_rochas_fluidos', {'prov:label':escape_label('Dataset com amostras de rochas e fluidos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Consolidado 2023 de amostras disponíveis.'), 'prov:format': 'zip' })\n",
    "    # Entidade para setores SIRGAS\n",
    "    dents[\"ent-setores-sirgas\"] = document_prov.entity('anp:setores_sirgas', {'prov:label':escape_label('Setores SIRGAS'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Modelos exploratórios em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para Bacias\n",
    "    dents[\"ent-bacias\"] = document_prov.entity('anp:bacias', {'prov:label':escape_label('Bacias Sedimentares'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Mapas Bacias Sedimentares.'), 'prov:format': 'zip'})\n",
    "    # Entidade para blocos exploratórios\n",
    "    dents[\"ent-blocos-exploratorios\"] = document_prov.entity('anp:blocos_exploratorios', {'prov:label':escape_label( 'Blocos exploratórios'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Blocos exploratórios com dados geoespaciais.'), 'prov:format': 'zip'})\n",
    "    # Entidade para campos de produção\n",
    "    dents[\"ent-campos-producao\"] = document_prov.entity('anp:campos_producao', {'prov:label':escape_label( 'Campos de Produção'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados dos campos de produção em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para reservas nacionais de hidrocarbonetos\n",
    "    dents[\"ent-reservas-nacionais-hc\"] = document_prov.entity('anp:reservas_nacionais_hc',{'prov:label':escape_label( 'Reservas Nacionais de Hidrocarbonetos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela com dados sobre reservas nacionais.'), 'prov:format': 'xlsx'})\n",
    "    # Entidade para poços perfurados (2023)\n",
    "    dents[\"ent-pocos-perfurados-2023\"] = document_prov.entity('anp:pocos_perfurados_2023',{'prov:label':escape_label( 'Poços perfurados - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com os poços perfurados no ano de 2023.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de levantamentos geoquímicos\n",
    "    dents[\"ent-tabela-levantamentos-geoquimica\"] = document_prov.entity('anp:tabela_levantamentos_geoquimica',{'prov:label':escape_label( 'Tabela de levantamentos geoquímicos 20/04/2022'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados sobre levantamentos geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para tabela de dados geoquímicos\n",
    "    dents[\"ent-tabela-dados-geoquimica\"] = document_prov.entity('anp:tabela_dados_geoquimica',{'prov:label':escape_label( 'Tabela_dados_geoquimica 06/08/2021'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para levantamento sísmico (2023)\n",
    "    dents[\"ent-levantamento-sismico-2023\"] = document_prov.entity('anp:levantamento_sismico_2023', {'prov:label':escape_label( 'Levantamento Sísmico - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com dados de levantamentos sísmicos públicos.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de poços (2024)\n",
    "    dents[\"ent-tabela-pocos-2024\"] = document_prov.entity('anp:tabela_pocos_2024', {'prov:label':escape_label( 'Tabela de Poços - 2024'.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela CSV com dados atualizados de poços para 2024.'), 'prov:format': 'csv'})\n",
    "     # Entidade para ANP dados técnicos\n",
    "    dents[\"ent-anp-dados_tec-ds\"] = document_prov.entity('anp-dados_tec:dataset', {'prov:label':escape_label( 'ANP Dataset de Dados Técnicos'.encode(\"ascii\", \"xmlcharrefreplace\").decode()),'prov:type': 'void:Dataset','prov:description':escape_label('Dataset com dados técnicos disponíveis publicamente.'),'prov:format': 'csv'})\n",
    "    # Entidade script python\n",
    "    dents[\"ent-eda-ipynb\"] = document_prov.entity('ufrj:eda-ipyn', {'prov:label':escape_label( \"Notebook Python utilizado no trabalho\".encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'foaf:Document'})\n",
    "    # Entidade Git\n",
    "    dents[\"ent-git-eda\"] = document_prov.entity('git:github-eda', {'prov:label':escape_label( 'Repositorio GIT projeto EDA '.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'prov:Collection'})\n",
    "    return dents\n",
    "\n",
    "\n",
    "    dict_entities.update({\n",
    "        \"ent-setores-sirgas\": document_prov.entity(\n",
    "            'anp:setores_sirgas',{'prov:label': escape_label('Setores SIRGAS'),'prov:type': 'void:Dataset','prov:description': escape_label('Modelos exploratórios em formato SIRGAS.'),\n",
    "                'prov:format': 'zip'}),\n",
    "        \"ent-blocos-exploratorios\": document_prov.entity(\n",
    "            'anp:blocos_exploratorios',{'prov:label': escape_label('Blocos exploratórios'),'prov:type': 'void:Dataset','prov:description': escape_label('Blocos exploratórios com dados geoespaciais.'),\n",
    "                'prov:format': 'zip'}),\n",
    "        \"ent-campos-producao\": document_prov.entity(\n",
    "            'anp:campos_producao',{'prov:label': escape_label('Campos de Produção'),'prov:type': 'void:Dataset','prov:description': escape_label('Dados dos campos de produção em formato SIRGAS.'),\n",
    "                'prov:format': 'zip'}),\n",
    "        \"ent-bacias\": document_prov.entity(\n",
    "            'anp:bacias', {'prov:label':escape_label('Bacias Sedimentares'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Mapas Bacias Sedimentares.'),\n",
    "                'prov:format': 'zip'}),\n",
    "    })\n",
    "\n",
    "def initial_association_agents_activities_entities(document_prov, dictionary_agents, dictionary_activities, dictionary_entities):\n",
    "\n",
    "    #Associate activity of generate dataset with ANP agent\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds\"], dictionary_agents[\"ag-anp\"])\n",
    "\n",
    "    #Associating datasets with activities of generate eba datasets\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-amostras-rochas-fluidos\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-setores-sirgas\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-bacias\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-blocos-exploratorios\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-campos-producao\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-reservas-nacionais-hc\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-pocos-perfurados-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-levantamentos-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-dados-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-levantamento-sismico-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-pocos-2024\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-anp-dados_tec-ds\"], dictionary_activities[\"act-create-ds\"])\n",
    "\n",
    "    #associate activity of eda, com autor\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-author-ubirajara\"])   \n",
    "\n",
    "    #associate notebook agent with eba dataset\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-eda-ipynb\"])    \n",
    "\n",
    "    #associate eda github repository with store datasets activity\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-git-eda\"], dictionary_activities[\"act-save-ipynb\"])\n",
    "\n",
    "def associate_save_activity(doc_prov, dict_agents, dict_entities):\n",
    "\n",
    "    #Associa a atividade de salvar notebook ao agente e à entidade relevante.\n",
    "\n",
    "    activity_id = \"ufrj:save-ipynb\"\n",
    "    save_activity = doc_prov.activity(activity_id, datetime.now(), None, {\"prov:label\": escape_label(\"Salvar notebook EDA\")})\n",
    "    # Associar ao agente eda.ipynb\n",
    "    if \"ag-eda-ipynb\" in dict_agents:\n",
    "        doc_prov.wasAssociatedWith(save_activity, dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar à entidade eda-ipynb\n",
    "    if \"ent-eda-ipynb\" in dict_entities:\n",
    "        doc_prov.wasGeneratedBy(dict_entities[\"ent-eda-ipynb\"], save_activity)\n",
    "\n",
    "    return doc_prov\n",
    "\n",
    "\n",
    "def initProvenance():\n",
    "    #Inicializa o documento de proveniência com namespaces, agentes, atividades e entidades.\n",
    "\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Criando um documento vazio de proveniência\n",
    "    doc_prov = ProvDocument()\n",
    "    dict_agents = {}\n",
    "    dict_activities = {}\n",
    "    dict_entities = {}\n",
    "\n",
    "    # Criar namespaces no documento de proveniência\n",
    "    doc_prov = adding_namespaces(doc_prov)\n",
    "\n",
    "    # Criar agentes\n",
    "    dict_agents = create_agents(doc_prov)\n",
    "\n",
    "    # Criar atividades iniciais\n",
    "    dict_activities = create_initial_activities(doc_prov)\n",
    "\n",
    "    # Criar entidades iniciais e atualizar o dicionário global\n",
    "    cria_entidades_iniciais(doc_prov)\n",
    "\n",
    "    # Criar entidades iniciais\n",
    "    dict_entities = cria_entidades_iniciais(doc_prov)\n",
    "\n",
    "    # Criar hierarquia de agentes\n",
    "    dict_agents = associate_ufrj_agents(dict_agents)\n",
    "\n",
    "    # Associar agentes, atividades e entidades\n",
    "    initial_association_agents_activities_entities(doc_prov, dict_agents, dict_activities, dict_entities)\n",
    "\n",
    "    # Adicionar proveniência do sistema e pacotes\n",
    "    doc_prov = add_system_and_package_provenance(doc_prov)\n",
    "\n",
    "    # Associar atividade ufrj:save-ipynb\n",
    "    doc_prov = associate_save_activity(doc_prov, dict_agents, dict_entities)\n",
    "\n",
    "    return doc_prov, dict_agents, dict_activities, dict_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e031f122-f76a-4894-be1a-cf55b4fdf9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÂO EM USO\n",
    "def analyze_zip_content(url, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Analisa o conteúdo de um arquivo ZIP e categoriza os tipos de arquivos encontrados.\n",
    "    Args:\n",
    "        url (str): URL para o arquivo ZIP.\n",
    "        temp_dir (str): Diretório temporário para extração dos arquivos.\n",
    "    Returns:\n",
    "        dict: Dicionário com categorias de arquivos (csv, xlsx, shp, others).\n",
    "    \"\"\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    file_types = {\"csv\": [], \"xlsx\": [], \"xls\": [], \"shp\": [], \"others\": []}\n",
    "\n",
    "    try:\n",
    "        # Baixar o arquivo ZIP\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Erro ao baixar o arquivo: {response.status_code}\")\n",
    "\n",
    "        # Abrir o ZIP em memória e extrair\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zf:\n",
    "            zf.extractall(temp_dir)\n",
    "            extracted_files = zf.namelist()\n",
    "\n",
    "        # Categorizar os arquivos extraídos\n",
    "        for file in extracted_files:\n",
    "            file_path = os.path.join(temp_dir, file)\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_types[\"csv\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".xlsx\"):\n",
    "                file_types[\"xlsx\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".xls\"):\n",
    "                file_types[\"xls\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".shp\"):\n",
    "                file_types[\"shp\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            else:\n",
    "                file_types[\"others\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "\n",
    "        print(f\"Conteúdo do ZIP analisado: {file_types}\")\n",
    "        return file_types\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"O arquivo fornecido não é um ZIP válido: {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar ZIP: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "02b413d1-798a-4d64-8949-4446900b33da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def diagnose_csv(file_path, encodings=[\"utf-8\", \"ISO-8859-1\", \"utf-8-sig\"]):\\n\\n    #Diagnostica problemas em arquivos CSV: delimitador e encoding.\\n\\n    print(f\"Diagnóstico do arquivo: {file_path}\")\\n    for encoding in encodings:\\n        try:\\n            print(f\"Tentando com encoding: {encoding}\")\\n            with open(file_path, \"r\", encoding=encoding) as f:\\n                sample = f.read(2048)  # Lê os primeiros 2048 caracteres\\n            print(f\"Primeiros caracteres ({encoding}):\")\\n            print(sample[:500])  # Mostra apenas os primeiros 500 caracteres\\n            print(\"\\n--- Fim da Amostra ---\\n\")\\n        except Exception as e:\\n            print(f\"Erro ao ler o arquivo com encoding {encoding}: {e}\")'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def diagnose_csv(file_path, encodings=[\"utf-8\", \"ISO-8859-1\", \"utf-8-sig\"]):\n",
    "\n",
    "    #Diagnostica problemas em arquivos CSV: delimitador e encoding.\n",
    "\n",
    "    print(f\"Diagnóstico do arquivo: {file_path}\")\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"Tentando com encoding: {encoding}\")\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                sample = f.read(2048)  # Lê os primeiros 2048 caracteres\n",
    "            print(f\"Primeiros caracteres ({encoding}):\")\n",
    "            print(sample[:500])  # Mostra apenas os primeiros 500 caracteres\n",
    "            print(\"\\n--- Fim da Amostra ---\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo com encoding {encoding}: {e}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8f878a87-e607-4e44-86d2-3d39946695ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def report_unknown_files(unknown_files):\\n\\n    #Exibe uma mensagem com arquivos de formatos não reconhecidos e registra a proveniência.\\n    #Args:\\n    #    unknown_files (list): Lista de dicionários com informações sobre arquivos não reconhecidos.\\n    #        Cada dicionário contém:\\n    #            - \\'name\\': Nome do arquivo.\\n    #            - \\'path\\': Caminho completo para o arquivo.\\n\\n    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\\n\\n    if unknown_files:\\n        print(f\"Arquivos não reconhecidos encontrados: {[file[\\'name\\'] for file in unknown_files]}\")\\n\\n        # Registrar proveniência da análise\\n        exec_start = datetime.now()\\n        activity_key = \"act-analyze-unknown-files\"\\n        dict_activities[activity_key] = doc_prov.activity(\\n            \"ufrj:analyze_unknown_files\",\\n            exec_start,\\n            None,\\n            {\"prov:label\": escape_label(\"Análise de arquivos desconhecidos\")}\\n        )\\n        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\\n\\n        # Criar entidades para cada arquivo desconhecido\\n        for file_info in unknown_files:\\n            entity_key = f\"ent-unknown-{file_info[\\'name\\']}\"\\n            dict_entities[entity_key] = doc_prov.entity(\\n                f\"ufrj:unknown_{file_info[\\'name\\']}\",\\n                {\"prov:label\": escape_label(f\"Arquivo desconhecido: {file_info[\\'name\\']}\"),\\n                 \"prov:type\": \"void:Dataset\",\\n                 \"prov:location\": file_info[\"path\"]}\\n            )\\n            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\\n\\n    else:\\n        print(\"Nenhum arquivo não reconhecido foi encontrado.\")    '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def report_unknown_files(unknown_files):\n",
    "\n",
    "    #Exibe uma mensagem com arquivos de formatos não reconhecidos e registra a proveniência.\n",
    "    #Args:\n",
    "    #    unknown_files (list): Lista de dicionários com informações sobre arquivos não reconhecidos.\n",
    "    #        Cada dicionário contém:\n",
    "    #            - 'name': Nome do arquivo.\n",
    "    #            - 'path': Caminho completo para o arquivo.\n",
    "\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\n",
    "\n",
    "    if unknown_files:\n",
    "        print(f\"Arquivos não reconhecidos encontrados: {[file['name'] for file in unknown_files]}\")\n",
    "\n",
    "        # Registrar proveniência da análise\n",
    "        exec_start = datetime.now()\n",
    "        activity_key = \"act-analyze-unknown-files\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(\n",
    "            \"ufrj:analyze_unknown_files\",\n",
    "            exec_start,\n",
    "            None,\n",
    "            {\"prov:label\": escape_label(\"Análise de arquivos desconhecidos\")}\n",
    "        )\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        # Criar entidades para cada arquivo desconhecido\n",
    "        for file_info in unknown_files:\n",
    "            entity_key = f\"ent-unknown-{file_info['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:unknown_{file_info['name']}\",\n",
    "                {\"prov:label\": escape_label(f\"Arquivo desconhecido: {file_info['name']}\"),\n",
    "                 \"prov:type\": \"void:Dataset\",\n",
    "                 \"prov:location\": file_info[\"path\"]}\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "    else:\n",
    "        print(\"Nenhum arquivo não reconhecido foi encontrado.\")    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "716a3e91-04d8-4964-b8cd-a3cbff355109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_sources_with_zip(data_sources, source_name, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Atualiza `data_sources` com os arquivos extraídos de um ZIP, criando uma relação hierárquica.\n",
    "    \"\"\"\n",
    "    # Obter fonte original\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Analisar o conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao processar ZIP '{source_name}'.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Criar relações de proveniência\n",
    "    for file_type, files in file_types.items():\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        for i, file_info in enumerate(files):\n",
    "            child_name = f\"{source_name}_{file_type}_{i+1}\"\n",
    "            data_sources[child_name] = {\n",
    "                \"url\": file_info[\"path\"],  # Caminho local do arquivo extraído\n",
    "                \"type\": file_type,        # Tipo do arquivo (csv, xlsx, shp, etc.)\n",
    "                \"parent\": source_name,    # Referência ao ZIP pai\n",
    "                \"sep\": source.get(\"sep\", \";\") if file_type == \"csv\" else None,\n",
    "                \"encoding\": source.get(\"encoding\", \"utf-8\") if file_type == \"csv\" else None,\n",
    "                \"header\": source.get(\"header\", 0) if file_type == \"csv\" else None,\n",
    "                \"date_columns\": source.get(\"date_columns\", []) if file_type == \"csv\" else None,\n",
    "            }\n",
    "            print(f\"Adicionado {child_name} como filho de {source_name}.\")\n",
    "\n",
    "    print(f\"Data sources atualizados com arquivos de '{source_name}'.\")\n",
    "    return data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0462131f-eba2-428c-a2d5-e56fe66c0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_provenance_for_zip_and_children(parent_source, children_sources, activity_prefix=\"process-zip\"):\n",
    "    \"\"\"\n",
    "    Registra a proveniência entre o ZIP pai e os arquivos extraídos (filhos).\n",
    "    A atividade é associada ao agente 'ag-eda-ipynb' para todos os filhos.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Criar atividade para o processamento do ZIP\n",
    "    exec_start = datetime.now()\n",
    "    activity_key = f\"{activity_prefix}-{parent_source}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        f\"ufrj:{activity_prefix}_{parent_source}\", exec_start, None,\n",
    "        {\"prov:label\": escape_label(f\"Processamento do ZIP {parent_source}\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Registrar cada filho como derivado do pai\n",
    "    for child_source in children_sources:\n",
    "        entity_key = f\"ent-{child_source}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{child_source}\", {\n",
    "            \"prov:label\": escape_label(f\"Arquivo derivado de {parent_source}\"),\n",
    "            \"prov:type\": \"void:Dataset\"\n",
    "        })\n",
    "\n",
    "        # Relacionar pai e filho\n",
    "        doc_prov.wasDerivedFrom(\n",
    "            dict_entities[entity_key], dict_entities.get(f\"ent-{parent_source}\")\n",
    "        )\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "        # Adiciona a atividade de proveniência do arquivo como \"gerado por\" o agente IPYNB\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "    print(f\"Proveniência registrada para arquivos derivados de {parent_source}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "df4ff600-8332-4879-b792-63dcc13dc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÂO EM USO\n",
    "def load_data_from_source_csv(source_name, data_sources):\n",
    "    \"\"\"\n",
    "    Carrega dados com base no nome da fonte e na configuração em data_sources.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com os dados carregados, ou None se houver erro.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents,  dict_activities, dict_entities  # Declare global variables\n",
    "    #save execution start time\n",
    "    execStartTime = datetime.now()\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "\n",
    "    if not source: #or source.get(\"type\") != \"csv\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é do tipo CSV.\")\n",
    "        return None\n",
    "\n",
    "    file_type = source.get(\"type\")\n",
    "    url = source.get(\"url\")\n",
    "    sep = source.get(\"sep\", \";\")  # Valor padrão para CSV\n",
    "    encoding = source.get(\"encoding\", \"utf-8\")  # Valor padrão para codificação\n",
    "    date_columns = source.get(\"date_columns\", [])  \n",
    "\n",
    "    try:\n",
    "        if file_type == \"csv\":\n",
    "             # Caso específico para tabela_pocos_2024\n",
    "            if source_name == \"tabela_pocos_2024\":\n",
    "                df = pd.read_csv(url, encoding=\"ANSI\", sep=sep)\n",
    "            # Caso específico para tabela_dados_geoquimica\n",
    "            elif source_name == \"tabela_dados_geoquimica\":\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, header=1)  # Cabeçalho na segunda linha\n",
    "            else:\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, parse_dates=date_columns)\n",
    "        elif file_type == \"xlsx\":\n",
    "            df = pd.read_excel(url)\n",
    "        else:\n",
    "            print(f\"Tipo de arquivo '{file_type}' não suportado.\")\n",
    "            return None\n",
    "        print(f\"Dados carregados com sucesso para '{source_name}'.\")\n",
    "\n",
    "        # End execution time for provenance tracking\n",
    "        execEndTime = datetime.now()\n",
    "\n",
    "        # Criar atividade com horário de término da execução\n",
    "        activity_key = f\"act-carga-{source_name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}\", execStartTime, execEndTime)\n",
    "\n",
    "        # Associar a atividade ao agente\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        # Associar a atividade com os dados carregados\n",
    "        entity_key = f\"ent-{source_name}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{source_name}\", {\n",
    "            \"prov:label\": escape_label(f\"Dataset carregado: {source_name}\"),\"prov:type\": \"void:Dataset\", \"prov:generatedAtTime\": execEndTime.isoformat(),})\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "        # Associar a atividade ufrj:carga à entidade correspondente criada pela ANP\n",
    "        anp_entity_key = f\"ent-{source_name.replace('_', '-')}\"  # Convert to ANP format (e.g., `tabela_pocos_2024` -> `ent-tabela-pocos-2024`)\n",
    "        if anp_entity_key in dict_entities:\n",
    "            # Establish the prov:used relationship\n",
    "            doc_prov.used(dict_activities[activity_key], dict_entities[anp_entity_key])\n",
    "        else:\n",
    "            print(f\"Warning: ANP entity '{anp_entity_key}' not found for activity '{activity_key}'.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados de '{source_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8225242c-86be-4ec0-a1a8-e43a5740930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÂO EM USO\n",
    "def process_zip_source_shp(source_name, data_sources, temp_dir=\"./dados/temp\", output_dir=\"./dados/shp_salvos\"):\n",
    "    \"\"\"\n",
    "    Processa um ZIP contendo arquivos SHP, descompacta e carrega os arquivos como GeoDataFrames.\n",
    "    Também salva os arquivos SHP extraídos em um diretório de saída.\n",
    "    Args:\n",
    "        source_name (str): Nome da fonte de dados.\n",
    "        data_sources (dict): Dicionário com informações das fontes de dados.\n",
    "        temp_dir (str): Diretório temporário para descompactar os arquivos.\n",
    "        output_dir (str): Diretório para salvar os arquivos SHP extraídos.\n",
    "    Returns:\n",
    "        dict: Dicionário contendo os GeoDataFrames carregados.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Validação dos parâmetros\n",
    "    if not isinstance(data_sources, dict):\n",
    "        raise ValueError(\"data_sources deve ser um dicionário contendo informações sobre as fontes de dados.\")\n",
    "\n",
    "    if source_name not in data_sources:\n",
    "        raise ValueError(f\"source_name '{source_name}' não encontrado em data_sources.\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    exec_start = datetime.now()\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {\"dataframes\": {}, \"geodataframes\": {}}\n",
    "\n",
    "    # Analisar conteúdo do ZIP\n",
    "    try:\n",
    "        file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "        if not file_types:\n",
    "            print(f\"Erro ao analisar o conteúdo do ZIP '{source_name}'.\")\n",
    "            return {\"dataframes\": {}, \"geodataframes\": {}}\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o ZIP '{source_name}': {e}\")\n",
    "        return {\"dataframes\": {}, \"geodataframes\": {}}\n",
    "\n",
    "    # Criar atividade de processamento\n",
    "    activity_key = f\"act-carga-{source_name}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}\", exec_start)\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar atividade à entidade ANP\n",
    "    anp_entity_key = f\"ent-{source_name.replace('_', '-')}\"\n",
    "    if anp_entity_key in dict_entities:\n",
    "        doc_prov.used(dict_activities[activity_key], dict_entities[anp_entity_key])\n",
    "    else:\n",
    "        print(f\"Warning: ANP entity '{anp_entity_key}' not found for activity '{activity_key}'.\")\n",
    "\n",
    "    # Criar entidade derivada\n",
    "    derived_entity_key = f\"ent-processed-{source_name}\"\n",
    "    dict_entities[derived_entity_key] = doc_prov.entity(\n",
    "        f\"ufrj:{source_name}\",\n",
    "        {\n",
    "            \"prov:label\": escape_label(f\"Dataset processado: {source_name}\"),\n",
    "            \"prov:type\": \"void:Dataset\",\n",
    "            \"prov:generatedAtTime\": datetime.now().isoformat(),\n",
    "        }\n",
    "    )\n",
    "    doc_prov.wasGeneratedBy(dict_entities[derived_entity_key], dict_activities[activity_key])\n",
    "\n",
    "    # Processar Shapefiles\n",
    "    geodataframes = {}\n",
    "    for shp_file in file_types[\"shp\"]:\n",
    "        try:\n",
    "            gdf = gpd.read_file(shp_file[\"path\"])\n",
    "\n",
    "            # Ajustar campos DateTime se necessário\n",
    "            for col in gdf.select_dtypes(include=[\"datetime64[ns]\"]).columns:\n",
    "                gdf[col] = gdf[col].dt.date\n",
    "\n",
    "            geodataframes[shp_file[\"name\"]] = gdf\n",
    "\n",
    "            # Salvar o shapefile no diretório de saída\n",
    "            shp_output_path = os.path.join(output_dir, shp_file[\"name\"])\n",
    "            gdf.to_file(shp_output_path)\n",
    "            print(f\"Shapefile '{shp_file['name']}' salvo em {shp_output_path}\")\n",
    "\n",
    "            # Atualizar proveniência para o arquivo salvo\n",
    "            shp_entity_key = f\"ent-shp-{shp_file['name']}\"\n",
    "            dict_entities[shp_entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:shp_{shp_file['name']}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"Shapefile carregado e salvo: {shp_file['name']}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.now().isoformat(),\n",
    "                    \"prov:location\": escape_label(shp_output_path)\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasDerivedFrom(dict_entities[shp_entity_key], dict_entities[derived_entity_key])\n",
    "            doc_prov.wasGeneratedBy(dict_entities[shp_entity_key], dict_activities[activity_key])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar ou salvar Shapefile {shp_file['name']}: {e}\")\n",
    "\n",
    "    return {\"dataframes\": {}, \"geodataframes\": geodataframes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bbb7daf3-3688-4e04-b58c-77ba1d5f58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÃO EM USO\n",
    "def process_csv_with_dtype_handling(file_path, header_row, initial_dtype=None):\n",
    "    \"\"\"\n",
    "    Carrega um arquivo CSV, detecta colunas com tipos mistos e força essas colunas para string.\n",
    "    Args:\n",
    "        file_path (str): Caminho para o arquivo CSV.\n",
    "        header_row (int): Linha onde o cabeçalho está localizado (baseado em zero).\n",
    "        initial_dtype (dict, optional): Tipos de dados esperados para as colunas. Padrão: None.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame carregado com colunas mistas forçadas para string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carregar com os tipos iniciais fornecidos\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            sep=\";\",\n",
    "            skiprows=header_row,\n",
    "            dtype=initial_dtype,\n",
    "            encoding=\"utf-8\",\n",
    "            on_bad_lines=\"skip\",\n",
    "            low_memory=False  # Evita warnings de chunks\n",
    "        )\n",
    "        print(f\"Arquivo {file_path} carregado com sucesso. Shape: {df.shape}\")\n",
    "    except DtypeWarning as warning:\n",
    "        print(f\"Detectados tipos mistos ao carregar {file_path}. Tentando corrigir...\")\n",
    "        mixed_columns = []\n",
    "        try:\n",
    "            # Identificar colunas com tipos mistos ao carregar sem forçar tipos\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=\";\",\n",
    "                skiprows=header_row,\n",
    "                encoding=\"utf-8\",\n",
    "                on_bad_lines=\"skip\",\n",
    "                low_memory=False\n",
    "            )\n",
    "            for col in df.columns:\n",
    "                if df[col].apply(type).nunique() > 1:  # Se mais de um tipo presente\n",
    "                    mixed_columns.append(col)\n",
    "            print(f\"Colunas com tipos mistos detectadas: {mixed_columns}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao identificar colunas com tipos mistos: {e}\")\n",
    "            return pd.DataFrame()  # Retorna DataFrame vazio em caso de erro\n",
    "\n",
    "        # Forçar colunas detectadas como strings\n",
    "        force_dtype = {col: 'string' for col in mixed_columns}\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=\";\",\n",
    "                skiprows=header_row,\n",
    "                dtype=force_dtype,\n",
    "                encoding=\"utf-8\",\n",
    "                on_bad_lines=\"skip\",\n",
    "                low_memory=False\n",
    "            )\n",
    "            print(f\"Colunas mistas corrigidas e forçadas para string no arquivo {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar {file_path} com colunas forçadas para string: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o arquivo {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c07275ee-e4fd-49f2-b7e1-349408b288ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zip_source_csv_with_headers(source_name, data_sources, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Processa um ZIP contendo CSVs, descompacta e carrega os CSVs como DataFrames.\n",
    "    Suporta arquivos com cabeçalhos em linhas diferentes e corrige tipos mistos.\n",
    "    Args:\n",
    "        source_name (str): Nome da fonte de dados no dicionário `data_sources`.\n",
    "        data_sources (dict): Dicionário contendo informações das fontes de dados.\n",
    "        temp_dir (str): Diretório temporário para extração dos arquivos.\n",
    "    Returns:\n",
    "        dict: Dicionário com DataFrames carregados.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {}\n",
    "\n",
    "    exec_start = datetime.now()\n",
    "\n",
    "    # Analisar conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types or not file_types[\"csv\"]:\n",
    "        print(f\"Erro: Nenhum CSV encontrado no ZIP '{source_name}'.\")\n",
    "        return {}\n",
    "\n",
    "    # Criar atividade de processamento do ZIP\n",
    "    activity_key = f\"act-carga-{source_name}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}\", exec_start)\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar a atividade ao ZIP original (entidade ANP)\n",
    "    anp_entity_key = f\"ent-{source_name.replace('_', '-')}\"\n",
    "    if anp_entity_key in dict_entities:\n",
    "        doc_prov.used(dict_activities[activity_key], dict_entities[anp_entity_key])\n",
    "    else:\n",
    "        print(f\"Warning: ANP entity '{anp_entity_key}' not found for activity '{activity_key}'.\")\n",
    "\n",
    "    # Criar entidade derivada para o ZIP processado\n",
    "    derived_entity_key = f\"ent-processed-{source_name}\"\n",
    "    dict_entities[derived_entity_key] = doc_prov.entity(\n",
    "        f\"ufrj:{source_name}\",\n",
    "        {\n",
    "            \"prov:label\": escape_label(f\"Dataset processado: {source_name}\"),\n",
    "            \"prov:type\": \"void:Dataset\",\n",
    "            \"prov:generatedAtTime\": datetime.now().isoformat(),\n",
    "        }\n",
    "    )\n",
    "    doc_prov.wasGeneratedBy(dict_entities[derived_entity_key], dict_activities[activity_key])\n",
    "\n",
    "    # Configuração de cabeçalhos conhecidos e dtypes\n",
    "    header_config = {\n",
    "        \"consolidacao-daa-2023-calha.csv\": 3,  # Cabeçalho na linha 4\n",
    "        \"consolidacao-daa-2023-fluidos.csv\": 2,  # Cabeçalho na linha 3\n",
    "        \"consolidacao-daa-2023-laminas.csv\": 3,  # Cabeçalho na linha 4\n",
    "        \"consolidacao-daa-2023-laterais.csv\": 3,\n",
    "        \"consolidacao-daa-2023-plugues.csv\": 3,\n",
    "        \"consolidacao-daa-2023-testemunhos.csv\": 3\n",
    "    }\n",
    "    dtype_config = {\n",
    "        \"consolidacao-daa-2023-calha.csv\": {\n",
    "            'Código do poço (API)': 'string',\n",
    "            'Identificador da caixa': 'string',\n",
    "            'Total de caixas': 'string',\n",
    "        },\n",
    "        # Outros `dtypes` podem ser adicionados aqui\n",
    "    }\n",
    "\n",
    "    # Processar os arquivos CSV extraídos\n",
    "    dataframes = {}\n",
    "    for csv_info in file_types[\"csv\"]:\n",
    "        try:\n",
    "            file_name = csv_info[\"name\"]\n",
    "            header_row = header_config.get(file_name, 0)  # Padrão é a primeira linha\n",
    "            dtype = dtype_config.get(file_name, None)\n",
    "\n",
    "            # Carregar CSV com tentativa de detecção de tipos mistos\n",
    "            df = pd.read_csv(\n",
    "                csv_info[\"path\"],\n",
    "                sep=\";\",\n",
    "                skiprows=header_row,\n",
    "                dtype=dtype,\n",
    "                encoding=\"utf-8\",\n",
    "                on_bad_lines=\"skip\",\n",
    "                low_memory=False  # Processar grandes arquivos\n",
    "            )\n",
    "\n",
    "            # Detectar colunas com tipos mistos e forçar para string\n",
    "            mixed_columns = []\n",
    "            for col in df.columns:\n",
    "                if df[col].apply(type).nunique() > 1:  # Mais de um tipo detectado\n",
    "                    mixed_columns.append(col)\n",
    "            if mixed_columns:\n",
    "                print(f\"Colunas com tipos mistos detectadas no arquivo '{file_name}': {mixed_columns}\")\n",
    "                dtype_update = {col: 'string' for col in mixed_columns}\n",
    "                df = pd.read_csv(\n",
    "                    csv_info[\"path\"],\n",
    "                    sep=\";\",\n",
    "                    skiprows=header_row,\n",
    "                    dtype={**dtype, **dtype_update} if dtype else dtype_update,\n",
    "                    encoding=\"utf-8\",\n",
    "                    on_bad_lines=\"skip\",\n",
    "                    low_memory=False\n",
    "                )\n",
    "                print(f\"Tipos mistos corrigidos no arquivo '{file_name}'.\")\n",
    "\n",
    "            # Validar cabeçalho\n",
    "            if df.columns[0].startswith(\"Todas as Informações\") or df.columns[0].startswith(\";;;;;;;;\"):\n",
    "                print(f\"Ajustando cabeçalho para '{file_name}'...\")\n",
    "                df = pd.read_csv(\n",
    "                    csv_info[\"path\"],\n",
    "                    sep=\";\",\n",
    "                    skiprows=header_row + 1,  # Ajustar o cabeçalho\n",
    "                    dtype=dtype,\n",
    "                    encoding=\"utf-8\",\n",
    "                    on_bad_lines=\"skip\",\n",
    "                    low_memory=False\n",
    "                )\n",
    "\n",
    "            dataframes[f\"df_{file_name}\"] = df\n",
    "\n",
    "            # Criar entidade para o CSV\n",
    "            csv_entity_key = f\"ent-csv-{file_name}\"\n",
    "            dict_entities[csv_entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:csv_{file_name}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"CSV carregado: {file_name}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[csv_entity_key], dict_activities[activity_key])\n",
    "            doc_prov.wasDerivedFrom(dict_entities[csv_entity_key], dict_entities[derived_entity_key])\n",
    "            print(f\"CSV carregado: {file_name} com shape {df.shape}\")\n",
    "        except Exception as e:\n",
    "            # Logar o erro e registrar a entidade como \"não processada\"\n",
    "            print(f\"Erro ao carregar CSV {file_name}: {e}\")\n",
    "            error_entity_key = f\"ent-error-{file_name}\"\n",
    "            dict_entities[error_entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:error_{file_name}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"Erro ao carregar CSV: {file_name}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:description\": escape_label(str(e)),\n",
    "                    \"prov:generatedAtTime\": datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[error_entity_key], dict_activities[activity_key])\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e0c2861d-5b7d-4ecc-82be-126430426f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO EM USO\n",
    "def create_eda_dataset():\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Inicializar o dicionário de datasets\n",
    "    datasets = {}\n",
    "\n",
    "    # Carregar datasets CSV/XLSX diretamente da URL\n",
    "    datasets[\"df_sismica_2023_orig\"] = load_data_from_source_csv(\"levantamento_sismico_2023\", data_sources)\n",
    "    datasets[\"df_pocos_orig\"] = load_data_from_source_csv(\"tabela_pocos_2024\", data_sources)\n",
    "    datasets[\"df_lev_geoq_2022_orig\"] = load_data_from_source_csv(\"tabela_levantamentos_geoquimica\", data_sources)\n",
    "    datasets[\"df_geoq_2021_orig\"] = load_data_from_source_csv(\"tabela_dados_geoquimica\", data_sources)\n",
    "    datasets[\"df_reservas_orig\"] = load_data_from_source_csv(\"reservas_nacionais_hc\", data_sources)\n",
    "    datasets[\"df_poco_2023_orig\"] = load_data_from_source_csv(\"pocos_perfurados_2023\", data_sources)\n",
    "\n",
    "    # Carregar shapefiles do ZIP\n",
    "    gdf_setores_sirgas = process_zip_source_shp(\"setores_sirgas\", data_sources).get(\"geodataframes\", {}).get(\"SETORES_TODOS_SIRGAS.shp\")\n",
    "    datasets[\"gdf_setores_sirgas\"] = gdf_setores_sirgas\n",
    "\n",
    "    gdf_bacias = process_zip_source_shp(\"bacias\", data_sources).get(\"geodataframes\", {}).get(\"BACIAS.shp\")\n",
    "    datasets[\"gdf_bacias\"] = gdf_setores_sirgas\n",
    "\n",
    "    gdf_blocos_exploratorios = process_zip_source_shp(\"blocos_exploratorios\", data_sources).get(\"geodataframes\", {}).get(\"BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp\")\n",
    "    datasets[\"gdf_blocos_exploratorios\"] = gdf_blocos_exploratorios\n",
    "\n",
    "    gdf_campos_producao = process_zip_source_shp(\"campos_producao\", data_sources).get(\"geodataframes\", {}).get(\"CAMPOS_PRODUCAO_SIRGASPolygon.shp\")\n",
    "    datasets[\"gdf_campos_producao\"] = gdf_campos_producao\n",
    "\n",
    "    # Carregar CSVs do ZIP 'amostras_rochas_fluidos' com tratamento de cabeçalhos\n",
    "    dfs_amostras_rochas_fluidos = process_zip_source_csv_with_headers(\"amostras_rochas_fluidos\", data_sources)\n",
    "    datasets.update(dfs_amostras_rochas_fluidos)\n",
    "\n",
    "    print(\"\\nDatasets prontos para análise:\")\n",
    "    for name, dataset in datasets.items():\n",
    "        if dataset is not None:\n",
    "            print(f\"- {name}: Shape {dataset.shape}\")\n",
    "\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "647fa83e-b154-4319-9a35-ece43d76aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframes_with_provenance(datasets, output_dir=\"./dados/saidas\"):\n",
    "    \"\"\"\n",
    "    Salva os DataFrames e GeoDataFrames no diretório especificado e registra a proveniência.\n",
    "    Args:\n",
    "        datasets (dict): Dicionário contendo os DataFrames e GeoDataFrames.\n",
    "        output_dir (str): Diretório de saída.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_activities, dict_entities, dict_agents\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_objects = []\n",
    "\n",
    "    # Mapeamento explícito de nomes\n",
    "    entity_mapping = {\n",
    "        \"df_sismica_2023_orig\": \"levantamento_sismico_2023\",\n",
    "        \"df_pocos_orig\": \"tabela_pocos_2024\",\n",
    "        \"df_lev_geoq_2022_orig\": \"tabela_levantamentos_geoquimica\",\n",
    "        \"df_geoq_2021_orig\": \"tabela_dados_geoquimica\",\n",
    "        \"df_reservas_orig\": \"reservas_nacionais_hc\",\n",
    "        \"df_poco_2023_orig\": \"pocos_perfurados_2023\",\n",
    "        \"gdf_setores_sirgas\": \"setores_sirgas\",\n",
    "        \"gdf_bacias\": \"bacias\",\n",
    "        \"gdf_blocos_exploratorios\": \"blocos_exploratorios\",\n",
    "        \"gdf_campos_producao\": \"campos_producao\",\n",
    "        # Mapear datasets derivados\n",
    "        \"df_consolidacao-daa-2023-calha.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-fluidos.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-laminas.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-laterais.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-plugues.csv\": \"amostras_rochas_fluidos\",\n",
    "        \"df_consolidacao-daa-2023-testemunhos.csv\": \"amostras_rochas_fluidos\",\n",
    "    }\n",
    "\n",
    "    # Garantir que todas as entidades originais estão registradas\n",
    "    for original_entity in set(entity_mapping.values()):\n",
    "        original_entity_key = f\"ent-{original_entity}\"\n",
    "        if original_entity_key not in dict_entities:\n",
    "            dict_entities[original_entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:{original_entity}\",\n",
    "                {\n",
    "                    \"prov:label\": f\"Entidade original: {original_entity}\",\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    for name, obj in datasets.items():\n",
    "        if obj is None:\n",
    "            continue\n",
    "\n",
    "        exec_start = datetime.now()\n",
    "        try:\n",
    "            if isinstance(obj, pd.DataFrame):\n",
    "                output_file = os.path.join(output_dir, f\"{name}.csv\")\n",
    "                obj.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "                file_type = \"csv\"\n",
    "            elif isinstance(obj, gpd.GeoDataFrame):\n",
    "                output_file = os.path.join(output_dir, f\"{name}.geojson\")\n",
    "                obj.to_file(output_file, driver=\"GeoJSON\")\n",
    "                file_type = \"geojson\"\n",
    "            else:\n",
    "                print(f\"Objeto '{name}' não é um DataFrame nem um GeoDataFrame e será ignorado.\")\n",
    "                continue\n",
    "\n",
    "            saved_objects.append(output_file)\n",
    "            print(f\"Objeto '{name}' salvo em '{output_file}'.\")\n",
    "\n",
    "            # Criar atividade de salvamento\n",
    "            exec_end = datetime.now()\n",
    "            activity_key = f\"act-save-{name}\"\n",
    "            dict_activities[activity_key] = doc_prov.activity(\n",
    "                f\"ufrj:save_{name}\", exec_start, exec_end, {\"prov:label\": f\"Salvamento do arquivo {name}\"}\n",
    "            )\n",
    "            doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "            # Criar entidade do arquivo salvo\n",
    "            entity_key = f\"ent-saved-{name}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:saved_{name}\",\n",
    "                {\n",
    "                    \"prov:label\": f\"Arquivo salvo: {name}\",\n",
    "                    \"prov:type\": f\"void:Dataset/{file_type}\",\n",
    "                    \"prov:generatedAtTime\": exec_end.isoformat(),\n",
    "                },\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "            # Relacionar com a entidade original\n",
    "            original_entity_name = entity_mapping.get(name)\n",
    "            if original_entity_name:\n",
    "                original_entity_key = f\"ent-{original_entity_name}\"\n",
    "                doc_prov.wasDerivedFrom(dict_entities[entity_key], dict_entities[original_entity_key])\n",
    "                doc_prov.used(dict_activities[activity_key], dict_entities[original_entity_key])\n",
    "            else:\n",
    "                print(f\"Warning: Sem mapeamento para a entidade original de '{name}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar '{name}': {e}\")\n",
    "\n",
    "    if saved_objects:\n",
    "        print(\"\\nObjetos saidas:\")\n",
    "        for obj in saved_objects:\n",
    "            print(f\"- {obj}\")\n",
    "    else:\n",
    "        print(\"Nenhum objeto foi salvo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2313b6cf-b51e-45b8-9d04-30e15c07ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities, datasets\n",
    "\n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "    # Carregar datasets\n",
    "    datasets = create_eda_dataset()\n",
    "\n",
    "    # Salvar datasets com rastreamento de proveniência\n",
    "    save_dataframes_with_provenance(datasets)\n",
    "\n",
    "    # Listar objetos carregados\n",
    "    print(\"\\nObjetos carregados e analisados:\")\n",
    "    for name, dataset in datasets.items():\n",
    "        if dataset is not None:\n",
    "            print(f\"- {name}: Shape {dataset.shape}\")\n",
    "\n",
    "    # Gerar saídas de proveniência\n",
    "    gerar_prov_outputs(doc_prov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "161613fe-8af3-47aa-bfe4-77609b30096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso para 'levantamento_sismico_2023'.\n",
      "Dados carregados com sucesso para 'tabela_pocos_2024'.\n",
      "Dados carregados com sucesso para 'tabela_levantamentos_geoquimica'.\n",
      "Dados carregados com sucesso para 'tabela_dados_geoquimica'.\n",
      "Dados carregados com sucesso para 'reservas_nacionais_hc'.\n",
      "Dados carregados com sucesso para 'pocos_perfurados_2023'.\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'SETORES_TODOS_SIRGAS.shp', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.shp'}], 'others': [{'name': 'SETORES_TODOS_SIRGAS.dbf', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.dbf'}, {'name': 'SETORES_TODOS_SIRGAS.prj', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.prj'}, {'name': 'SETORES_TODOS_SIRGAS.sbn', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.sbn'}, {'name': 'SETORES_TODOS_SIRGAS.sbx', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.sbx'}, {'name': 'SETORES_TODOS_SIRGAS.shx', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.shx'}, {'name': 'SETORES_TODOS_SIRGAS.xml', 'path': './dados/temp\\\\SETORES_TODOS_SIRGAS.xml'}]}\n",
      "Shapefile 'SETORES_TODOS_SIRGAS.shp' salvo em ./dados/shp_salvos\\SETORES_TODOS_SIRGAS.shp\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'bacias_gishub_db.shp', 'path': './dados/temp\\\\bacias_gishub_db.shp'}], 'others': [{'name': 'bacias_gishub_db.cst', 'path': './dados/temp\\\\bacias_gishub_db.cst'}, {'name': 'bacias_gishub_db.prj', 'path': './dados/temp\\\\bacias_gishub_db.prj'}, {'name': 'bacias_gishub_db.dbf', 'path': './dados/temp\\\\bacias_gishub_db.dbf'}, {'name': 'bacias_gishub_db.shx', 'path': './dados/temp\\\\bacias_gishub_db.shx'}, {'name': 'wfsrequest.txt', 'path': './dados/temp\\\\wfsrequest.txt'}]}\n",
      "Shapefile 'bacias_gishub_db.shp' salvo em ./dados/shp_salvos\\bacias_gishub_db.shp\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp'}], 'others': [{'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.cst', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.cst'}, {'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.prj', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.prj'}, {'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.shx', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.shx'}, {'name': 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.dbf', 'path': './dados/temp\\\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.dbf'}, {'name': 'wfsrequest.txt', 'path': './dados/temp\\\\wfsrequest.txt'}]}\n",
      "Shapefile 'BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp' salvo em ./dados/shp_salvos\\BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp\n",
      "Conteúdo do ZIP analisado: {'csv': [], 'xlsx': [], 'xls': [], 'shp': [{'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.shp', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.shp'}], 'others': [{'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.cst', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.cst'}, {'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.prj', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.prj'}, {'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.shx', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.shx'}, {'name': 'CAMPOS_PRODUCAO_SIRGASPolygon.dbf', 'path': './dados/temp\\\\CAMPOS_PRODUCAO_SIRGASPolygon.dbf'}, {'name': 'wfsrequest.txt', 'path': './dados/temp\\\\wfsrequest.txt'}]}\n",
      "Shapefile 'CAMPOS_PRODUCAO_SIRGASPolygon.shp' salvo em ./dados/shp_salvos\\CAMPOS_PRODUCAO_SIRGASPolygon.shp\n",
      "Conteúdo do ZIP analisado: {'csv': [{'name': 'consolidacao-daa-2023-calha.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-calha.csv'}, {'name': 'consolidacao-daa-2023-fluidos.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-fluidos.csv'}, {'name': 'consolidacao-daa-2023-laminas.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-laminas.csv'}, {'name': 'consolidacao-daa-2023-laterais.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-laterais.csv'}, {'name': 'consolidacao-daa-2023-plugues.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-plugues.csv'}, {'name': 'consolidacao-daa-2023-testemunhos.csv', 'path': './dados/temp\\\\consolidacao-2023/consolidacao-daa-2023-testemunhos.csv'}], 'xlsx': [], 'xls': [], 'shp': [], 'others': []}\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-calha.csv': ['Código do poço (API)', 'Identificador da caixa', 'Total de caixas', 'Profundidade topo (m)', 'Profundidade base (m)', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-calha.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-calha.csv com shape (193949, 12)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-fluidos.csv': ['Tipo de fluido', 'Profundidade topo (m)', 'Profundidade base (m)', 'Volume (L)', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-fluidos.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-fluidos.csv com shape (3697, 12)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-laminas.csv': ['Código do poço (API)', 'Tipo de amostra de origem', 'Quantidade de lâminas delgadas', 'Quantidade de lâminas bioestratigráficas', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-laminas.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-laminas.csv com shape (401955, 10)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-laterais.csv': ['Identificador da caixa', 'Total de caixas', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-laterais.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-laterais.csv com shape (75284, 9)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-plugues.csv': ['Número do testemunho', 'Profundidade do plugue (m)', 'Orientação do plugue (H ou V)', 'Identificador da caixa', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-plugues.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-plugues.csv com shape (182167, 11)\n",
      "Colunas com tipos mistos detectadas no arquivo 'consolidacao-daa-2023-testemunhos.csv': ['Número do testemunho', 'Profundidade topo (m)', 'Profundidade base (m)', 'Data de conclusão do poço']\n",
      "Tipos mistos corrigidos no arquivo 'consolidacao-daa-2023-testemunhos.csv'.\n",
      "CSV carregado: consolidacao-daa-2023-testemunhos.csv com shape (59425, 10)\n",
      "\n",
      "Datasets prontos para análise:\n",
      "- df_sismica_2023_orig: Shape (52, 15)\n",
      "- df_pocos_orig: Shape (30827, 60)\n",
      "- df_lev_geoq_2022_orig: Shape (69, 8)\n",
      "- df_geoq_2021_orig: Shape (4665, 38)\n",
      "- df_reservas_orig: Shape (419, 10)\n",
      "- df_poco_2023_orig: Shape (106, 59)\n",
      "- gdf_setores_sirgas: Shape (188, 4)\n",
      "- gdf_bacias: Shape (188, 4)\n",
      "- gdf_blocos_exploratorios: Shape (424, 15)\n",
      "- gdf_campos_producao: Shape (432, 18)\n",
      "- df_consolidacao-daa-2023-calha.csv: Shape (193949, 12)\n",
      "- df_consolidacao-daa-2023-fluidos.csv: Shape (3697, 12)\n",
      "- df_consolidacao-daa-2023-laminas.csv: Shape (401955, 10)\n",
      "- df_consolidacao-daa-2023-laterais.csv: Shape (75284, 9)\n",
      "- df_consolidacao-daa-2023-plugues.csv: Shape (182167, 11)\n",
      "- df_consolidacao-daa-2023-testemunhos.csv: Shape (59425, 10)\n",
      "Objeto 'df_sismica_2023_orig' salvo em './dados/saidas\\df_sismica_2023_orig.csv'.\n",
      "Objeto 'df_pocos_orig' salvo em './dados/saidas\\df_pocos_orig.csv'.\n",
      "Objeto 'df_lev_geoq_2022_orig' salvo em './dados/saidas\\df_lev_geoq_2022_orig.csv'.\n",
      "Objeto 'df_geoq_2021_orig' salvo em './dados/saidas\\df_geoq_2021_orig.csv'.\n",
      "Objeto 'df_reservas_orig' salvo em './dados/saidas\\df_reservas_orig.csv'.\n",
      "Objeto 'df_poco_2023_orig' salvo em './dados/saidas\\df_poco_2023_orig.csv'.\n",
      "Objeto 'gdf_setores_sirgas' salvo em './dados/saidas\\gdf_setores_sirgas.csv'.\n",
      "Objeto 'gdf_bacias' salvo em './dados/saidas\\gdf_bacias.csv'.\n",
      "Objeto 'gdf_blocos_exploratorios' salvo em './dados/saidas\\gdf_blocos_exploratorios.csv'.\n",
      "Objeto 'gdf_campos_producao' salvo em './dados/saidas\\gdf_campos_producao.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-calha.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-calha.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-fluidos.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-fluidos.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-laminas.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-laminas.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-laterais.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-laterais.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-plugues.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-plugues.csv.csv'.\n",
      "Objeto 'df_consolidacao-daa-2023-testemunhos.csv' salvo em './dados/saidas\\df_consolidacao-daa-2023-testemunhos.csv.csv'.\n",
      "\n",
      "Objetos saidas:\n",
      "- ./dados/saidas\\df_sismica_2023_orig.csv\n",
      "- ./dados/saidas\\df_pocos_orig.csv\n",
      "- ./dados/saidas\\df_lev_geoq_2022_orig.csv\n",
      "- ./dados/saidas\\df_geoq_2021_orig.csv\n",
      "- ./dados/saidas\\df_reservas_orig.csv\n",
      "- ./dados/saidas\\df_poco_2023_orig.csv\n",
      "- ./dados/saidas\\gdf_setores_sirgas.csv\n",
      "- ./dados/saidas\\gdf_bacias.csv\n",
      "- ./dados/saidas\\gdf_blocos_exploratorios.csv\n",
      "- ./dados/saidas\\gdf_campos_producao.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-calha.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-fluidos.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-laminas.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-laterais.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-plugues.csv.csv\n",
      "- ./dados/saidas\\df_consolidacao-daa-2023-testemunhos.csv.csv\n",
      "\n",
      "Objetos carregados e analisados:\n",
      "- df_sismica_2023_orig: Shape (52, 15)\n",
      "- df_pocos_orig: Shape (30827, 60)\n",
      "- df_lev_geoq_2022_orig: Shape (69, 8)\n",
      "- df_geoq_2021_orig: Shape (4665, 38)\n",
      "- df_reservas_orig: Shape (419, 10)\n",
      "- df_poco_2023_orig: Shape (106, 59)\n",
      "- gdf_setores_sirgas: Shape (188, 4)\n",
      "- gdf_bacias: Shape (188, 4)\n",
      "- gdf_blocos_exploratorios: Shape (424, 15)\n",
      "- gdf_campos_producao: Shape (432, 18)\n",
      "- df_consolidacao-daa-2023-calha.csv: Shape (193949, 12)\n",
      "- df_consolidacao-daa-2023-fluidos.csv: Shape (3697, 12)\n",
      "- df_consolidacao-daa-2023-laminas.csv: Shape (401955, 10)\n",
      "- df_consolidacao-daa-2023-laterais.csv: Shape (75284, 9)\n",
      "- df_consolidacao-daa-2023-plugues.csv: Shape (182167, 11)\n",
      "- df_consolidacao-daa-2023-testemunhos.csv: Shape (59425, 10)\n",
      "Provenance graph generated successfully: EDA-PROV.png\n",
      "Provenance serialized as XML and TTL.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b5afe-5acb-413a-a317-b99a66bbf4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOCO DE ANÁLISE DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf3cc7-420c-484c-bf04-88e8e134fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_datasets(datasets):\n",
    "\n",
    "    #Itera sobre todos os DataFrames e GeoDataFrames do conjunto de datasets,\n",
    "    #imprimindo as colunas e os tipos de dados.\n",
    "\n",
    "    for name, dataset in datasets.items():\n",
    "        if isinstance(dataset, pd.DataFrame):\n",
    "            print(f\"DataFrame: {name}\")\n",
    "            print(\"Columns:\")\n",
    "            print(dataset.columns.tolist())\n",
    "            print(\"Dtypes:\")\n",
    "            print(dataset.dtypes)\n",
    "            print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "        elif isinstance(dataset, gpd.GeoDataFrame):\n",
    "            print(f\"GeoDataFrame: {name}\")\n",
    "            print(\"Columns:\")\n",
    "            print(dataset.columns.tolist())\n",
    "            print(\"Dtypes:\")\n",
    "            print(dataset.dtypes)\n",
    "            print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Objeto '{name}' não é um DataFrame nem um GeoDataFrame. Ignorando...\\n\")\n",
    "\n",
    "# Chamar a função após carregar os datasets na função `main`\n",
    "# Isso carrega os datasets usando a lógica implementada no `main`\n",
    "\n",
    "# Analisar os datasets carregados\n",
    "print(\"Análise Básica dos Datasets:\")\n",
    "analyze_datasets(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd681c-db09-4eb4-a654-9f9bcea41f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanitize_and_process_all(output_dir=\"./dados/saneados\", input_dir=\"./dados/saidas\"):\n",
    "    \"\"\"\n",
    "    Processa e saneia todos os datasets fornecidos, aplicando os tipos de dados especificados,\n",
    "    removendo entradas inválidas e salvando os resultados.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    dataset_paths = {\n",
    "    \"df_sismica_2023_orig\": os.path.join(input_dir, \"df_sismica_2023_orig.csv\"),\n",
    "    \"df_pocos_orig\": os.path.join(input_dir, \"df_pocos_orig.csv\"),\n",
    "    \"df_lev_geoq_2022_orig\": os.path.join(input_dir, \"df_lev_geoq_2022_orig.csv\"),\n",
    "    \"df_geoq_2021_orig\": os.path.join(input_dir, \"df_geoq_2021_orig.csv\"),\n",
    "    \"df_reservas_orig\": os.path.join(input_dir, \"df_reservas_orig.csv\"),\n",
    "    \"df_poco_2023_orig\": os.path.join(input_dir, \"df_poco_2023_orig.csv\"),\n",
    "    \"gdf_setores_sirgas\": os.path.join(input_dir, \"gdf_setores_sirgas.csv\"),\n",
    "    \"gdf_blocos_exploratorios\": os.path.join(input_dir, \"gdf_blocos_exploratorios.csv\"),\n",
    "    \"gdf_campos_producao\": os.path.join(input_dir, \"gdf_campos_producao.csv\"),\n",
    "    \"df_consolidacao-daa-2023-calha.csv\": os.path.join(input_dir, \"df_consolidacao-daa-2023-calha.csv.csv\"),\n",
    "    \"df_consolidacao-daa-2023-fluidos.csv\": os.path.join(input_dir, \"df_consolidacao-daa-2023-fluidos.csv.csv\"),\n",
    "    \"df_consolidacao-daa-2023-laminas.csv\": os.path.join(input_dir, \"df_consolidacao-daa-2023-laminas.csv.csv\"),\n",
    "    \"df_consolidacao-daa-2023-laterais.csv\": os.path.join(input_dir, \"df_consolidacao-daa-2023-laterais.csv.csv\"),\n",
    "    \"df_consolidacao-daa-2023-plugues.csv\": os.path.join(input_dir, \"df_consolidacao-daa-2023-plugues.csv.csv\"),\n",
    "    \"df_consolidacao-daa-2023-testemunhos.csv\": os.path.join(input_dir, \"df_consolidacao-daa-2023-testemunhos.csv.csv\"),\n",
    "}\n",
    "\n",
    "\n",
    "    # Registrar log de processamento\n",
    "    log = []\n",
    "    log.append(f\"Início do processamento: {datetime.now()}\\n\")\n",
    "\n",
    "    datasets = {}\n",
    "    for name, path in dataset_paths.items():\n",
    "        if not os.path.exists(path):\n",
    "            log.append(f\"Arquivo '{path}' não encontrado. Ignorando o dataset '{name}'.\")\n",
    "            continue\n",
    "        try:\n",
    "            if path.endswith(\".csv\"):\n",
    "                datasets[name] = pd.read_csv(path, sep=\";\", skiprows=2 if \"consolidacao\" in name else 0)\n",
    "            elif path.endswith(\".shp\"):\n",
    "                datasets[name] = gpd.read_file(path)\n",
    "        except Exception as e:\n",
    "            log.append(f\"Erro ao carregar o arquivo '{path}' para o dataset '{name}': {e}\")\n",
    "\n",
    "    # Definir tipos esperados para cada dataset\n",
    "    type_definitions = {\n",
    "    \"df_sismica_2023_orig\": {\n",
    "        \"Nome\": \"string\",\n",
    "        \"Categoria\": \"string\",\n",
    "        \"Tipo\": \"string\",\n",
    "        \"Autorização\": \"string\",\n",
    "        \"Inicio\": \"date\",\n",
    "        \"Término Real\": \"date\",\n",
    "        \"Confidencialidade\": \"string\",\n",
    "        \"Público em\": \"date\",\n",
    "        \"Ato Normativo\": \"string\",\n",
    "        \"Tecnologia\": \"string\",\n",
    "        \"Bloco\": \"string\",\n",
    "        \"Campo\": \"string\",\n",
    "        \"BACIA\": \"string\",\n",
    "        \"E.A.D\": \"string\",\n",
    "        \"OPERADORA\": \"string\",\n",
    "    },\n",
    "    \"df_pocos_orig\": {\n",
    "        \"POCO\": \"string\",\n",
    "        \"CADASTRO\": \"int64\",\n",
    "        \"OPERADOR\": \"string\",\n",
    "        \"POCO_OPERADOR\": \"string\",\n",
    "        \"ESTADO\": \"string\",\n",
    "        \"BACIA\": \"string\",\n",
    "        \"BLOCO\": \"string\",\n",
    "        \"SIG_CAMPO\": \"string\",\n",
    "        \"CAMPO\": \"string\",\n",
    "        \"TERRA_MAR\": \"string\",\n",
    "        \"POCO_POS_ANP\": \"string\",\n",
    "        \"TIPO\": \"int64\",\n",
    "        \"CATEGORIA\": \"string\",\n",
    "        \"RECLASSIFICACAO\": \"string\",\n",
    "        \"SITUACAO\": \"string\",\n",
    "        \"INÍCIO\": \"date\",\n",
    "        \"TÉRMINO\": \"date\",\n",
    "        \"CONCLUSAO\": \"date\",\n",
    "        \"TITULARIDADE\": \"object\",\n",
    "        \"LATITUDE_BASE_4C\": \"string\",\n",
    "        \"LONGITUDE_BASE_4C\": \"string\",\n",
    "        \"LATITUDE_BASE_DD\": \"string\",\n",
    "        \"LONGITUDE_BASE_DD\": \"string\",\n",
    "        \"DATUM_HORIZONTAL\": \"object\",\n",
    "        \"TIPO_DE_COORDENADA_DE_BASE\": \"object\",\n",
    "        \"DIRECAO\": \"object\",\n",
    "        \"PROFUNDIDADE_VERTICAL_M\": \"float64\",\n",
    "        \"PROFUNDIDADE_SONDADOR_M\": \"float64\",\n",
    "        \"PROFUNDIDADE_MEDIDA_M\": \"float64\",\n",
    "        \"REFERENCIA_DE_PROFUNDIDADE\": \"string\",\n",
    "        \"MESA_ROTATIVA\": \"float64\",\n",
    "        \"COTA_ALTIMETRICA_M\": \"float64\",\n",
    "        \"LAMINA_D_AGUA_M\": \"float64\",\n",
    "        \"DATUM_VERTICAL\": \"string\",\n",
    "        \"UNIDADE_ESTRATIGRAFICA\": \"string\",\n",
    "        \"GEOLOGIA_GRUPO_FINAL\": \"string\",\n",
    "        \"GEOLOGIA_FORMACAO_FINAL\": \"string\",\n",
    "        \"GEOLOGIA_MEMBRO_FINAL\": \"string\",\n",
    "        \"CDPE\": \"string\",\n",
    "        \"AGP\": \"string\",\n",
    "        \"PC\": \"string\",\n",
    "        \"PAG\": \"string\",\n",
    "        \"PERFIS_CONVENCIONAIS\": \"string\",\n",
    "        \"DURANTE_PERFURACAO\": \"string\",\n",
    "        \"PERFIS_DIGITAIS\": \"string\",\n",
    "        \"PERFIS_PROCESSADOS\": \"string\",\n",
    "        \"PERFIS_ESPECIAIS\": \"string\",\n",
    "        \"AMOSTRA_LATERAL\": \"string\",\n",
    "        \"SISMICA\": \"string\",\n",
    "        \"TABELA_TEMPO_PROFUNDIDADE\": \"string\",\n",
    "        \"DADOS_DIRECIONAIS\": \"string\",\n",
    "        \"TESTE_A_CABO\": \"string\",\n",
    "        \"TESTE_DE_FORMACAO\": \"string\",\n",
    "        \"CANHONEIO\": \"string\",\n",
    "        \"TESTEMUNHO\": \"string\",\n",
    "        \"GEOQUIMICA\": \"string\",\n",
    "        \"SIG_SONDA\": \"string\",\n",
    "        \"NOM_SONDA\": \"string\",\n",
    "        \"ATINGIU_PRESAL\": \"string\",\n",
    "        \"DHA_ATUALIZACAO\": \"date\",\n",
    "    },\n",
    "    \"df_lev_geoq_2022_orig\": {\n",
    "        \"Nome\": \"string\",\n",
    "        \"BACIA\": \"string\",\n",
    "        \"Natureza da atividade\": \"string\",\n",
    "        \"Início Aquisição\": \"date\",\n",
    "        \"Término Aquisição\": \"date\",\n",
    "        \"Ambiente\": \"string\",\n",
    "        \"Confidencialidade\": \"string\",\n",
    "        \"Shape\": \"object\",\n",
    "    },\n",
    "    \"df_geoq_2021_orig\": {\n",
    "        \"Poço\": \"string\",\n",
    "        \"Extensão do Arquivo\": \"string\",\n",
    "        \"Fonte dos dados\": \"string\",\n",
    "        \"Tipo de Geoquímica\": \"string\",\n",
    "        \"Topo (m)\": \"float64\",\n",
    "        \"Base (m)\": \"float64\",\n",
    "        \"Cromatografia Gasosa\": \"string\",\n",
    "        \"Cromatografia Líquida\": \"string\",\n",
    "        \"Relatório\": \"string\",\n",
    "        \"Análise de Óleo\": \"string\",\n",
    "        \"Análise de Gás\": \"string\",\n",
    "        \"Análise de Fluidos\": \"string\",\n",
    "        \"Análise de Água\": \"string\",\n",
    "        \"Biomarcadores\": \"string\",\n",
    "        \"Aromáticos\": \"string\",\n",
    "        \"Saturados\": \"string\",\n",
    "        \"Isótopos\": \"string\",\n",
    "        \"Espectrometria de Massas\": \"string\",\n",
    "        \"Diamantoides\": \"string\",\n",
    "        \"Carbono Orgânico Total\": \"string\",\n",
    "        \"Pirólise\": \"object\",\n",
    "        \"Inclusões Fluidas\": \"string\",\n",
    "        \"Sumário de Óleo Cru\": \"string\",\n",
    "        \"PVT\": \"string\",\n",
    "        \"Re/Os de Asfaltenos\": \"string\",\n",
    "        \"Litogeoquímica\": \"string\",\n",
    "        \"Reflectância de Vitrinita\": \"string\",\n",
    "        \"Viscosidade\": \"string\",\n",
    "        \"CADASTRO\": \"int64\",\n",
    "        \"OPERADOR\": \"string\",\n",
    "        \"ESTADO\": \"string\",\n",
    "        \"BACIA\": \"string\",\n",
    "        \"CAMPO\": \"string\",\n",
    "        \"TITULARIDADE\": \"string\",\n",
    "        \"LATITUDE\": \"string\",\n",
    "        \"LONGITUDE\": \"string\",\n",
    "        \"DATUM\": \"string\",\n",
    "        \"DIRECAO\": \"string\",\n",
    "    },\n",
    "    \"df_reservas_orig\": {\n",
    "        \"Ano\": \"int64\",\n",
    "        \"Campo/Área de desenvolvimento\": \"string\",\n",
    "        \"BACIA\": \"string\",\n",
    "        \"ESTADO\": \"string\",\n",
    "        \"VOIP (bbl)\": \"float64\",\n",
    "        \"VGIP (m³)\": \"float64\",\n",
    "        \"Petróleo Acumulado (bbl)\": \"float64\",\n",
    "        \"Gás Natural Acumulado\": \"float64\",\n",
    "        \"Fração Recuperada de Petróleo\": \"float64\",\n",
    "        \"Situação\": \"string\",\n",
    "    },\n",
    "    \"df_POCO_2023_orig\": {\n",
    "        \"POCO\": \"string\",\n",
    "        \"CADASTRO\": \"int64\",\n",
    "        \"OPERADOR\": \"string\",\n",
    "        \"POCO_OPERADOR\": \"string\",\n",
    "        \"ESTADO\": \"string\",\n",
    "        \"BACIA\": \"string\",\n",
    "        \"BLOCO\": \"string\",\n",
    "        \"SIG_CAMPO\": \"string\",\n",
    "        \"CAMPO\": \"string\",\n",
    "        \"TERRA_MAR\": \"string\",\n",
    "        \"POCO_POS_ANP\": \"string\",\n",
    "        \"TIPO\": \"int64\",\n",
    "        \"CATEGORIA\": \"string\",\n",
    "        \"RECLASSIFICACAO\": \"string\",\n",
    "        \"SITUACAO\": \"string\",\n",
    "        \"INICIO\": \"date\",\n",
    "        \"TERMINO\": \"date\",\n",
    "        \"CONCLUSAO\": \"date\",\n",
    "        \"LATITUDE_BASE_4C\": \"string\",\n",
    "        \"LONGITUDE_BASE_4C\": \"string\",\n",
    "        \"LATITUDE_BASE_DD\": \"string\",\n",
    "        \"LONGITUDE_BASE_DD\": \"string\",\n",
    "        \"DATUM_HORIZONTAL\": \"string\",\n",
    "        \"TIPO_DE_COORDENADA_DE_BASE\": \"string\",\n",
    "        \"DIRECAO\": \"string\",\n",
    "        \"PROFUNDIDADE_VERTICAL_M\": \"float64\",\n",
    "        \"PROFUNDIDADE_SONDADOR_M\": \"float64\",\n",
    "        \"PROFUNDIDADE_MEDIDA_M\": \"float64\",\n",
    "        \"REFERENCIA_DE_PROFUNDIDADE\": \"string\",\n",
    "        \"MESA_ROTATIVA\": \"float64\",\n",
    "        \"COTA_ALTIMETRICA_M\": \"float64\",\n",
    "        \"LAMINA_D_AGUA_M\": \"float64\",\n",
    "        \"DATUM_VERTICAL\": \"string\",\n",
    "        \"UNIDADE_ESTRATIGRAFICA\": \"float64\",\n",
    "        \"GEOLOGIA_GRUPO_FINAL\": \"string\",\n",
    "        \"GEOLOGIA_FORMACAO_FINAL\": \"string\",\n",
    "        \"GEOLOGIA_MEMBRO_FINAL\": \"string\",\n",
    "        \"CDPE\": \"string\",\n",
    "        \"AGP\": \"string\",\n",
    "        \"PC\": \"string\",\n",
    "        \"PAG\": \"string\",\n",
    "        \"PERFIS_CONVENCIONAIS\": \"string\",\n",
    "        \"DURANTE_PERFURACAO\": \"string\",\n",
    "        \"PERFIS_DIGITAIS\": \"string\",\n",
    "        \"PERFIS_PROCESSADOS\": \"string\",\n",
    "        \"PERFIS_ESPECIAIS\": \"string\",\n",
    "        \"AMOSTRA_LATERAL\": \"string\",\n",
    "        \"SISMICA\": \"string\",\n",
    "        \"TABELA_TEMPO_PROFUNDIDADE\": \"string\",\n",
    "        \"DADOS_DIRECIONAIS\": \"string\",\n",
    "        \"TESTE_A_CABO\": \"string\",\n",
    "        \"TESTE_DE_FORMACAO\": \"string\",\n",
    "        \"CANHONEIO\": \"string\",\n",
    "        \"TESTEMUNHO\": \"string\",\n",
    "        \"GEOQUIMICA\": \"string\",\n",
    "        \"SIG_SONDA\": \"string\",\n",
    "        \"NOM_SONDA\": \"string\",\n",
    "        \"ATINGIU_PRESAL\": \"string\",\n",
    "        \"DHA_ATUALIZACAO\": \"date\",\n",
    "    },\n",
    "    \"gdf_setores_sirgas\": {\n",
    "        \"ID1\": \"int64\",\n",
    "        \"BACIA\": \"string\",\n",
    "        \"Nome_SETOR\": \"string\",\n",
    "        \"geometry\": \"geometry\",\n",
    "    },\n",
    "    \"gdf_blocos_exploratorios\": {\n",
    "        \"COD_BLOCO\": \"string\",\n",
    "        \"COD_FASE_C\": \"string\",\n",
    "        \"DAT_ASSINA\": \"date\",\n",
    "        \"DAT_TERMIN\": \"date\",\n",
    "        \"NOM_BACIA\": \"string\",\n",
    "        \"NOM_BLOCO\": \"string\",\n",
    "        \"NOM_FANTAS\": \"string\",\n",
    "        \"NUM_CONTRA\": \"string\",\n",
    "        \"NUM_DESCOB\": \"string\",\n",
    "        \"OPERADOR_C\": \"object\",\n",
    "        \"RODADA\": \"object\",\n",
    "        \"AREA_TOTAL\": \"float64\",\n",
    "        \"AMBIENTE\": \"string\",\n",
    "        \"BLOCOS\": \"string\",\n",
    "        \"geometry\": \"geometry\",\n",
    "    },\n",
    "    \"gdf_campos_producao\": {\n",
    "        \"NUM_RODADA\": \"string\",\n",
    "        \"NOM_CAMPO\": \"string\",\n",
    "        \"AREA\": \"float64\",\n",
    "        \"OPERADOR_C\": \"string\",\n",
    "        \"NUM_CONTRA\": \"string\",\n",
    "        \"DAT_ASSINA\": \"date\",\n",
    "        \"DAT_TERMIN\": \"date\",\n",
    "        \"NOM_BACIA\": \"string\",\n",
    "        \"COD_CAMPO\": \"string\",\n",
    "        \"SIG_CAMPO\": \"string\",\n",
    "        \"DAT_DESCOB\": \"date\",\n",
    "        \"DAT_INICIO\": \"date\",\n",
    "        \"ETAPA\": \"string\",\n",
    "        \"MED_LAMINA\": \"float64\",\n",
    "        \"FLUIDO_PRI\": \"string\",\n",
    "        \"ID\": \"int64\",\n",
    "        \"AMBIENTE\": \"string\",\n",
    "        \"geometry\": \"geometry\",\n",
    "    },\n",
    "    \"df_consolidacao_daa_2023_calha.csv\": {\n",
    "        \"BACIA Sedimentar\": \"string\",\n",
    "        \"Código do poço (API)\": \"string\",\n",
    "        \"Nome do poço (ANP)\": \"string\",\n",
    "        \"Identificador da caixa\": \"string\",\n",
    "        \"Total de caixas\": \"int64\",\n",
    "        \"Profundidade topo (m)\": \"float64\",\n",
    "        \"Profundidade base (m)\": \"float64\",\n",
    "        \"Data de conclusão do poço\": \"string\",\n",
    "        \"Município/ESTADO\": \"string\",\n",
    "        \"Depositária\": \"string\",\n",
    "        \"Unnamed:10\": \"object\",\n",
    "        \"Unnamed:11\": \"object\",\n",
    "    },\n",
    "    \"df_consolidacao_daa_2023_fluidos.csv\": {\n",
    "        \"Bacia sedimentar\": \"string\",\n",
    "        \"Código do poço (API)\": \"int64\",\n",
    "        \"Nome do poço (ANP)\": \"string\",\n",
    "        \"Tipo de fluido\": \"string\",\n",
    "        \"Profundidade topo (m)\": \"string\",\n",
    "        \"Profundidade base (m)\": \"string\",\n",
    "        \"Quantidade de amostra\": \"int64\",\n",
    "        \"Tipo de container\": \"string\",\n",
    "        \"Volume (L)\": \"string\",\n",
    "        \"Data de conclusão do poço\": \"date\",\n",
    "        \"Município/ESTADO\": \"string\",\n",
    "        \"Depositária\": \"string\",\n",
    "    },\n",
    "    \"df_consolidacao_daa_2023_laminas.csv\": {\n",
    "        \"BACIA Sedimentar\": \"string\",\n",
    "        \"Código do poço (API)\": \"int64\",\n",
    "        \"Nome do poço (ANP)\": \"string\",\n",
    "        \"Profundidade da lâmina (m)\": \"float64\",\n",
    "        \"Tipo de amostra de origem\": \"string\",\n",
    "        \"Quantidade de lâminas delgadas\": \"int64\",\n",
    "        \"Quantidade de lâminas bioestratigráficas\": \"int64\",\n",
    "        \"Data de conclusão do poço\": \"date\",\n",
    "        \"Município/ESTADO\": \"string\",\n",
    "        \"Depositária\": \"string\",\n",
    "    },\n",
    "    \"df_consolidacao_daa_2023_laterais.csv\": {\n",
    "        \"BACIA sedimentar\": \"string\",\n",
    "        \"Código do poço (API)\": \"int64\",\n",
    "        \"Nome do poço (ANP)\": \"string\",\n",
    "        \"Profundidade (m)\": \"float64\",\n",
    "        \"Identificador da caixa\": \"string\",\n",
    "        \"Total de caixas\": \"int64\",\n",
    "        \"Data de conclusão do poço\": \"date\",\n",
    "        \"Município/ESTADO\": \"string\",\n",
    "        \"Depositária\": \"string\",\n",
    "    },\n",
    "    \"df_consolidacao_daa_2023_plugues.csv\": {\n",
    "        \"BACIA sedimentar\": \"string\",\n",
    "        \"Código do poço (API)\": \"int64\",\n",
    "        \"Nome do poço (ANP)\": \"string\",\n",
    "        \"Número do testemunho\": \"int64\",\n",
    "        \"Profundidade do plugue (m)\": \"float64\",\n",
    "        \"Orientação do plugue (H ou V)\": \"string\",\n",
    "        \"Identificador da caixa\": \"string\",\n",
    "        \"Total de caixas\": \"int64\",\n",
    "        \"Data de conclusão do poço\": \"date\",\n",
    "        \"Município/ESTADO\": \"object\",\n",
    "        \"Depositária\": \"object\",\n",
    "    },\n",
    "    \"df_consolidacao_daa_2023_testemunhos.csv\": {\n",
    "        \"BACIA sedimentar\": \"string\",\n",
    "        \"Código do poço (API)\": \"int64\",\n",
    "        \"Nome do poço (ANP)\": \"string\",\n",
    "        \"Número do testemunho\": \"int64\",\n",
    "        \"Total de caixas\": \"int64\",\n",
    "        \"Profundidade topo (m)\": \"float64\",\n",
    "        \"Profundidade base (m)\": \"float64\",\n",
    "        \"Data de conclusão do poço\": \"date\",\n",
    "        \"Município/ESTADO\": \"string\",\n",
    "        \"Depositária\": \"string\",\n",
    "    },\n",
    "    }\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        log.append(f\"\\nProcessando dataset '{name}'...\")\n",
    "        if name not in type_definitions:\n",
    "            log.append(f\"  Sem definições de tipo para '{name}'. Ignorando...\")\n",
    "            continue\n",
    "\n",
    "        expected_types = type_definitions.get(name, {})\n",
    "        original_shape = df.shape\n",
    "\n",
    "        for col, expected_type in expected_types.items():\n",
    "            if col not in df.columns:\n",
    "                log.append(f\"  Coluna '{col}' não encontrada. Ignorando...\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if expected_type == \"string\":\n",
    "                    df[col] = df[col].astype(str)\n",
    "                elif expected_type == \"float64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                elif expected_type == \"int64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\", downcast=\"integer\")\n",
    "                elif expected_type == \"date\":\n",
    "                    df[col] = pd.to_datetime(df[col], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "            except Exception as e:\n",
    "                log.append(f\"  Erro ao converter coluna '{col}' para {expected_type}: {e}\")\n",
    "\n",
    "        log.append(f\"  Shape original: {original_shape}\")\n",
    "        log.append(f\"  Shape final: {df.shape}\")\n",
    "\n",
    "        # Salvar dataset saneado\n",
    "        output_file = os.path.join(output_dir, f\"{name}_sn.csv\")\n",
    "        if isinstance(df, gpd.GeoDataFrame):\n",
    "            output_file = os.path.join(output_dir, f\"{name}_sn.shp\")\n",
    "            df.to_file(output_file)\n",
    "        else:\n",
    "            df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "        log.append(f\"  Dataset '{name}' saneado salvo em: {output_file}\")\n",
    "\n",
    "    # Salvar log\n",
    "    log_file = os.path.join(output_dir, \"saneamento_log.txt\")\n",
    "    with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(log))\n",
    "\n",
    "    print(\"\\n\".join(log))\n",
    "\n",
    "# Executar a função\n",
    "sanitize_and_process_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec654d7-1cbf-4e97-b9d3-b10d20a532b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422bb87e-4ebb-4d88-bdf0-79cb3b2981eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3c054-4ba5-4f4e-85f0-d6ceaa577d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9351a4-71b0-45a8-b02a-a29f6953997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOCO DE VISUALIZAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ceb29c-7c33-43a2-b265-91c43ac7cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÃO EM USO\n",
    "def plot_shapefiles_with_basemap(geodataframes, output_dir=\"./dados/plots\"):\n",
    "    \"\"\"\n",
    "    Plota os GeoDataFrames fornecidos em mapas separados com um mapa base da América do Sul.\n",
    "\n",
    "    Args:\n",
    "        geodataframes (dict): Dicionário contendo GeoDataFrames a serem plotados.\n",
    "        output_dir (str): Diretório para salvar os plots.\n",
    "    \"\"\"\n",
    "    # Criar o diretório de saída para salvar os plots\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterar sobre os GeoDataFrames e plotar\n",
    "    for name, gdf in geodataframes.items():\n",
    "        # Configurar a figura\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.set_title(f\"Mapa de {name}\", fontsize=15)\n",
    "\n",
    "        # Garantir que o CRS esteja no formato Web Mercator (EPSG:3857)\n",
    "        if gdf.crs and gdf.crs.to_string() != \"EPSG:3857\":\n",
    "            gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "        # Plotar o GeoDataFrame\n",
    "        gdf.plot(ax=ax, alpha=0.7, edgecolor=\"black\", facecolor=\"none\")\n",
    "\n",
    "        # Adicionar o mapa base com Contextily (usando OpenStreetMap como provedor)\n",
    "        ctx.add_basemap(\n",
    "            ax,\n",
    "            source=ctx.providers.OpenStreetMap.Mapnik,\n",
    "            zoom=5,\n",
    "            attribution_size=6\n",
    "        )\n",
    "\n",
    "        # Ajustar os limites do mapa para o GeoDataFrame\n",
    "        ax.set_xlim(gdf.total_bounds[0], gdf.total_bounds[2])\n",
    "        ax.set_ylim(gdf.total_bounds[1], gdf.total_bounds[3])\n",
    "\n",
    "        # Salvar o plot no diretório de saída\n",
    "        output_file = os.path.join(output_dir, f\"{name}_map.png\")\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Mapa salvo em: {output_file}\")\n",
    "\n",
    "        # Exibir o mapa\n",
    "        plt.show()\n",
    "\n",
    "# Carregar os shapefiles previamente salvos\n",
    "gdf_setores = gpd.read_file(\"./dados/shp_salvos/SETORES_TODOS_SIRGAS.shp\")\n",
    "gdf_blocos = gpd.read_file(\"./dados/shp_salvos/BLOCOS_EXPLORATORIOS_SIRGASPolygon.shp\")\n",
    "gdf_campos = gpd.read_file(\"./dados/shp_salvos/CAMPOS_PRODUCAO_SIRGASPolygon.shp\")\n",
    "\n",
    "# Dicionário com os GeoDataFrames\n",
    "geodataframes = {\n",
    "    \"Setores_SIRGAS\": gdf_setores,\n",
    "    \"Blocos_Exploratorios\": gdf_blocos,\n",
    "    \"Campos_Producao\": gdf_campos\n",
    "}\n",
    "\n",
    "# Chamar a função para plotar os mapas\n",
    "plot_shapefiles_with_basemap(geodataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98abfb-f5fb-434d-ba8c-13d245568fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaafbe4-2a12-4d0c-ae6b-fde02228c54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6b511-0820-4441-9e09-e58a8e2dbcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d406e0c0-1b35-468d-841f-3e08bfec7676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440471a8-cc2e-421e-9171-d296659cdb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bda194-b2b5-4fbd-aa29-9ed90f93c87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970ee26-8203-4436-b8cf-f4cb5213e0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bc7ba-96a5-4045-aeef-6d2e40631b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d1dad-241f-4963-9b9b-366ac685d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6819c-aab9-46b9-a8fa-27eebe8a99c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74c138-fbb6-4b29-9aad-cc19bc0dd3a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"def inspect_dataframes(dataframes):\n",
    "\n",
    "    #Inspeciona um dicionário de DataFrames, exibindo colunas, shape e estatísticas descritivas.\n",
    "\n",
    "    for name, df in dataframes.items():\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(\"\\nHead:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nDescribe:\")\n",
    "        print(df.describe(include='all', datetime_is_numeric=True))\n",
    "\n",
    "    def clean_dataframe_columns(dataframes):\n",
    "\n",
    "    #Realiza o saneamento e a renomeação de colunas em um conjunto de DataFrames.\n",
    "\n",
    "    cleaned_dataframes = {}\n",
    "    for name, df in dataframes.items():\n",
    "        # Renomear colunas removendo espaços e convertendo para letras minúsculas\n",
    "        df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "        # Registrar atividade de saneamento na proveniência\n",
    "        execStartTime = datetime.now()\n",
    "        activity_key = f\"act-saneamento-{name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:saneamento_{name}\", execStartTime, None, {\n",
    "            \"prov:label\": escape_label(f\"Saneamento de colunas: {name}\")\n",
    "        })\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        entity_key = f\"ent-saneado-{name}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{name}_saneado\", {\n",
    "            \"prov:label\": escape_label(f\"Dataset saneado: {name}\"),\n",
    "            \"prov:type\": \"void:Dataset\",\n",
    "        })\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "        cleaned_dataframes[name] = df\n",
    "    return cleaned_dataframes\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

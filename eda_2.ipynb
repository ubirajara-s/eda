{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "5f14207f-be28-4283-9314-1e02e715db8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fundamentos de Ciência de Dados\n",
    "## PPGI/UFRJ 2024.2\n",
    "### Profs Sergio Serra e Jorge Zavaleta\n",
    "### Aluno Ubirajara S. Santos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "21fc0eda-8216-4ae5-ba67-9a060afea2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prov\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "4ac38fd8-c6aa-4138-ae54-2a103a3f52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './dados/saidas'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "38838849-a486-40fb-a8c1-35e945ee947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fontes de Dados\n",
    "data_sources = {\n",
    "     \"amostras_rochas_fluidos\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-amostras-de-rochas-e-fluidos/acervo-de-amostras/consolidacao-2023.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"setores_sirgas\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/assuntos/exploracao-e-producao-de-oleo-e-gas/estudos-geologicos-e-geofisicos/arquivos-classificacao-de-modelos-exploratorios/setores-sirgas.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"blocos_exploratorios\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ABLOCOS_EXPLORATORIOS_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"campos_producao\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ACAMPOS_PRODUCAO_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"reservas_nacionais_hc\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-estatisticos/arquivos-reservas-nacionais-de-petroleo-e-gas-natural/tabela-dados-bar-2023.xlsx\",\n",
    "        \"type\": \"xlsx\"},\n",
    "     \"pocos_perfurados_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/pocos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_levantamentos_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-levantamentos-geoquimicos.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"levantamento_sismico_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/sismicos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_pocos_2024\": {\n",
    "        \"url\": \"./dados/entradas/Tabela_pocos_2024_Novembro_24.csv\",\n",
    "        \"type\": \"csv\", \"sep\": \";\" ,\"encoding\": \"ANSI\"},\n",
    "     \"tabela_dados_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-dados-geoquimicos.csv\",\n",
    "        \"type\": \"csv\",\n",
    "        \"header\": 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "d2c3e5eb-413c-433b-ad53-5bfb214de691",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, datetime\n",
    "from prov.model import ProvDocument, Namespace\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "import plotly\n",
    "import graphviz\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import importlib.metadata\n",
    "\n",
    "def gerar_prov_outputs(doc_prov):\n",
    "    entity = \"EDA-PROV\"\n",
    "    output_file = f\"{entity}.png\"\n",
    "    try:\n",
    "        dot = prov_to_dot(doc_prov)\n",
    "        # Write to PNG\n",
    "        dot.write_png(output_file)\n",
    "        print(f\"Provenance graph generated successfully: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating provenance graph: {e}\")\n",
    "        # Save the DOT file for debugging\n",
    "        with open(\"debug.dot\", \"w\") as f:\n",
    "            f.write(dot.to_string())\n",
    "        print(\"Saved DOT file for debugging as 'debug.dot'.\")\n",
    "   \n",
    "    # Serialização do documento\n",
    "    doc_prov.serialize(entity + \".xml\", format='xml') \n",
    "    doc_prov.serialize(entity + \".ttl\", format='rdf', rdf_format='ttl',encoding=\"utf-8\")\n",
    "    print(\"Provenance serialized as XML and TTL.\")\n",
    "    \n",
    "\n",
    "def adding_namespaces(document_prov):\n",
    "    # Adiciona namespaces ao documento de proveniência.\n",
    "    document_prov.add_namespace('void', 'http://vocab.deri.ie/void#')\n",
    "    document_prov.add_namespace('ufrj', 'https://www.ufrj.br')\n",
    "    document_prov.add_namespace('schema', 'http://schema.org/')    # Dados estruturados Schema.org\n",
    "    document_prov.add_namespace('prov', 'http://www.w3.org/ns/prov#')     # Padrões PROV\n",
    "    document_prov.add_namespace('foaf', 'http://xmlns.com/foaf/0.1/')     # Agentes FOAF\n",
    "    document_prov.add_namespace('ufrj-ppgi', 'http://www.ufrj.br/ppgi/')  # UFRJ PPGI\n",
    "    document_prov.add_namespace('anp', 'https://www.gov.br/anp/pt-br')    # ANP - Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\n",
    "    document_prov.add_namespace('anp-dados_tec','https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/acervo-de-dados-tecnicos') # ANP - Acervo de Dados Técnicos \n",
    "    document_prov.add_namespace('petrobras','https://petrobras.com.br/')  # PETROBRAS\n",
    "    document_prov.add_namespace('br','http://br.org/ns/')    # Organizações Brasileiras\n",
    "    return document_prov\n",
    "\n",
    "\n",
    "def escape_label(text):\n",
    "    \"\"\"\n",
    "    Escapes special characters for Graphviz.\n",
    "    Encodes text to ASCII with XML character references.\n",
    "    \"\"\"\n",
    "    return text.encode(\"ascii\", \"xmlcharrefreplace\").decode()\n",
    "\n",
    "def get_installed_packages():\n",
    "    #Retorna os pacotes instalados no ambiente com suas versões.\n",
    "    try:\n",
    "        return {pkg.metadata['Name']: pkg.version for pkg in importlib.metadata.distributions()}\n",
    "    except ImportError:\n",
    "        import pkg_resources\n",
    "        return {dist.project_name: dist.version for dist in pkg_resources.working_set}\n",
    "\n",
    "def get_system_info():\n",
    "    #Retorna informações do sistema.\n",
    "    return {\n",
    "        \"OS\": platform.system(),\n",
    "        \"OS Version\": platform.version(),\n",
    "        \"OS Release\": platform.release(),\n",
    "        \"Python Version\": sys.version,\n",
    "        \"Python Executable\": sys.executable,\n",
    "        \"Current Working Directory\": str(Path.cwd()),}\n",
    "\n",
    "def add_system_and_package_provenance(doc_prov, generation_activity):\n",
    "    #Adiciona informações do sistema e pacotes ao documento de proveniência\n",
    "    # Criar atividade para rastrear informações de sistema e pacotes\n",
    "    #activity_id = \"ufrj:track_system_and_packages\"\n",
    "    #tracking_activity = doc_prov.activity(activity_id, datetime.datetime.now(), None, {\"prov:label\": escape_label(\"Track system and package provenance\")})\n",
    "    # Criar atividade para rastrear informações de sistema e pacotes\n",
    "    activity_id = \"ufrj:track_system_and_packages\"\n",
    "    tracking_activity = doc_prov.activity(\n",
    "        activity_id,\n",
    "        datetime.datetime.now(), None, {\"prov:label\": escape_label(\"Track system and package provenance\")}\n",
    "    )\n",
    "\n",
    "    # Associar a atividade ao agente do notebook\n",
    "    if \"ag-eda-ipynb\" in dict_agents:\n",
    "        doc_prov.wasAssociatedWith(tracking_activity, dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Adicionar informações do sistema como entidades\n",
    "    system_info = get_system_info()\n",
    "    for key, value in system_info.items():\n",
    "        sanitized_key = key.replace(\" \", \"_\")  # Substituir espaços por _\n",
    "        sys_entity = doc_prov.entity(f\"schema:{sanitized_key}\", {\"prov:value\": value})\n",
    "        doc_prov.wasGeneratedBy(sys_entity, tracking_activity)\n",
    "\n",
    "    # Adicionar pacotes instalados como entidades\n",
    "    installed_packages = get_installed_packages()\n",
    "    for pkg, version in installed_packages.items():\n",
    "        pkg_entity = doc_prov.entity(f\"schema:{pkg}\", {\"prov:value\": version})\n",
    "        doc_prov.wasGeneratedBy(pkg_entity, tracking_activity)\n",
    "\n",
    "    return doc_prov\n",
    "\n",
    "def create_agents(document_prov):\n",
    "    \n",
    "    #creating agents\n",
    "    dagnts={} #cria dic\n",
    "    dagnts[\"ag-orgbr\"] = document_prov.agent(\"br:orgBr\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Oraganizações Brasileiras\")})\n",
    "    dagnts[\"ag-anp\"] = document_prov.agent(\"anp:ANP\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\")})\n",
    "    dagnts[\"ag-ufrj\"] = document_prov.agent(\"ufrj:UFRJ\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Universidade Federal do Rio de Janeiro\")})\n",
    "    dagnts[\"ag-ppgi\"] = document_prov.agent(\"ufrj:PPGI\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Programa de Pós Graduação em Informática\")})\n",
    "    dagnts[\"ag-greco\"] = document_prov.agent(\"ufrj:GRECO\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Grupo de Engenharia do Conhecimento\")})\n",
    "    dagnts[\"ag-author-ubirajara\"] = document_prov.agent(\"ufrj:Ubirajara\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Ubirajara Simões Santos\"), \"foaf:mbox\":\"ubirajas@hotmail.com\"})\n",
    "    dagnts[\"ag-author-sergio\"] = document_prov.agent(\"ufrj:Sergio\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Sergio Serra\"), \"foaf:mbox\":\"serra@ppgi.ufrj.br\"})\n",
    "    dagnts[\"ag-author-jorge\"] = document_prov.agent(\"ufrj:Jorge\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Jorge Zavaleta\"), \"foaf:mbox\":\"zavaleta@pet-si.ufrrj.br\"})\n",
    "    dagnts[\"ag-petrobras\"] = document_prov.agent(\"petrobras:Petrobras\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Petróleo Brasiliero S.A\")})\n",
    "    dagnts[\"ag-eda-ipynb\"] = document_prov.agent(\"ufrj:eda.ipynb\", {\"prov:type\":\"prov:SoftwareAgent\", \"foaf:name\":escape_label(\"eda.ipynb\"), \"prov:label\":escape_label(\"Notebook Python utilizado no trabalho\")})\n",
    "    return dagnts\n",
    "\n",
    "def associate_ufrj_agents(agents_dictionary):\n",
    "    agents_dictionary[\"ag-anp\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-petrobras\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ufrj\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ppgi\"].actedOnBehalfOf(agents_dictionary[\"ag-ufrj\"])\n",
    "    agents_dictionary[\"ag-greco\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-greco\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-petrobras\"])\n",
    "    agents_dictionary[\"ag-author-sergio\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-jorge\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-eda-ipynb\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    return agents_dictionary\n",
    "\n",
    "def create_initial_activities(document_prov):\n",
    "    #creating activities\n",
    "    #dataDownloadDatasets = datetime.datetime.strptime('29/11/24', '%d/%m/%y')\n",
    "    \n",
    "    dativs={}\n",
    "    dativs[\"act-create-ds\"] = document_prov.activity(\"anp:create-dataset\", None, None, {\"prov:label\":escape_label( \"Criação de datasets pela ANP\")})\n",
    "    #dativs[\"act-extract-ds\"] = document_prov.activity(\"ufrj:extract-dataset\")\n",
    "    dativs[\"act-create-ds-eda\"] = document_prov.activity(\"ufrj:create-ds-eda\", None, None, {\"prov:label\":escape_label( \"Criação de datasets para EDA\")})\n",
    "    \n",
    "    dativs[\"act-load-ds-eda\"] = document_prov.activity(\"ufrj:load-ds-eda\")\n",
    "    dativs[\"act-save-ipynb\"] = document_prov.activity(\"ufrj:save-ipynb\", None, None, {\"prov:label\":escape_label(\"Salvar notebook EDA\")})\n",
    "    return dativs\n",
    "\n",
    "def cria_entidades_iniciais(document_prov):\n",
    "    #creating entidades\n",
    "    dents={}\n",
    "    \n",
    "    # Entidade para amostras de rochas e fluidos\n",
    "    dents[\"ent-amostras-rochas-fluidos\"] = document_prov.entity('anp:amostras_rochas_fluidos', {'prov:label':escape_label('Dataset com amostras de rochas e fluidos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Consolidado 2023 de amostras disponíveis.'), 'prov:format': 'zip' })\n",
    "    # Entidade para setores SIRGAS\n",
    "    dents[\"ent-setores-sirgas\"] = document_prov.entity('anp:setores_sirgas', {'prov:label':escape_label('Setores SIRGAS'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Modelos exploratórios em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para blocos exploratórios\n",
    "    dents[\"ent-blocos-exploratorios\"] = document_prov.entity('anp:blocos_exploratorios', {'prov:label':escape_label( 'Blocos exploratórios'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Blocos exploratórios com dados geoespaciais.'), 'prov:format': 'zip'})\n",
    "    # Entidade para campos de produção\n",
    "    dents[\"ent-campos-producao\"] = document_prov.entity('anp:campos_producao', {'prov:label':escape_label( 'Campos de Produção'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados dos campos de produção em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para reservas nacionais de hidrocarbonetos\n",
    "    dents[\"ent-reservas-nacionais-hc\"] = document_prov.entity('anp:reservas_nacionais_hc',{'prov:label':escape_label( 'Reservas Nacionais de Hidrocarbonetos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela com dados sobre reservas nacionais.'), 'prov:format': 'xlsx'})\n",
    "    # Entidade para poços perfurados (2023)\n",
    "    dents[\"ent-pocos-perfurados-2023\"] = document_prov.entity('anp:pocos_perfurados_2023',{'prov:label':escape_label( 'Poços perfurados - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com os poços perfurados no ano de 2023.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de levantamentos geoquímicos\n",
    "    dents[\"ent-tabela-levantamentos-geoquimica\"] = document_prov.entity('anp:tabela_levantamentos_geoquimica',{'prov:label':escape_label( 'Tabela de levantamentos geoquímicos 20/04/2022'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados sobre levantamentos geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para tabela de dados geoquímicos\n",
    "    dents[\"ent-tabela-dados-geoquimica\"] = document_prov.entity('anp:tabela_dados_geoquimica',{'prov:label':escape_label( 'Tabela_dados_geoquimica 06/08/2021'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para levantamento sísmico (2023)\n",
    "    dents[\"ent-levantamento-sismico-2023\"] = document_prov.entity('anp:levantamento_sismico_2023', {'prov:label':escape_label( 'Levantamento Sísmico - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com dados de levantamentos sísmicos públicos.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de poços (2024)\n",
    "    dents[\"ent-tabela-pocos-2024\"] = document_prov.entity('anp:tabela_pocos_2024', {'prov:label':escape_label( 'Tabela de Poços - 2024'.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela CSV com dados atualizados de poços para 2024.'), 'prov:format': 'csv'})\n",
    "     # Entidade para ANP dados técnicos\n",
    "    dents[\"ent-anp-dados_tec-ds\"] = document_prov.entity('anp-dados_tec:dataset', {'prov:label':escape_label( 'ANP Dataset de Dados Técnicos'.encode(\"ascii\", \"xmlcharrefreplace\").decode()),'prov:type': 'void:Dataset','prov:description':escape_label('Dataset com dados técnicos disponíveis publicamente.'),'prov:format': 'csv'})\n",
    "    \n",
    "    # Entidade script python\n",
    "    dents[\"ent-eda-ipynb\"] = document_prov.entity('ufrj:eda-ipyn', {'prov:label':escape_label( \"Notebook Python utilizado no trabalho\".encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'foaf:Document'})\n",
    "    # Entidade Git\n",
    "    dents[\"ent-git-eda\"] = document_prov.entity('anp:github-eda', {'prov:label':escape_label( 'Repositorio Eba da ANP'.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'prov:Collection'})\n",
    "    return dents\n",
    "  \n",
    "\n",
    "def initial_association_agents_activities_entities(document_prov, dictionary_agents, dictionary_activities, dictionary_entities):\n",
    "    \n",
    "    #Associate activity of generate dataset with ANP agent\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds\"], dictionary_agents[\"ag-anp\"])\n",
    "    \n",
    "    #Associating datasets with activities of generate eba datasets\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-amostras-rochas-fluidos\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-setores-sirgas\"], dictionary_activities[\"act-create-ds\"])    \n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-blocos-exploratorios\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-campos-producao\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-reservas-nacionais-hc\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-pocos-perfurados-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-levantamentos-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-dados-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-levantamento-sismico-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-pocos-2024\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-anp-dados_tec-ds\"], dictionary_activities[\"act-create-ds\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Associating ZIPs, XLSX, CSV com entities do dataset genérico\n",
    "    #document_prov.wasDerivedFrom(dictionary_entities[\"ent-dredfp2021-zip\"], dictionary_entities[\"ent-dredfp\"])  \n",
    "       \n",
    "    #associate activity of eda, com autor\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-author-ubirajara\"])   \n",
    "\n",
    "    #associate notebook agent with eba dataset\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-eda-ipynb\"])    \n",
    "             \n",
    "    #associate eda github repository with store datasets activity\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-git-eda\"], dictionary_activities[\"act-save-ipynb\"])\n",
    "    \n",
    "    \n",
    "def initProvenance():\n",
    "    #Inicializa o documento de proveniência com namespaces, agentes, atividades e entidades.\n",
    "    \n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Criando um documento vazio de proveniência\n",
    "    doc_prov = ProvDocument()\n",
    "\n",
    "    # Criar namespaces no documento de proveniência\n",
    "    doc_prov = adding_namespaces(doc_prov)\n",
    "\n",
    "    # Criar agentes\n",
    "    dict_agents = create_agents(doc_prov)\n",
    "\n",
    "    # Criar atividades iniciais\n",
    "    dict_activities = create_initial_activities(doc_prov)\n",
    "\n",
    "    # Criar entidades iniciais\n",
    "    dict_entities = cria_entidades_iniciais(doc_prov)\n",
    "\n",
    "    # Criar hierarquia de agentes\n",
    "    dict_agents = associate_ufrj_agents(dict_agents)\n",
    "\n",
    "    # Associar agentes, atividades e entidades\n",
    "    initial_association_agents_activities_entities(doc_prov, dict_agents, dict_activities, dict_entities)\n",
    "\n",
    "    # Adicionar proveniência do sistema e pacotes\n",
    "    doc_prov = add_system_and_package_provenance(doc_prov, dict_agents)\n",
    "\n",
    "    return doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "66ffb913-8d8d-47e9-b7a3-3c2e9f98ed5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import openpyxl\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import zipfile\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "9b0c37d3-49e2-46df-a7b9-a71a92541577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv_from_zip(url, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Baixa, extrai e retorna os caminhos dos arquivos CSV contidos em um ZIP.\n",
    "    \"\"\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    csv_files = []\n",
    "\n",
    "    try:\n",
    "        # Baixar o arquivo ZIP\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Erro ao baixar o arquivo: {response.status_code}\")\n",
    "        \n",
    "        # Abrir o ZIP em memória e extrair\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zf:\n",
    "            zf.extractall(temp_dir)\n",
    "            extracted_files = zf.namelist()\n",
    "\n",
    "        # Identificar e coletar arquivos CSV\n",
    "        for file in extracted_files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(temp_dir, file)\n",
    "                csv_files.append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "\n",
    "        if not csv_files:\n",
    "            raise ValueError(\"Nenhum arquivo CSV encontrado no ZIP.\")\n",
    "        \n",
    "        print(f\"Arquivos CSV extraídos: {[file['name'] for file in csv_files]}\")\n",
    "        return csv_files\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar ZIP: {e}\")\n",
    "        return None\n",
    "        \n",
    "def update_data_sources_with_zip(data_sources, source_name, temp_dir=\"./temp\"):\n",
    "    \"\"\"\n",
    "    Atualiza `data_sources` com os arquivos CSV extraídos de um ZIP.\n",
    "    \"\"\"\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return data_sources\n",
    "\n",
    "    url = source[\"url\"]\n",
    "    csv_files = extract_csv_from_zip(url, temp_dir=temp_dir)\n",
    "\n",
    "    if csv_files:\n",
    "        for i, csv_file in enumerate(csv_files):\n",
    "            new_source_name = f\"{source_name}_csv_{i+1}\"\n",
    "            data_sources[new_source_name] = {\n",
    "                \"url\": csv_file,\n",
    "                \"type\": \"csv\",\n",
    "                \"sep\": source.get(\"sep\", \";\"),\n",
    "                \"encoding\": source.get(\"encoding\", \"utf-8\"),\n",
    "                \"header\": source.get(\"header\", 0),\n",
    "                \"date_columns\": source.get(\"date_columns\", [])\n",
    "            }\n",
    "        print(f\"Data sources atualizados com os arquivos CSV de '{source_name}'.\")\n",
    "    return data_sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "df4ff600-8332-4879-b792-63dcc13dc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_source_csv(source_name, data_sources):\n",
    "    \"\"\"\n",
    "    Carrega dados com base no nome da fonte e na configuração em data_sources.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com os dados carregados, ou None se houver erro.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents,  dict_activities, dict_entities  # Declare global variables\n",
    "    #save execution start time\n",
    "    execStartTime = datetime.datetime.now()\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "   \n",
    "    if not source:\n",
    "        print(f\"Fonte '{source_name}' não encontrada em data_sources.\")\n",
    "        return None\n",
    "\n",
    "    file_type = source.get(\"type\")\n",
    "    url = source.get(\"url\")\n",
    "    sep = source.get(\"sep\", \";\")  # Valor padrão para CSV\n",
    "    encoding = source.get(\"encoding\", \"utf-8\")  # Valor padrão para codificação\n",
    "    date_columns = source.get(\"date_columns\", [])  \n",
    "\n",
    "    try:\n",
    "        if file_type == \"csv\":\n",
    "             # Caso específico para tabela_pocos_2024\n",
    "            if source_name == \"tabela_pocos_2024\":\n",
    "                df = pd.read_csv(url, encoding=\"ANSI\", sep=sep)\n",
    "            # Caso específico para tabela_dados_geoquimica\n",
    "            elif source_name == \"tabela_dados_geoquimica\":\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, header=1)  # Cabeçalho na segunda linha\n",
    "            else:\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, parse_dates=date_columns)\n",
    "        elif file_type == \"xlsx\":\n",
    "            df = pd.read_excel(url)\n",
    "        else:\n",
    "            print(f\"Tipo de arquivo '{file_type}' não suportado.\")\n",
    "            return None\n",
    "        print(f\"Dados carregados com sucesso para '{source_name}'.\")\n",
    "    \n",
    "        # End execution time for provenance tracking\n",
    "        execEndTime = datetime.datetime.now()\n",
    "    \n",
    "        # Criar atividade com horário de término da execução\n",
    "        activity_key = f\"act-carga-{source_name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}\", execStartTime, execEndTime)\n",
    "    \n",
    "        # Associar a atividade ao agente\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "        \n",
    "        # Associar a atividade com os dados carregados\n",
    "        entity_key = f\"ent-{source_name}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{source_name}\", {\n",
    "            \"prov:label\": escape_label(f\"Dataset carregado: {source_name}\"),\"prov:type\": \"void:Dataset\", \"prov:generatedAtTime\": execEndTime.isoformat(),})\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "    \n",
    "        # Associar a atividade ufrj:carga à entidade correspondente criada pela ANP\n",
    "        anp_entity_key = f\"ent-{source_name.replace('_', '-')}\"  # Convert to ANP format (e.g., `tabela_pocos_2024` -> `ent-tabela-pocos-2024`)\n",
    "        if anp_entity_key in dict_entities:\n",
    "            # Establish the prov:used relationship\n",
    "            doc_prov.used(dict_activities[activity_key], dict_entities[anp_entity_key])\n",
    "        else:\n",
    "            print(f\"Warning: ANP entity '{anp_entity_key}' not found for activity '{activity_key}'.\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados de '{source_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "0c0ab2cc-cc26-432b-a2e3-b3f6e10faa81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_pocos_orig = load_data_from_source(\"tabela_pocos_2024\", data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "e93ead43-b565-4fa0-b0c7-fec6078bf061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pocos_orig.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "498bbbcd-bf7d-465c-97dc-0e853178826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sismica_2023_orig = load_data_from_source(\"levantamento_sismico_2023\", data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "ee466c73-7e1a-43e8-a2a5-84e351f624af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sismica_2023_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "a618a908-1dfa-41cb-b4c0-00d8236b489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lev_geoq_2022 = load_data_from_source(\"tabela_levantamentos_geoquimica\", data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "b51f81e5-229e-4a00-bc95-a3fac7b27125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lev_geoq_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "d67b0050-2ad2-4ef5-87f5-d718b5df1309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_geoq_2021 = load_data_from_source(\"tabela_dados_geoquimica\", data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "9403b858-cbe0-47d4-ab1a-b0b95568ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_geoq_2021.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "61f6a3ec-34a9-44f1-baa9-bb932ec77e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reservas = load_data_from_source(\"reservas_nacionais_hc\", data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "822a7910-cf2d-4218-9b97-c9fe126a8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reservas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "98157ebb-b8b4-470c-b5c6-17b68710a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##df_amostras = load_data_from_source(\"amostras_rochas_fluidos\", data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "897f1c6f-dc55-4f22-97fe-fe518fa45719",
   "metadata": {},
   "outputs": [],
   "source": [
    "##data_sources = update_data_sources_with_zip(data_sources, \"amostras_rochas_fluidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "373b39d0-6baf-47de-84d3-1e3e15fdd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def create_eda_dataset():\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Load datasets and associate with provenance\n",
    "    df_sismica_2023_orig = load_data_from_source_csv(\"levantamento_sismico_2023\", data_sources)\n",
    "    df_pocos_orig = load_data_from_source_csv(\"tabela_pocos_2024\", data_sources)\n",
    "    df_lev_geoq_2022 = load_data_from_source_csv(\"tabela_levantamentos_geoquimica\", data_sources)\n",
    "    df_geoq_2021 = load_data_from_source_csv(\"tabela_dados_geoquimica\", data_sources)\n",
    "    df_reservas = load_data_from_source_csv(\"reservas_nacionais_hc\", data_sources)\n",
    "\n",
    "    # Debugging provenance dictionaries\n",
    "    print(\"Activities after data load:\", dict_activities.keys())\n",
    "    print(\"Entities after data load:\", dict_entities.keys())\"\"\"\n",
    "    \n",
    "\n",
    "def create_eda_dataset():\n",
    "    \"\"\"\n",
    "    Lógica para criar o dataset EDA.\n",
    "    Adicione toda a lógica para manipulação de dados e associação de proveniência.\n",
    "    \"\"\"\n",
    "    activity_key = \"act-create-ds-eda\"\n",
    "    exec_start = datetime.datetime.now()\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Load datasets and associate with provenance\n",
    "    df_sismica_2023_orig = load_data_from_source_csv(\"levantamento_sismico_2023\", data_sources)\n",
    "    df_pocos_orig = load_data_from_source_csv(\"tabela_pocos_2024\", data_sources)\n",
    "    df_lev_geoq_2022 = load_data_from_source_csv(\"tabela_levantamentos_geoquimica\", data_sources)\n",
    "    df_geoq_2021 = load_data_from_source_csv(\"tabela_dados_geoquimica\", data_sources)\n",
    "    df_reservas = load_data_from_source_csv(\"reservas_nacionais_hc\", data_sources)\n",
    "\n",
    "    # Debugging provenance dictionaries\n",
    "    print(\"Activities after data load:\", dict_activities.keys())\n",
    "    print(\"Entities after data load:\", dict_entities.keys())\n",
    "    \n",
    "    exec_end = datetime.datetime.now()\n",
    "\n",
    "    # Atualiza atividade no documento de proveniência\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        \"ufrj:create-ds-eda\", exec_start, exec_end,\n",
    "        {\"prov:label\": escape_label(\"Criação de datasets para EDA\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    print(\"Dataset creation tracked in provenance.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "967d4e6f-cccf-46a3-be2a-1c27dedd730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal para inicializar e executar o rastreamento de proveniência.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Inicializar o documento de proveniência\n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "    # Verificar a inicialização\n",
    "    print(\"Agents:\", dict_agents.keys())\n",
    "    print(\"Activities:\", dict_activities.keys())\n",
    "    print(\"Entities:\", dict_entities.keys())\n",
    "\n",
    "    # Executar a lógica de criação do dataset\n",
    "    create_eda_dataset()\n",
    "\n",
    "    # Serializar e gerar saídas\n",
    "    gerar_prov_outputs(doc_prov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "b55d4ba5-8304-4916-9301-a31d3c312594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def main():\n",
    "    # Inicializa o documento de proveniência, agentes, atividades e entidades\n",
    "    print(\"Initializing Provenance Tracking...\")\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "    # Verifica a inicialização\n",
    "    print(\"Agents:\", dict_agents.keys())\n",
    "    print(\"Activities:\", dict_activities.keys())\n",
    "    print(\"Entities:\", dict_entities.keys())\n",
    "\n",
    "    # Carrega datasets e associa suas proveniências\n",
    "    print(\"Loading datasets and tracking provenance...\")\n",
    "    datasets = {\n",
    "        \"levantamento_sismico_2023\": {\"type\": \"csv\", \"url\": \"path_to_file.csv\", \"sep\": \";\"},\n",
    "        \"tabela_pocos_2024\": {\"type\": \"csv\", \"url\": \"path_to_file.csv\", \"sep\": \";\"},\n",
    "        # Adicione outros datasets conforme necessário\n",
    "    }\n",
    "\n",
    "    for source_name, config in datasets.items():\n",
    "        load_data_from_source_csv(source_name, datasets)\n",
    "\n",
    "    # Executa a lógica de criação do dataset para EDA\n",
    "    print(\"Creating EDA dataset...\")\n",
    "    create_eda_dataset()\n",
    "\n",
    "    # Serializa e gera saídas\n",
    "    print(\"Generating provenance outputs...\")\n",
    "    gerar_prov_outputs(doc_prov)\n",
    "\n",
    "    print(\"Provenance Tracking Completed.\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "eec0da88-a052-43a1-8b42-1745be7a4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: dict_keys(['ag-orgbr', 'ag-anp', 'ag-ufrj', 'ag-ppgi', 'ag-greco', 'ag-author-ubirajara', 'ag-author-sergio', 'ag-author-jorge', 'ag-petrobras', 'ag-eda-ipynb'])\n",
      "Activities: dict_keys(['act-create-ds', 'act-create-ds-eda', 'act-load-ds-eda', 'act-save-ipynb'])\n",
      "Entities: dict_keys(['ent-amostras-rochas-fluidos', 'ent-setores-sirgas', 'ent-blocos-exploratorios', 'ent-campos-producao', 'ent-reservas-nacionais-hc', 'ent-pocos-perfurados-2023', 'ent-tabela-levantamentos-geoquimica', 'ent-tabela-dados-geoquimica', 'ent-levantamento-sismico-2023', 'ent-tabela-pocos-2024', 'ent-anp-dados_tec-ds', 'ent-eda-ipynb', 'ent-git-eda'])\n",
      "Dados carregados com sucesso para 'levantamento_sismico_2023'.\n",
      "Dados carregados com sucesso para 'tabela_pocos_2024'.\n",
      "Dados carregados com sucesso para 'tabela_levantamentos_geoquimica'.\n",
      "Dados carregados com sucesso para 'tabela_dados_geoquimica'.\n",
      "Dados carregados com sucesso para 'reservas_nacionais_hc'.\n",
      "Activities after data load: dict_keys(['act-create-ds', 'act-create-ds-eda', 'act-load-ds-eda', 'act-save-ipynb', 'act-carga-levantamento_sismico_2023', 'act-carga-tabela_pocos_2024', 'act-carga-tabela_levantamentos_geoquimica', 'act-carga-tabela_dados_geoquimica', 'act-carga-reservas_nacionais_hc'])\n",
      "Entities after data load: dict_keys(['ent-amostras-rochas-fluidos', 'ent-setores-sirgas', 'ent-blocos-exploratorios', 'ent-campos-producao', 'ent-reservas-nacionais-hc', 'ent-pocos-perfurados-2023', 'ent-tabela-levantamentos-geoquimica', 'ent-tabela-dados-geoquimica', 'ent-levantamento-sismico-2023', 'ent-tabela-pocos-2024', 'ent-anp-dados_tec-ds', 'ent-eda-ipynb', 'ent-git-eda', 'ent-levantamento_sismico_2023', 'ent-tabela_pocos_2024', 'ent-tabela_levantamentos_geoquimica', 'ent-tabela_dados_geoquimica', 'ent-reservas_nacionais_hc'])\n",
      "Dataset creation tracked in provenance.\n",
      "Provenance graph generated successfully: EDA-PROV.png\n",
      "Provenance serialized as XML and TTL.\n"
     ]
    }
   ],
   "source": [
    "#EXECUTAR\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "281c37a4-6a78-4b70-ac95-3aed03f21d4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tracking_activity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[553], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Serializa e gera saídas\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     gerar_prov_outputs(doc_prov)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[553], line 7\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m doc_prov, dict_agents, dict_activities, dict_entities\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Adicionar inicialização de proveniência, agentes e atividades\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m doc_prov, dict_agents, dict_activities, dict_entities \u001b[38;5;241m=\u001b[39m \u001b[43minitProvenance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Adicionar informações de sistema e pacotes ao documento de proveniência\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#add_system_and_package_provenance(doc_prov)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Check initialization\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgents:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dict_agents\u001b[38;5;241m.\u001b[39mkeys())\n",
      "Cell \u001b[1;32mIn[552], line 248\u001b[0m, in \u001b[0;36minitProvenance\u001b[1;34m()\u001b[0m\n\u001b[0;32m    245\u001b[0m doc_prov\u001b[38;5;241m.\u001b[39mwasAssociatedWith(generation_activity, dict_agents[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag-eda-ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Adding system and package provenance\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m doc_prov \u001b[38;5;241m=\u001b[39m \u001b[43madd_system_and_package_provenance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_prov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_activity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc_prov, dict_agents, dict_activities, dict_entities\n",
      "Cell \u001b[1;32mIn[552], line 94\u001b[0m, in \u001b[0;36madd_system_and_package_provenance\u001b[1;34m(doc_prov, generation_activity)\u001b[0m\n\u001b[0;32m     92\u001b[0m     sanitized_key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Substituir espaços por _\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     sys_entity \u001b[38;5;241m=\u001b[39m doc_prov\u001b[38;5;241m.\u001b[39mentity(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprov:value\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})\n\u001b[1;32m---> 94\u001b[0m     doc_prov\u001b[38;5;241m.\u001b[39mwasGeneratedBy(sys_entity, \u001b[43mtracking_activity\u001b[49m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Adicionar pacotes instalados como entidades\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pkg, version \u001b[38;5;129;01min\u001b[39;00m installed_packages\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tracking_activity' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #Função principal para gerar o documento de proveniência.\n",
    "    \n",
    "    # Initialize provenance objects\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Adicionar inicialização de proveniência, agentes e atividades\n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "\n",
    "    # Check initializationAssociatedWith(tracking_activ\n",
    "    print(\"Agents:\", dict_agents.keys())\n",
    "    print(\"Activities:\", dict_activities.keys())\n",
    "    print(\"Entities:\", dict_entities.keys())\n",
    "\n",
    "    # Executa a lógica de criação do dataset\n",
    "    create_eda_dataset()\n",
    "\n",
    "    # Serializa e gera saídas\n",
    "    gerar_prov_outputs(doc_prov)\n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd642769-e0a8-46d6-be83-39ea12671be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a17b0-533c-4b78-8f2f-f14902322878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e8324-b2d1-4894-b4b1-4c6668f966d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d97b7-c25b-4d4c-b602-bb9049fccd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a1fac-9ef9-4fa7-9b3a-4fa0e3256199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56345402-fcaf-4913-9861-2dc9efcfbf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6c990-9abc-4fc4-a1ae-68b420c3b49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2ee99-f1a9-47cb-82ea-52aa76d2a37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec94a2-98e4-427e-929d-081f710d6677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f506e37-92db-4a88-ab2b-203f80583d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c143ab9-8012-4232-b168-77a432c3bcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92298a26-a92d-4000-9b52-34e1f1c7a3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf1381-e1c6-407c-896f-4232e02f8e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55abba-9f5a-45a9-997a-d31ee4b51883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda23c4-7eaf-426c-aaec-04eacd62a1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8cebd-3dda-47a4-bf8a-eee945a0e105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

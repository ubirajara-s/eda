{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f14207f-be28-4283-9314-1e02e715db8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fundamentos de Ciência de Dados\n",
    "## PPGI/UFRJ 2024.2\n",
    "### Profs Sergio Serra e Jorge Zavaleta\n",
    "### Aluno Ubirajara S. Santos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21fc0eda-8216-4ae5-ba67-9a060afea2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prov\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ac38fd8-c6aa-4138-ae54-2a103a3f52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './dados/saidas'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38838849-a486-40fb-a8c1-35e945ee947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fontes de Dados\n",
    "data_sources = {\n",
    "     \"amostras_rochas_fluidos\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-amostras-de-rochas-e-fluidos/acervo-de-amostras/consolidacao-2023.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"setores_sirgas\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/assuntos/exploracao-e-producao-de-oleo-e-gas/estudos-geologicos-e-geofisicos/arquivos-classificacao-de-modelos-exploratorios/setores-sirgas.zip\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"blocos_exploratorios\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ABLOCOS_EXPLORATORIOS_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"campos_producao\": {\n",
    "        \"url\": \"https://gishub.anp.gov.br/geoserver/BD_ANP/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=BD_ANP%3ACAMPOS_PRODUCAO_SIRGAS&maxFeatures=40000&outputFormat=SHAPE-ZIP\",\n",
    "        \"type\": \"zip\"},\n",
    "     \"reservas_nacionais_hc\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-estatisticos/arquivos-reservas-nacionais-de-petroleo-e-gas-natural/tabela-dados-bar-2023.xlsx\",\n",
    "        \"type\": \"xlsx\"},\n",
    "     \"pocos_perfurados_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/pocos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_levantamentos_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-levantamentos-geoquimicos.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"levantamento_sismico_2023\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/sismicos-publicos-2023.csv\",\n",
    "        \"type\": \"csv\"},\n",
    "     \"tabela_pocos_2024\": {\n",
    "        \"url\": \"./dados/entradas/Tabela_pocos_2024_Novembro_24.csv\",\n",
    "        \"type\": \"csv\", \"sep\": \";\" ,\"encoding\": \"ANSI\"},\n",
    "     \"tabela_dados_geoquimica\": {\n",
    "        \"url\": \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-dados-geoquimicos.csv\",\n",
    "        \"type\": \"csv\",\n",
    "        \"header\": 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2c3e5eb-413c-433b-ad53-5bfb214de691",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, datetime\n",
    "from prov.model import ProvDocument, Namespace\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "import plotly\n",
    "import graphviz\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import importlib.metadata\n",
    "\n",
    "def gerar_prov_outputs(doc_prov):\n",
    "    entity = \"EDA-PROV\"\n",
    "    output_file = f\"{entity}.png\"\n",
    "    try:\n",
    "        dot = prov_to_dot(doc_prov)\n",
    "        # Write to PNG\n",
    "        dot.write_png(output_file)\n",
    "        print(f\"Provenance graph generated successfully: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating provenance graph: {e}\")\n",
    "        # Save the DOT file for debugging\n",
    "        with open(\"debug.dot\", \"w\") as f:\n",
    "            f.write(dot.to_string())\n",
    "        print(\"Saved DOT file for debugging as 'debug.dot'.\")\n",
    "   \n",
    "    # Serialização do documento\n",
    "    doc_prov.serialize(entity + \".xml\", format='xml') \n",
    "    doc_prov.serialize(entity + \".ttl\", format='rdf', rdf_format='ttl',encoding=\"utf-8\")\n",
    "    print(\"Provenance serialized as XML and TTL.\")\n",
    "    \n",
    "\n",
    "def adding_namespaces(document_prov):\n",
    "    # Adiciona namespaces ao documento de proveniência.\n",
    "    document_prov.add_namespace('void', 'http://vocab.deri.ie/void#')\n",
    "    document_prov.add_namespace('ufrj', 'https://www.ufrj.br')\n",
    "    document_prov.add_namespace('schema', 'http://schema.org/')    # Dados estruturados Schema.org\n",
    "    document_prov.add_namespace('prov', 'http://www.w3.org/ns/prov#')     # Padrões PROV\n",
    "    document_prov.add_namespace('foaf', 'http://xmlns.com/foaf/0.1/')     # Agentes FOAF\n",
    "    document_prov.add_namespace('ufrj-ppgi', 'http://www.ufrj.br/ppgi/')  # UFRJ PPGI\n",
    "    document_prov.add_namespace('anp', 'https://www.gov.br/anp/pt-br')    # ANP - Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\n",
    "    document_prov.add_namespace('anp-dados_tec','https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/acervo-de-dados-tecnicos') # ANP - Acervo de Dados Técnicos \n",
    "    document_prov.add_namespace('petrobras','https://petrobras.com.br/')  # PETROBRAS\n",
    "    document_prov.add_namespace('br','http://br.org/ns/')    # Organizações Brasileiras\n",
    "    return document_prov\n",
    "\n",
    "\n",
    "def escape_label(text):\n",
    "    \"\"\"\n",
    "    Escapes special characters for Graphviz.\n",
    "    Encodes text to ASCII with XML character references.\n",
    "    \"\"\"\n",
    "    return text.encode(\"ascii\", \"xmlcharrefreplace\").decode()\n",
    "\n",
    "def get_installed_packages():\n",
    "    #Retorna os pacotes instalados no ambiente com suas versões.\n",
    "    try:\n",
    "        return {pkg.metadata['Name']: pkg.version for pkg in importlib.metadata.distributions()}\n",
    "    except ImportError:\n",
    "        import pkg_resources\n",
    "        return {dist.project_name: dist.version for dist in pkg_resources.working_set}\n",
    "\n",
    "def get_system_info():\n",
    "    #Retorna informações do sistema.\n",
    "    return {\n",
    "        \"OS\": platform.system(),\n",
    "        \"OS Version\": platform.version(),\n",
    "        \"OS Release\": platform.release(),\n",
    "        \"Python Version\": sys.version,\n",
    "        \"Python Executable\": sys.executable,\n",
    "        \"Current Working Directory\": str(Path.cwd()),}\n",
    "\n",
    "def get_used_packages():\n",
    "    \n",
    "    #Retorna um dicionário dos pacotes usados explicitamente no projeto e suas versões.\n",
    "    \n",
    "    packages = ['numpy', 'pandas', 'matplotlib', 'seaborn', 'rdflib', 'prov', 'graphviz', \n",
    "                'openpyxl', 'folium', 'pydot', 'requests', 'geopandas']  # Adicione ou remova pacotes usados\n",
    "    package_versions = {}\n",
    "    for package in packages:\n",
    "        try:\n",
    "            import importlib.metadata\n",
    "            version = importlib.metadata.version(package)\n",
    "            package_versions[package] = version\n",
    "        except ImportError:\n",
    "            print(f\"Pacote {package} não encontrado.\")\n",
    "    return package_versions\n",
    "\n",
    "def add_system_and_package_provenance(doc_prov):\n",
    "    #Adiciona informações do sistema e pacotes ao documento de proveniência\n",
    "    \n",
    "    # Criar atividade para rastrear informações de sistema e pacotes\n",
    "    activity_id = \"ufrj:track_system_and_packages\"\n",
    "    tracking_activity = doc_prov.activity(activity_id, datetime.datetime.now(), None, {\"prov:label\": escape_label(\"Track system and package provenance\")})\n",
    "\n",
    "    # Associar a atividade ao agente do notebook\n",
    "    if \"ag-eda-ipynb\" in dict_agents:\n",
    "        doc_prov.wasAssociatedWith(tracking_activity, dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Adicionar informações do sistema como entidades\n",
    "    system_info = get_system_info()\n",
    "    for key, value in system_info.items():\n",
    "        sanitized_key = key.replace(\" \", \"_\")  # Substituir espaços por _\n",
    "        sys_entity = doc_prov.entity(f\"schema:{sanitized_key}\", {\"prov:value\": value})\n",
    "        doc_prov.wasGeneratedBy(sys_entity, tracking_activity)\n",
    "\n",
    "    # Adicionar pacotes usados como entidades\n",
    "    used_packages = get_used_packages()\n",
    "    for pkg, version in used_packages.items():\n",
    "        pkg_entity = doc_prov.entity(f\"schema:{pkg}\", {\"prov:value\": version})\n",
    "        doc_prov.wasGeneratedBy(pkg_entity, tracking_activity)\n",
    "\n",
    "    return doc_prov\n",
    "\n",
    "def create_agents(document_prov):\n",
    "    \n",
    "    #creating agents\n",
    "    dagnts={} #cria dic\n",
    "    dagnts[\"ag-orgbr\"] = document_prov.agent(\"br:orgBr\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Oraganizações Brasileiras\")})\n",
    "    dagnts[\"ag-anp\"] = document_prov.agent(\"anp:ANP\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Agência Nacional do Petróleo, Gás Natural e Biocombustíveis\")})\n",
    "    dagnts[\"ag-ufrj\"] = document_prov.agent(\"ufrj:UFRJ\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Universidade Federal do Rio de Janeiro\")})\n",
    "    dagnts[\"ag-ppgi\"] = document_prov.agent(\"ufrj:PPGI\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Programa de Pós Graduação em Informática\")})\n",
    "    dagnts[\"ag-greco\"] = document_prov.agent(\"ufrj:GRECO\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Grupo de Engenharia do Conhecimento\")})\n",
    "    dagnts[\"ag-author-ubirajara\"] = document_prov.agent(\"ufrj:Ubirajara\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Ubirajara Simões Santos\"), \"foaf:mbox\":\"ubirajas@hotmail.com\"})\n",
    "    dagnts[\"ag-author-sergio\"] = document_prov.agent(\"ufrj:Sergio\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Sergio Serra\"), \"foaf:mbox\":\"serra@ppgi.ufrj.br\"})\n",
    "    dagnts[\"ag-author-jorge\"] = document_prov.agent(\"ufrj:Jorge\", {\"prov:type\":\"prov:Person\", \"foaf:name\":escape_label(\"Jorge Zavaleta\"), \"foaf:mbox\":\"zavaleta@pet-si.ufrrj.br\"})\n",
    "    dagnts[\"ag-petrobras\"] = document_prov.agent(\"petrobras:Petrobras\", {\"prov:type\":\"prov:Organization\", \"foaf:name\":escape_label(\"Petróleo Brasiliero S.A\")})\n",
    "    dagnts[\"ag-eda-ipynb\"] = document_prov.agent(\"ufrj:eda.ipynb\", {\"prov:type\":\"prov:SoftwareAgent\", \"foaf:name\":escape_label(\"eda.ipynb\"), \"prov:label\":escape_label(\"Notebook Python utilizado no trabalho\")})\n",
    "    return dagnts\n",
    "\n",
    "def associate_ufrj_agents(agents_dictionary):\n",
    "    agents_dictionary[\"ag-anp\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-petrobras\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ufrj\"].actedOnBehalfOf(agents_dictionary[\"ag-orgbr\"])\n",
    "    agents_dictionary[\"ag-ppgi\"].actedOnBehalfOf(agents_dictionary[\"ag-ufrj\"])\n",
    "    agents_dictionary[\"ag-greco\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-greco\"])\n",
    "    agents_dictionary[\"ag-author-ubirajara\"].actedOnBehalfOf(agents_dictionary[\"ag-petrobras\"])\n",
    "    agents_dictionary[\"ag-author-sergio\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-author-jorge\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    agents_dictionary[\"ag-eda-ipynb\"].actedOnBehalfOf(agents_dictionary[\"ag-ppgi\"])\n",
    "    return agents_dictionary\n",
    "\n",
    " \n",
    "\n",
    "def create_initial_activities(document_prov):\n",
    "    #creating activities\n",
    "    #dataDownloadDatasets = datetime.datetime.strptime('29/11/24', '%d/%m/%y')\n",
    "    \n",
    "    dativs={}\n",
    "    dativs[\"act-create-ds\"] = document_prov.activity(\"anp:create-dataset\", None, None, {\"prov:label\":escape_label( \"Criação de datasets pela ANP\")})\n",
    "    #dativs[\"act-extract-ds\"] = document_prov.activity(\"ufrj:extract-dataset\")\n",
    "    dativs[\"act-create-ds-eda\"] = document_prov.activity(\"ufrj:create-ds-eda\", None, None, {\"prov:label\":escape_label( \"Criação de datasets para EDA\")})\n",
    "    #dativs[\"act-load-ds-eda\"] = document_prov.activity(\"ufrj:load-ds-eda\")\n",
    "    dativs[\"act-save-ipynb\"] = document_prov.activity(\"ufrj:save-ipynb\", None, None, {\"prov:label\":escape_label(\"Salvar notebook EDA\")})\n",
    "    return dativs\n",
    "\n",
    "def cria_entidades_iniciais(document_prov):\n",
    "    #creating entidades\n",
    "    dents={}\n",
    "    \n",
    "    # Entidade para amostras de rochas e fluidos\n",
    "    dents[\"ent-amostras-rochas-fluidos\"] = document_prov.entity('anp:amostras_rochas_fluidos', {'prov:label':escape_label('Dataset com amostras de rochas e fluidos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Consolidado 2023 de amostras disponíveis.'), 'prov:format': 'zip' })\n",
    "    # Entidade para setores SIRGAS\n",
    "    dents[\"ent-setores-sirgas\"] = document_prov.entity('anp:setores_sirgas', {'prov:label':escape_label('Setores SIRGAS'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Modelos exploratórios em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para blocos exploratórios\n",
    "    dents[\"ent-blocos-exploratorios\"] = document_prov.entity('anp:blocos_exploratorios', {'prov:label':escape_label( 'Blocos exploratórios'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Blocos exploratórios com dados geoespaciais.'), 'prov:format': 'zip'})\n",
    "    # Entidade para campos de produção\n",
    "    dents[\"ent-campos-producao\"] = document_prov.entity('anp:campos_producao', {'prov:label':escape_label( 'Campos de Produção'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados dos campos de produção em formato SIRGAS.'), 'prov:format': 'zip'})\n",
    "    # Entidade para reservas nacionais de hidrocarbonetos\n",
    "    dents[\"ent-reservas-nacionais-hc\"] = document_prov.entity('anp:reservas_nacionais_hc',{'prov:label':escape_label( 'Reservas Nacionais de Hidrocarbonetos'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela com dados sobre reservas nacionais.'), 'prov:format': 'xlsx'})\n",
    "    # Entidade para poços perfurados (2023)\n",
    "    dents[\"ent-pocos-perfurados-2023\"] = document_prov.entity('anp:pocos_perfurados_2023',{'prov:label':escape_label( 'Poços perfurados - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com os poços perfurados no ano de 2023.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de levantamentos geoquímicos\n",
    "    dents[\"ent-tabela-levantamentos-geoquimica\"] = document_prov.entity('anp:tabela_levantamentos_geoquimica',{'prov:label':escape_label( 'Tabela de levantamentos geoquímicos 20/04/2022'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados sobre levantamentos geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para tabela de dados geoquímicos\n",
    "    dents[\"ent-tabela-dados-geoquimica\"] = document_prov.entity('anp:tabela_dados_geoquimica',{'prov:label':escape_label( 'Tabela_dados_geoquimica 06/08/2021'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Dados geoquímicos.'), 'prov:format': 'csv'})\n",
    "     # Entidade para levantamento sísmico (2023)\n",
    "    dents[\"ent-levantamento-sismico-2023\"] = document_prov.entity('anp:levantamento_sismico_2023', {'prov:label':escape_label( 'Levantamento Sísmico - 2023'), 'prov:type': 'void:Dataset', 'prov:description':escape_label('CSV com dados de levantamentos sísmicos públicos.'), 'prov:format': 'csv'})\n",
    "    # Entidade para tabela de poços (2024)\n",
    "    dents[\"ent-tabela-pocos-2024\"] = document_prov.entity('anp:tabela_pocos_2024', {'prov:label':escape_label( 'Tabela de Poços - 2024'.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'void:Dataset', 'prov:description':escape_label('Tabela CSV com dados atualizados de poços para 2024.'), 'prov:format': 'csv'})\n",
    "     # Entidade para ANP dados técnicos\n",
    "    dents[\"ent-anp-dados_tec-ds\"] = document_prov.entity('anp-dados_tec:dataset', {'prov:label':escape_label( 'ANP Dataset de Dados Técnicos'.encode(\"ascii\", \"xmlcharrefreplace\").decode()),'prov:type': 'void:Dataset','prov:description':escape_label('Dataset com dados técnicos disponíveis publicamente.'),'prov:format': 'csv'})\n",
    "    \n",
    "    # Entidade script python\n",
    "    dents[\"ent-eda-ipynb\"] = document_prov.entity('ufrj:eda-ipyn', {'prov:label':escape_label( \"Notebook Python utilizado no trabalho\".encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'foaf:Document'})\n",
    "    # Entidade Git\n",
    "    dents[\"ent-git-eda\"] = document_prov.entity('anp:github-eda', {'prov:label':escape_label( 'Repositorio Eba da ANP'.encode(\"ascii\", \"xmlcharrefreplace\").decode()), 'prov:type': 'prov:Collection'})\n",
    "    return dents\n",
    "  \n",
    "\n",
    "def initial_association_agents_activities_entities(document_prov, dictionary_agents, dictionary_activities, dictionary_entities):\n",
    "    \n",
    "    #Associate activity of generate dataset with ANP agent\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds\"], dictionary_agents[\"ag-anp\"])\n",
    "    \n",
    "    #Associating datasets with activities of generate eba datasets\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-amostras-rochas-fluidos\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-setores-sirgas\"], dictionary_activities[\"act-create-ds\"])    \n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-blocos-exploratorios\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-campos-producao\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-reservas-nacionais-hc\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-pocos-perfurados-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-levantamentos-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-dados-geoquimica\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-levantamento-sismico-2023\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-tabela-pocos-2024\"], dictionary_activities[\"act-create-ds\"])\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-anp-dados_tec-ds\"], dictionary_activities[\"act-create-ds\"])\n",
    "    \n",
    "    \n",
    "    #Associating ZIPs, XLSX, CSV com entities do dataset genérico\n",
    "    #document_prov.wasDerivedFrom(dictionary_entities[\"ent-dredfp2021-zip\"], dictionary_entities[\"ent-dredfp\"])  \n",
    "       \n",
    "    #associate activity of eda, com autor\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-author-ubirajara\"])   \n",
    "\n",
    "    #associate notebook agent with eba dataset\n",
    "    document_prov.wasAssociatedWith(dictionary_activities[\"act-create-ds-eda\"], dictionary_agents[\"ag-eda-ipynb\"])    \n",
    "             \n",
    "    #associate eda github repository with store datasets activity\n",
    "    document_prov.wasGeneratedBy(dictionary_entities[\"ent-git-eda\"], dictionary_activities[\"act-save-ipynb\"])\n",
    "\n",
    "def associate_save_activity(doc_prov, dict_agents, dict_entities):\n",
    "    \n",
    "    #Associa a atividade de salvar notebook ao agente e à entidade relevante.\n",
    "   \n",
    "    activity_id = \"ufrj:save-ipynb\"\n",
    "    save_activity = doc_prov.activity(activity_id, datetime.datetime.now(), None, {\"prov:label\": escape_label(\"Salvar notebook EDA\")})\n",
    "    # Associar ao agente eda.ipynb\n",
    "    if \"ag-eda-ipynb\" in dict_agents:\n",
    "        doc_prov.wasAssociatedWith(save_activity, dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar à entidade eda-ipynb\n",
    "    if \"ent-eda-ipynb\" in dict_entities:\n",
    "        doc_prov.wasGeneratedBy(dict_entities[\"ent-eda-ipynb\"], save_activity)\n",
    "\n",
    "    return doc_prov\n",
    "    \n",
    "    \n",
    "def initProvenance():\n",
    "    #Inicializa o documento de proveniência com namespaces, agentes, atividades e entidades.\n",
    "    \n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Criando um documento vazio de proveniência\n",
    "    doc_prov = ProvDocument()\n",
    "\n",
    "    # Criar namespaces no documento de proveniência\n",
    "    doc_prov = adding_namespaces(doc_prov)\n",
    "\n",
    "    # Criar agentes\n",
    "    dict_agents = create_agents(doc_prov)\n",
    "\n",
    "    # Criar atividades iniciais\n",
    "    dict_activities = create_initial_activities(doc_prov)\n",
    "\n",
    "    # Criar entidades iniciais\n",
    "    dict_entities = cria_entidades_iniciais(doc_prov)\n",
    "\n",
    "    # Criar hierarquia de agentes\n",
    "    dict_agents = associate_ufrj_agents(dict_agents)\n",
    "\n",
    "    # Associar agentes, atividades e entidades\n",
    "    initial_association_agents_activities_entities(doc_prov, dict_agents, dict_activities, dict_entities)\n",
    "\n",
    "    # Adicionar proveniência do sistema e pacotes\n",
    "    doc_prov = add_system_and_package_provenance(doc_prov)\n",
    "\n",
    "    # Associar atividade ufrj:save-ipynb\n",
    "    doc_prov = associate_save_activity(doc_prov, dict_agents, dict_entities)\n",
    "\n",
    "    return doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66ffb913-8d8d-47e9-b7a3-3c2e9f98ed5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import openpyxl\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import zipfile\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e031f122-f76a-4894-be1a-cf55b4fdf9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_zip_content(url, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Analisa o conteúdo de um arquivo ZIP e categoriza os tipos de arquivos encontrados.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL para o arquivo ZIP.\n",
    "        temp_dir (str): Diretório temporário para extração dos arquivos.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicionário com categorias de arquivos (csv, xlsx, shp, others).\n",
    "    \"\"\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    file_types = {\"csv\": [], \"xlsx\": [], \"xls\": [], \"shp\": [], \"others\": []}\n",
    "\n",
    "    try:\n",
    "        # Baixar o arquivo ZIP\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Erro ao baixar o arquivo: {response.status_code}\")\n",
    "\n",
    "        # Abrir o ZIP em memória e extrair\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zf:\n",
    "            zf.extractall(temp_dir)\n",
    "            extracted_files = zf.namelist()\n",
    "\n",
    "        # Categorizar os arquivos extraídos\n",
    "        for file in extracted_files:\n",
    "            file_path = os.path.join(temp_dir, file)\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_types[\"csv\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".xlsx\"):\n",
    "                file_types[\"xlsx\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".xls\"):\n",
    "                file_types[\"xls\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            elif file.endswith(\".shp\"):\n",
    "                file_types[\"shp\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "            else:\n",
    "                file_types[\"others\"].append({\"name\": os.path.basename(file), \"path\": file_path})\n",
    "\n",
    "        print(f\"Conteúdo do ZIP analisado: {file_types}\")\n",
    "        return file_types\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"O arquivo fornecido não é um ZIP válido: {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar ZIP: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02b413d1-798a-4d64-8949-4446900b33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_csv(file_path, encodings=[\"utf-8\", \"ISO-8859-1\", \"utf-8-sig\"]):\n",
    "    \"\"\"\n",
    "    Diagnostica problemas em arquivos CSV: delimitador e encoding.\n",
    "    \"\"\"\n",
    "    print(f\"Diagnóstico do arquivo: {file_path}\")\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"Tentando com encoding: {encoding}\")\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                sample = f.read(2048)  # Lê os primeiros 2048 caracteres\n",
    "            print(f\"Primeiros caracteres ({encoding}):\")\n",
    "            print(sample[:500])  # Mostra apenas os primeiros 500 caracteres\n",
    "            print(\"\\n--- Fim da Amostra ---\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo com encoding {encoding}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b3eb38b-6ac4-47aa-a22d-71cad5bfeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_csv(file_path, encodings=[\"utf-8\", \"ISO-8859-1\", \"utf-8-sig\"], max_lines=5):\n",
    "    \"\"\"\n",
    "    Diagnostica problemas em arquivos CSV: delimitador e encoding.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Caminho do arquivo CSV a ser diagnosticado.\n",
    "        encodings (list): Lista de encodings para tentar.\n",
    "        max_lines (int): Número máximo de linhas a serem lidas para análise.\n",
    "\n",
    "    Returns:\n",
    "        dict: Informações diagnosticadas incluindo encoding funcional e conteúdo inicial.\n",
    "    \"\"\"\n",
    "    print(f\"=== Diagnóstico do arquivo: {file_path} ===\")\n",
    "    diagnostics = {\"file_path\": file_path, \"encoding\": None, \"sample\": None, \"error\": None}\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"Tentando com encoding: {encoding}\")\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                sample = [f.readline().strip() for _ in range(max_lines)]\n",
    "\n",
    "            diagnostics[\"encoding\"] = encoding\n",
    "            diagnostics[\"sample\"] = sample\n",
    "            print(f\"Arquivo lido com sucesso usando encoding '{encoding}'.\")\n",
    "            print(f\"Amostra das primeiras {max_lines} linhas:\")\n",
    "            print(\"\\n\".join(sample))\n",
    "            print(\"\\n--- Fim da Amostra ---\\n\")\n",
    "            return diagnostics  # Retorna na primeira tentativa bem-sucedida\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo com encoding {encoding}: {e}\")\n",
    "            diagnostics[\"error\"] = str(e)\n",
    "\n",
    "    # Retorna informações de erro se nenhum encoding funcionar\n",
    "    print(\"Falha ao diagnosticar o arquivo com os encodings fornecidos.\")\n",
    "    return diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21a4ae8d-1f68-4c4d-94de-699433f36b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xlsx_files(xlsx_files):\n",
    "    \"\"\"\n",
    "    Carrega arquivos XLSX em DataFrames e retorna um dicionário de DataFrames.\n",
    "\n",
    "    Args:\n",
    "        xlsx_files (list): Lista de dicionários com informações dos arquivos XLSX. Cada dicionário contém:\n",
    "            - 'name': Nome do arquivo.\n",
    "            - 'path': Caminho para o arquivo.\n",
    "\n",
    "    Returns:\n",
    "        dict: Um dicionário onde as chaves são os nomes dos arquivos e os valores são os DataFrames.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Para rastrear proveniência\n",
    "\n",
    "    dataframes = {}\n",
    "    for xlsx_info in xlsx_files:\n",
    "        try:\n",
    "            # Início da atividade de carga\n",
    "            exec_start = datetime.datetime.now()\n",
    "            \n",
    "            # Carregar o arquivo XLSX\n",
    "            df = pd.read_excel(xlsx_info[\"path\"])\n",
    "            dataframes[xlsx_info[\"name\"]] = df\n",
    "\n",
    "            # Rastrear proveniência\n",
    "            exec_end = datetime.datetime.now()\n",
    "            activity_key = f\"act-load-xlsx-{xlsx_info['name']}\"\n",
    "            dict_activities[activity_key] = doc_prov.activity(\n",
    "                f\"ufrj:load_xlsx_{xlsx_info['name']}\",\n",
    "                exec_start,\n",
    "                exec_end,\n",
    "                {\"prov:label\": escape_label(f\"Carregamento do arquivo XLSX: {xlsx_info['name']}\")}\n",
    "            )\n",
    "            doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "            entity_key = f\"ent-xlsx-{xlsx_info['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:{xlsx_info['name']}\",\n",
    "                {\"prov:label\": escape_label(f\"Arquivo XLSX carregado: {xlsx_info['name']}\"),\n",
    "                 \"prov:type\": \"void:Dataset\",\n",
    "                 \"prov:generatedAtTime\": exec_end.isoformat()}\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "            print(f\"Arquivo XLSX carregado: {xlsx_info['name']} com shape {df.shape}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar XLSX {xlsx_info['name']}: {e}\")\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c252bc4-2c43-4d95-a5df-32c997dc4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "def process_shp_files(shp_files):\n",
    "    \"\"\"\n",
    "    Carrega arquivos Shapefiles em GeoDataFrames e retorna um dicionário de GeoDataFrames.\n",
    "\n",
    "    Args:\n",
    "        shp_files (list): Lista de dicionários com informações dos arquivos Shapefile.\n",
    "            Cada dicionário contém:\n",
    "                - 'name': Nome do arquivo.\n",
    "                - 'path': Caminho completo para o arquivo.\n",
    "\n",
    "    Returns:\n",
    "        dict: Um dicionário onde as chaves são os nomes dos arquivos e os valores são os GeoDataFrames.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Para rastrear proveniência\n",
    "\n",
    "    geodataframes = {}\n",
    "    for shp_info in shp_files:\n",
    "        try:\n",
    "            # Início da atividade de carga\n",
    "            exec_start = datetime.datetime.now()\n",
    "            \n",
    "            # Carregar o arquivo Shapefile em um GeoDataFrame\n",
    "            gdf = gpd.read_file(shp_info[\"path\"])\n",
    "            geodataframes[shp_info[\"name\"]] = gdf\n",
    "\n",
    "            # Rastrear proveniência\n",
    "            exec_end = datetime.datetime.now()\n",
    "            activity_key = f\"act-load-shp-{shp_info['name']}\"\n",
    "            dict_activities[activity_key] = doc_prov.activity(\n",
    "                f\"ufrj:load_shp_{shp_info['name']}\",\n",
    "                exec_start,\n",
    "                exec_end,\n",
    "                {\"prov:label\": escape_label(f\"Carregamento do Shapefile: {shp_info['name']}\")}\n",
    "            )\n",
    "            doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "            entity_key = f\"ent-shp-{shp_info['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:{shp_info['name']}\",\n",
    "                {\"prov:label\": escape_label(f\"Shapefile carregado: {shp_info['name']}\"),\n",
    "                 \"prov:type\": \"void:Dataset\",\n",
    "                 \"prov:generatedAtTime\": exec_end.isoformat()}\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "            print(f\"Arquivo Shapefile carregado: {shp_info['name']} com shape {gdf.shape}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar Shapefile {shp_info['name']}: {e}\")\n",
    "\n",
    "    return geodataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f878a87-e607-4e44-86d2-3d39946695ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_unknown_files(unknown_files):\n",
    "    \"\"\"\n",
    "    Exibe uma mensagem com arquivos de formatos não reconhecidos e registra a proveniência.\n",
    "\n",
    "    Args:\n",
    "        unknown_files (list): Lista de dicionários com informações sobre arquivos não reconhecidos.\n",
    "            Cada dicionário contém:\n",
    "                - 'name': Nome do arquivo.\n",
    "                - 'path': Caminho completo para o arquivo.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\n",
    "\n",
    "    if unknown_files:\n",
    "        print(f\"Arquivos não reconhecidos encontrados: {[file['name'] for file in unknown_files]}\")\n",
    "\n",
    "        # Registrar proveniência da análise\n",
    "        exec_start = datetime.datetime.now()\n",
    "        activity_key = \"act-analyze-unknown-files\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(\n",
    "            \"ufrj:analyze_unknown_files\",\n",
    "            exec_start,\n",
    "            None,\n",
    "            {\"prov:label\": escape_label(\"Análise de arquivos desconhecidos\")}\n",
    "        )\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        # Criar entidades para cada arquivo desconhecido\n",
    "        for file_info in unknown_files:\n",
    "            entity_key = f\"ent-unknown-{file_info['name']}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:unknown_{file_info['name']}\",\n",
    "                {\"prov:label\": escape_label(f\"Arquivo desconhecido: {file_info['name']}\"),\n",
    "                 \"prov:type\": \"void:Dataset\",\n",
    "                 \"prov:location\": file_info[\"path\"]}\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "    else:\n",
    "        print(\"Nenhum arquivo não reconhecido foi encontrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cab894a6-a88a-427b-a10d-f00715eba650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zip_source(source_name, data_sources, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Processa um ZIP com base no seu conteúdo, chamando funções específicas para cada tipo de arquivo.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {}\n",
    "\n",
    "    exec_start = datetime.datetime.now()\n",
    "\n",
    "    # Analisar conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao analisar o conteúdo do ZIP '{source_name}'.\")\n",
    "        return {}\n",
    "\n",
    "    # Criar atividade de processamento de ZIP\n",
    "    activity_key = f\"act-process-zip-{source_name}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        f\"ufrj:process_zip_{source_name}\",\n",
    "        exec_start,\n",
    "        None,\n",
    "        {\"prov:label\": escape_label(f\"Processamento do ZIP: {source_name}\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Associar a entidade do ZIP original à atividade\n",
    "    zip_entity_key = f\"ent-{source_name}\"\n",
    "    if zip_entity_key in dict_entities:\n",
    "        doc_prov.used(dict_activities[activity_key], dict_entities[zip_entity_key])\n",
    "\n",
    "    # Processar arquivos por tipo\n",
    "    dataframes = {}\n",
    "    geodataframes = {}\n",
    "    if file_types[\"csv\"]:\n",
    "        dataframes.update(process_csv_files(file_types[\"csv\"]))\n",
    "    if file_types[\"xlsx\"]:\n",
    "        dataframes.update(process_xlsx_files(file_types[\"xlsx\"]))\n",
    "    if file_types[\"shp\"]:\n",
    "        geodataframes.update(process_shp_files(file_types[\"shp\"]))\n",
    "\n",
    "    # Relatar arquivos não reconhecidos\n",
    "    report_unknown_files(file_types[\"others\"])\n",
    "\n",
    "    exec_end = datetime.datetime.now()\n",
    "    dict_activities[activity_key].add_attributes({\"prov:endTime\": exec_end.isoformat()})\n",
    "\n",
    "    return {\"dataframes\": dataframes, \"geodataframes\": geodataframes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b6c917a-b7cb-40f6-bd89-29517d9622a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import datetime\n",
    "\n",
    "def extract_and_load_shapefile_from_zip(source_name, data_sources, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Extrai e carrega Shapefiles de um ZIP com rastreamento de proveniência.\n",
    "    Args:\n",
    "        source_name (str): Nome da fonte de dados no `data_sources`.\n",
    "        data_sources (dict): Dicionário contendo informações das fontes de dados.\n",
    "        temp_dir (str): Diretório temporário para extração dos arquivos.\n",
    "    Returns:\n",
    "        dict: Dicionário contendo informações sobre os Shapefiles e seus GeoDataFrames.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities  # Proveniência\n",
    "\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {}\n",
    "\n",
    "    exec_start = datetime.datetime.now()\n",
    "    shapefile_info = {}\n",
    "\n",
    "    try:\n",
    "        # Baixar o arquivo ZIP\n",
    "        response = requests.get(source[\"url\"])\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Erro ao baixar o arquivo: {response.status_code}\")\n",
    "\n",
    "        # Abrir o ZIP em memória e extrair\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zf:\n",
    "            zf.extractall(temp_dir)\n",
    "            extracted_files = zf.namelist()\n",
    "\n",
    "        # Procurar arquivos .shp\n",
    "        shp_files = [file for file in extracted_files if file.endswith(\".shp\")]\n",
    "\n",
    "        if not shp_files:\n",
    "            raise ValueError(\"Nenhum arquivo Shapefile (.shp) encontrado no ZIP.\")\n",
    "\n",
    "        # Criar atividade de extração no documento de proveniência\n",
    "        activity_key = f\"act-extract-shp-{source_name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(\n",
    "            f\"ufrj:extract_shp_{source_name}\",\n",
    "            exec_start,\n",
    "            None,\n",
    "            {\"prov:label\": escape_label(f\"Extração de Shapefile do ZIP: {source_name}\")}\n",
    "        )\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        # Associar atividade ao ZIP original\n",
    "        zip_entity_key = f\"ent-{source_name}\"\n",
    "        if zip_entity_key in dict_entities:\n",
    "            doc_prov.used(dict_activities[activity_key], dict_entities[zip_entity_key])\n",
    "\n",
    "        # Processar todos os Shapefiles encontrados\n",
    "        for shp_file in shp_files:\n",
    "            shapefile_path = os.path.join(temp_dir, shp_file)\n",
    "            gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "            # Criar entidade para cada Shapefile\n",
    "            entity_key = f\"ent-shp-{source_name}-{os.path.basename(shp_file)}\"\n",
    "            dict_entities[entity_key] = doc_prov.entity(\n",
    "                f\"ufrj:{source_name}_{os.path.basename(shp_file)}\",\n",
    "                {\n",
    "                    \"prov:label\": escape_label(f\"Shapefile extraído: {os.path.basename(shp_file)}\"),\n",
    "                    \"prov:type\": \"void:Dataset\",\n",
    "                    \"prov:generatedAtTime\": datetime.datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "            # Armazenar informações do Shapefile\n",
    "            shapefile_info[os.path.basename(shp_file)] = {\n",
    "                \"path\": shapefile_path,\n",
    "                \"crs\": gdf.crs,\n",
    "                \"columns\": gdf.columns.tolist(),\n",
    "                \"geometry_type\": gdf.geom_type.unique(),\n",
    "                \"gdf\": gdf\n",
    "            }\n",
    "            print(f\"Shapefile carregado: {shapefile_path} com shape {gdf.shape}\")\n",
    "\n",
    "        exec_end = datetime.datetime.now()\n",
    "        dict_activities[activity_key].add_attributes({\"prov:endTime\": exec_end.isoformat()})\n",
    "\n",
    "        return shapefile_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar Shapefile no ZIP '{source_name}': {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "716a3e91-04d8-4964-b8cd-a3cbff355109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_sources_with_zip(data_sources, source_name, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Atualiza `data_sources` com os arquivos extraídos de um ZIP, criando uma relação hierárquica.\n",
    "    \"\"\"\n",
    "    # Obter fonte original\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Analisar o conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao processar ZIP '{source_name}'.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Criar relações de proveniência\n",
    "    for file_type, files in file_types.items():\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        for i, file_info in enumerate(files):\n",
    "            child_name = f\"{source_name}_{file_type}_{i+1}\"\n",
    "            data_sources[child_name] = {\n",
    "                \"url\": file_info[\"path\"],  # Caminho local do arquivo extraído\n",
    "                \"type\": file_type,        # Tipo do arquivo (csv, xlsx, shp, etc.)\n",
    "                \"parent\": source_name,    # Referência ao ZIP pai\n",
    "                \"sep\": source.get(\"sep\", \";\") if file_type == \"csv\" else None,\n",
    "                \"encoding\": source.get(\"encoding\", \"utf-8\") if file_type == \"csv\" else None,\n",
    "                \"header\": source.get(\"header\", 0) if file_type == \"csv\" else None,\n",
    "                \"date_columns\": source.get(\"date_columns\", []) if file_type == \"csv\" else None,\n",
    "            }\n",
    "            print(f\"Adicionado {child_name} como filho de {source_name}.\")\n",
    "    \n",
    "    print(f\"Data sources atualizados com arquivos de '{source_name}'.\")\n",
    "    return data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0462131f-eba2-428c-a2d5-e56fe66c0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_provenance_for_zip_and_children(parent_source, children_sources, activity_prefix=\"process-zip\"):\n",
    "    \"\"\"\n",
    "    Registra a proveniência entre o ZIP pai e os arquivos extraídos (filhos).\n",
    "    A atividade é associada ao agente 'ag-eda-ipynb' para todos os filhos.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Criar atividade para o processamento do ZIP\n",
    "    exec_start = datetime.datetime.now()\n",
    "    activity_key = f\"{activity_prefix}-{parent_source}\"\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        f\"ufrj:{activity_prefix}_{parent_source}\", exec_start, None,\n",
    "        {\"prov:label\": escape_label(f\"Processamento do ZIP {parent_source}\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    # Registrar cada filho como derivado do pai\n",
    "    for child_source in children_sources:\n",
    "        entity_key = f\"ent-{child_source}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{child_source}\", {\n",
    "            \"prov:label\": escape_label(f\"Arquivo derivado de {parent_source}\"),\n",
    "            \"prov:type\": \"void:Dataset\"\n",
    "        })\n",
    "        \n",
    "        # Relacionar pai e filho\n",
    "        doc_prov.wasDerivedFrom(\n",
    "            dict_entities[entity_key], dict_entities.get(f\"ent-{parent_source}\")\n",
    "        )\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "        # Adiciona a atividade de proveniência do arquivo como \"gerado por\" o agente IPYNB\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "    print(f\"Proveniência registrada para arquivos derivados de {parent_source}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b8adbc9-5994-4efd-978d-69f183713f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_sources_with_zip(data_sources, source_name, temp_dir=\"./temp\"):\n",
    "    \"\"\"\n",
    "    Atualiza `data_sources` com os arquivos extraídos de um ZIP, criando uma relação hierárquica.\n",
    "    E registra a proveniência entre o ZIP (pai) e os arquivos extraídos (filhos).\n",
    "    \"\"\"\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Analisar o conteúdo do ZIP\n",
    "    file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "    if not file_types:\n",
    "        print(f\"Erro ao processar ZIP '{source_name}'.\")\n",
    "        return data_sources\n",
    "\n",
    "    # Lista de filhos (arquivos extraídos)\n",
    "    children_sources = []\n",
    "\n",
    "    for file_type, files in file_types.items():\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        for i, file_info in enumerate(files):\n",
    "            child_name = f\"{source_name}_{file_type}_{i+1}\"\n",
    "            data_sources[child_name] = {\n",
    "                \"url\": file_info[\"path\"],  # Caminho local do arquivo extraído\n",
    "                \"type\": file_type,        # Tipo do arquivo (csv, xlsx, shp, etc.)\n",
    "                \"parent\": source_name,    # Referência ao ZIP pai\n",
    "                \"sep\": source.get(\"sep\", \";\") if file_type == \"csv\" else None,\n",
    "                \"encoding\": source.get(\"encoding\", \"utf-8\") if file_type == \"csv\" else None,\n",
    "                \"header\": source.get(\"header\", 0) if file_type == \"csv\" else None,\n",
    "                \"date_columns\": source.get(\"date_columns\", []) if file_type == \"csv\" else None,\n",
    "            }\n",
    "            children_sources.append(child_name)\n",
    "            print(f\"Adicionado {child_name} como filho de {source_name}.\")\n",
    "\n",
    "    # Registrar a proveniência para o ZIP e seus filhos\n",
    "    register_provenance_for_zip_and_children(source_name, children_sources)\n",
    "    \n",
    "    print(f\"Data sources atualizados com arquivos de '{source_name}'.\")\n",
    "    return data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df4ff600-8332-4879-b792-63dcc13dc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_source_csv(source_name, data_sources):\n",
    "    \"\"\"\n",
    "    Carrega dados com base no nome da fonte e na configuração em data_sources.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com os dados carregados, ou None se houver erro.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents,  dict_activities, dict_entities  # Declare global variables\n",
    "    #save execution start time\n",
    "    execStartTime = datetime.datetime.now()\n",
    "\n",
    "    source = data_sources.get(source_name)\n",
    "   \n",
    "    if not source: #or source.get(\"type\") != \"csv\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é do tipo CSV.\")\n",
    "        return None\n",
    "\n",
    "    file_type = source.get(\"type\")\n",
    "    url = source.get(\"url\")\n",
    "    sep = source.get(\"sep\", \";\")  # Valor padrão para CSV\n",
    "    encoding = source.get(\"encoding\", \"utf-8\")  # Valor padrão para codificação\n",
    "    date_columns = source.get(\"date_columns\", [])  \n",
    "\n",
    "    try:\n",
    "        if file_type == \"csv\":\n",
    "             # Caso específico para tabela_pocos_2024\n",
    "            if source_name == \"tabela_pocos_2024\":\n",
    "                df = pd.read_csv(url, encoding=\"ANSI\", sep=sep)\n",
    "            # Caso específico para tabela_dados_geoquimica\n",
    "            elif source_name == \"tabela_dados_geoquimica\":\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, header=1)  # Cabeçalho na segunda linha\n",
    "            else:\n",
    "                #header_line = detect_csv_header(url, delimiter=sep)\n",
    "                df = pd.read_csv(url, sep=sep, encoding=encoding, parse_dates=date_columns)\n",
    "        elif file_type == \"xlsx\":\n",
    "            df = pd.read_excel(url)\n",
    "        else:\n",
    "            print(f\"Tipo de arquivo '{file_type}' não suportado.\")\n",
    "            return None\n",
    "        print(f\"Dados carregados com sucesso para '{source_name}'.\")\n",
    "    \n",
    "        # End execution time for provenance tracking\n",
    "        execEndTime = datetime.datetime.now()\n",
    "    \n",
    "        # Criar atividade com horário de término da execução\n",
    "        activity_key = f\"act-carga-{source_name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}\", execStartTime, execEndTime)\n",
    "    \n",
    "        # Associar a atividade ao agente\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "        \n",
    "        # Associar a atividade com os dados carregados\n",
    "        entity_key = f\"ent-{source_name}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{source_name}\", {\n",
    "            \"prov:label\": escape_label(f\"Dataset carregado: {source_name}\"),\"prov:type\": \"void:Dataset\", \"prov:generatedAtTime\": execEndTime.isoformat(),})\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "    \n",
    "        # Associar a atividade ufrj:carga à entidade correspondente criada pela ANP\n",
    "        anp_entity_key = f\"ent-{source_name.replace('_', '-')}\"  # Convert to ANP format (e.g., `tabela_pocos_2024` -> `ent-tabela-pocos-2024`)\n",
    "        if anp_entity_key in dict_entities:\n",
    "            # Establish the prov:used relationship\n",
    "            doc_prov.used(dict_activities[activity_key], dict_entities[anp_entity_key])\n",
    "        else:\n",
    "            print(f\"Warning: ANP entity '{anp_entity_key}' not found for activity '{activity_key}'.\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados de '{source_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b00d974a-e712-421e-9511-a00d2ff9563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_zip_source(source_name, data_sources, temp_dir=\"./dados/temp\"):\n",
    "    \"\"\"\n",
    "    Carrega dados de um ZIP, extrai arquivos (CSV e outros) e retorna os DataFrames resultantes.\n",
    "    Args:\n",
    "        source_name (str): Nome da fonte no `data_sources`.\n",
    "        data_sources (dict): Dicionário de fontes de dados.\n",
    "        temp_dir (str): Diretório temporário para extração.\n",
    "    Returns:\n",
    "        dict: Dicionário com DataFrames e GeoDataFrames carregados.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "    execStartTime = datetime.datetime.now()\n",
    "\n",
    "    # Verificar se a fonte existe\n",
    "    source = data_sources.get(source_name)\n",
    "    if not source or source.get(\"type\") != \"zip\":\n",
    "        print(f\"Fonte '{source_name}' não encontrada ou não é um ZIP.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        # Analisar conteúdo do ZIP\n",
    "        file_types = analyze_zip_content(source[\"url\"], temp_dir=temp_dir)\n",
    "        if not file_types:\n",
    "            print(f\"Erro ao analisar conteúdo do ZIP '{source_name}'.\")\n",
    "            return {}\n",
    "\n",
    "        # Inicializar dicionário para resultados\n",
    "        dataframes = {}\n",
    "        geodataframes = {}\n",
    "\n",
    "        # Processar arquivos CSV\n",
    "        for csv_info in file_types[\"csv\"]:\n",
    "            csv_name = csv_info[\"name\"]\n",
    "            csv_path = csv_info[\"path\"]\n",
    "\n",
    "            try:\n",
    "                # Detectar cabeçalho e delimitador automaticamente\n",
    "                #header_line = detect_csv_header(csv_path, max_lines_to_check=5, delimiter=\",\")\n",
    "                df = pd.read_csv(csv_path, header=header_line, sep=None, engine=\"python\", encoding=\"utf-8\")\n",
    "\n",
    "                # Registrar proveniência\n",
    "                execEndTime = datetime.datetime.now()\n",
    "                activity_key = f\"act-carga-{source_name}-{csv_name}\"\n",
    "                dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}_{csv_name}\", execStartTime, execEndTime)\n",
    "                doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "                entity_key = f\"ent-{source_name}-{csv_name}\"\n",
    "                dict_entities[entity_key] = doc_prov.entity(\n",
    "                    f\"ufrj:{source_name}_{csv_name}\",\n",
    "                    {\n",
    "                        \"prov:label\": escape_label(f\"Dataset carregado: {csv_name}\"),\n",
    "                        \"prov:type\": \"void:Dataset\",\n",
    "                        \"prov:generatedAtTime\": execEndTime.isoformat(),\n",
    "                    },\n",
    "                )\n",
    "                doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "                # Adicionar ao dicionário de DataFrames\n",
    "                dataframes[csv_name] = df\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar CSV '{csv_name}': {e}\")\n",
    "\n",
    "        # Processar arquivos Shapefiles\n",
    "        for shp_info in file_types[\"shp\"]:\n",
    "            shp_name = shp_info[\"name\"]\n",
    "            shp_path = shp_info[\"path\"]\n",
    "\n",
    "            try:\n",
    "                gdf = gpd.read_file(shp_path)\n",
    "\n",
    "                # Registrar proveniência\n",
    "                execEndTime = datetime.datetime.now()\n",
    "                activity_key = f\"act-carga-{source_name}-{shp_name}\"\n",
    "                dict_activities[activity_key] = doc_prov.activity(f\"ufrj:carga_{source_name}_{shp_name}\", execStartTime, execEndTime)\n",
    "                doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "                entity_key = f\"ent-{source_name}-{shp_name}\"\n",
    "                dict_entities[entity_key] = doc_prov.entity(\n",
    "                    f\"ufrj:{source_name}_{shp_name}\",\n",
    "                    {\n",
    "                        \"prov:label\": escape_label(f\"GeoDataset carregado: {shp_name}\"),\n",
    "                        \"prov:type\": \"void:Dataset\",\n",
    "                        \"prov:generatedAtTime\": execEndTime.isoformat(),\n",
    "                    },\n",
    "                )\n",
    "                doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "                # Adicionar ao dicionário de GeoDataFrames\n",
    "                geodataframes[shp_name] = gdf\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar Shapefile '{shp_name}': {e}\")\n",
    "\n",
    "        # Relatar arquivos não reconhecidos\n",
    "        report_unknown_files(file_types[\"others\"])\n",
    "\n",
    "        print(f\"DataFrames carregados: {list(dataframes.keys())}\")\n",
    "        print(f\"GeoDataFrames carregados: {list(geodataframes.keys())}\")\n",
    "\n",
    "        return {\"dataframes\": dataframes, \"geodataframes\": geodataframes}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar ZIP '{source_name}': {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc74c138-fbb6-4b29-9aad-cc19bc0dd3a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"def inspect_dataframes(dataframes):\n",
    "    \n",
    "    #Inspeciona um dicionário de DataFrames, exibindo colunas, shape e estatísticas descritivas.\n",
    "   \n",
    "    for name, df in dataframes.items():\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(\"\\nHead:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nDescribe:\")\n",
    "        print(df.describe(include='all', datetime_is_numeric=True))\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "388302a3-c694-409a-8ce3-df0177baba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def clean_dataframe_columns(dataframes):\n",
    "  \n",
    "    #Realiza o saneamento e a renomeação de colunas em um conjunto de DataFrames.\n",
    "\n",
    "    cleaned_dataframes = {}\n",
    "    for name, df in dataframes.items():\n",
    "        # Renomear colunas removendo espaços e convertendo para letras minúsculas\n",
    "        df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "        # Registrar atividade de saneamento na proveniência\n",
    "        execStartTime = datetime.datetime.now()\n",
    "        activity_key = f\"act-saneamento-{name}\"\n",
    "        dict_activities[activity_key] = doc_prov.activity(f\"ufrj:saneamento_{name}\", execStartTime, None, {\n",
    "            \"prov:label\": escape_label(f\"Saneamento de colunas: {name}\")\n",
    "        })\n",
    "        doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "        entity_key = f\"ent-saneado-{name}\"\n",
    "        dict_entities[entity_key] = doc_prov.entity(f\"ufrj:{name}_saneado\", {\n",
    "            \"prov:label\": escape_label(f\"Dataset saneado: {name}\"),\n",
    "            \"prov:type\": \"void:Dataset\",\n",
    "        })\n",
    "        doc_prov.wasGeneratedBy(dict_entities[entity_key], dict_activities[activity_key])\n",
    "\n",
    "        cleaned_dataframes[name] = df\n",
    "    return cleaned_dataframes\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15532133-e0c4-4fe0-8db4-a40e4443004c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c701c-586b-4efc-909a-9aa79af38af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0ab2cc-cc26-432b-a2e3-b3f6e10faa81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_pocos_orig = load_data_from_source(\"tabela_pocos_2024\", data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "e93ead43-b565-4fa0-b0c7-fec6078bf061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pocos_orig.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "498bbbcd-bf7d-465c-97dc-0e853178826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sismica_2023_orig = load_data_from_source(\"levantamento_sismico_2023\", data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "ee466c73-7e1a-43e8-a2a5-84e351f624af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sismica_2023_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "98157ebb-b8b4-470c-b5c6-17b68710a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##df_amostras = load_data_from_source(\"amostras_rochas_fluidos\", data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "897f1c6f-dc55-4f22-97fe-fe518fa45719",
   "metadata": {},
   "outputs": [],
   "source": [
    "##data_sources = update_data_sources_with_zip(data_sources, \"amostras_rochas_fluidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3d527-169a-4c5e-8d11-ee81c3522d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0111a0d0-293c-4143-8223-89b0f55778cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eda_dataset():\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "    activity_key = \"act-create-ds-eda\"\n",
    "    exec_start = datetime.datetime.now()\n",
    "\n",
    "    datasets = [\n",
    "        {\"name\": \"df_sismica_2023_orig\", \"source\": \"levantamento_sismico_2023\"},\n",
    "        {\"name\": \"df_pocos_orig\", \"source\": \"tabela_pocos_2024\"},\n",
    "        {\"name\": \"df_lev_geoq_2022_orig\", \"source\": \"tabela_levantamentos_geoquimica\"},\n",
    "        {\"name\": \"df_geoq_2021_orig\", \"source\": \"tabela_dados_geoquimica\"},\n",
    "        {\"name\": \"df_reservas_orig\", \"source\": \"reservas_nacionais_hc\"},\n",
    "        {\"name\": \"df_poco_2023_orig\", \"source\": \"pocos_perfurados_2023\"},\n",
    "    ]\n",
    "\n",
    "    loaded_datasets = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        print(f\"Carregando dataset: {dataset['name']} (Fonte: {dataset['source']})\")\n",
    "        df = load_data_from_source_csv(dataset[\"source\"], data_sources)\n",
    "        if df is not None:\n",
    "            loaded_datasets[dataset[\"name\"]] = df\n",
    "            print(f\"Dataset '{dataset['name']}' carregado com sucesso. Shape: {df.shape}\")\n",
    "        else:\n",
    "            print(f\"Falha ao carregar o dataset '{dataset['name']}' (Fonte: {dataset['source']}).\")\n",
    "\n",
    "    exec_end = datetime.datetime.now()\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        \"ufrj:create-ds-eda\", exec_start, exec_end,\n",
    "        {\"prov:label\": escape_label(\"Criação de datasets para EDA\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    print(\"Datasets prontos para análise.\")\n",
    "    return loaded_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "23400fd1-92ee-4264-99be-a71789622927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal para gerar o documento de proveniência e processar datasets.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Inicializar proveniência\n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "    print(\"Agents:\", dict_agents.keys())\n",
    "    print(\"Activities:\", dict_activities.keys())\n",
    "    print(\"Entities:\", dict_entities.keys())\n",
    "\n",
    "    # Criar dataset EDA\n",
    "    datasets = create_eda_dataset()\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        print(f\"Dataset '{name}' pronto para análise. Shape: {df.shape}\")\n",
    "\n",
    "    # Serializar e gerar saídas de proveniência\n",
    "    gerar_prov_outputs(doc_prov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9788113-3467-422f-bc7d-6162031ceb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: dict_keys(['ag-orgbr', 'ag-anp', 'ag-ufrj', 'ag-ppgi', 'ag-greco', 'ag-author-ubirajara', 'ag-author-sergio', 'ag-author-jorge', 'ag-petrobras', 'ag-eda-ipynb'])\n",
      "Activities: dict_keys(['act-create-ds', 'act-create-ds-eda', 'act-save-ipynb'])\n",
      "Entities: dict_keys(['ent-amostras-rochas-fluidos', 'ent-setores-sirgas', 'ent-blocos-exploratorios', 'ent-campos-producao', 'ent-reservas-nacionais-hc', 'ent-pocos-perfurados-2023', 'ent-tabela-levantamentos-geoquimica', 'ent-tabela-dados-geoquimica', 'ent-levantamento-sismico-2023', 'ent-tabela-pocos-2024', 'ent-anp-dados_tec-ds', 'ent-eda-ipynb', 'ent-git-eda'])\n",
      "Carregando dataset: df_sismica_2023_orig (Fonte: levantamento_sismico_2023)\n",
      "Dados carregados com sucesso para 'levantamento_sismico_2023'.\n",
      "Dataset 'df_sismica_2023_orig' carregado com sucesso. Shape: (52, 15)\n",
      "Carregando dataset: df_pocos_orig (Fonte: tabela_pocos_2024)\n",
      "Dados carregados com sucesso para 'tabela_pocos_2024'.\n",
      "Dataset 'df_pocos_orig' carregado com sucesso. Shape: (30827, 60)\n",
      "Carregando dataset: df_lev_geoq_2022_orig (Fonte: tabela_levantamentos_geoquimica)\n",
      "Dados carregados com sucesso para 'tabela_levantamentos_geoquimica'.\n",
      "Dataset 'df_lev_geoq_2022_orig' carregado com sucesso. Shape: (69, 8)\n",
      "Carregando dataset: df_geoq_2021_orig (Fonte: tabela_dados_geoquimica)\n",
      "Dados carregados com sucesso para 'tabela_dados_geoquimica'.\n",
      "Dataset 'df_geoq_2021_orig' carregado com sucesso. Shape: (4665, 38)\n",
      "Carregando dataset: df_reservas_orig (Fonte: reservas_nacionais_hc)\n",
      "Dados carregados com sucesso para 'reservas_nacionais_hc'.\n",
      "Dataset 'df_reservas_orig' carregado com sucesso. Shape: (419, 10)\n",
      "Carregando dataset: df_poco_2023_orig (Fonte: pocos_perfurados_2023)\n",
      "Dados carregados com sucesso para 'pocos_perfurados_2023'.\n",
      "Dataset 'df_poco_2023_orig' carregado com sucesso. Shape: (106, 59)\n",
      "Datasets prontos para análise.\n",
      "Dataset 'df_sismica_2023_orig' pronto para análise. Shape: (52, 15)\n",
      "Dataset 'df_pocos_orig' pronto para análise. Shape: (30827, 60)\n",
      "Dataset 'df_lev_geoq_2022_orig' pronto para análise. Shape: (69, 8)\n",
      "Dataset 'df_geoq_2021_orig' pronto para análise. Shape: (4665, 38)\n",
      "Dataset 'df_reservas_orig' pronto para análise. Shape: (419, 10)\n",
      "Dataset 'df_poco_2023_orig' pronto para análise. Shape: (106, 59)\n",
      "Provenance graph generated successfully: EDA-PROV.png\n",
      "Provenance serialized as XML and TTL.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ab20f-8dd1-4ea0-b3c9-9a4945c33aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11b7ef-0e0f-451f-b4b6-745bd31bb9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31638f-050f-437f-85e3-bf8063242cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a0b4c-ff3f-461c-96d4-e70329ffc1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10447b9-d1c4-4491-bc7a-f7c3fbef0ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b346143-5414-4788-992c-b92c65ad6c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04bba1-10c6-446d-bfbc-e659021932cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94a4a0-7469-4072-a0f0-3287f8d37580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d076ec5-5ddf-45d7-b020-dfa716aa7c88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "974fb0b7-e9c2-4fa4-bf9c-1b7cdec33a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "    print(\"Agents:\", dict_agents.keys())\n",
    "    print(\"Activities:\", dict_activities.keys())\n",
    "    print(\"Entities:\", dict_entities.keys())\n",
    "\n",
    "    datasets = create_eda_dataset()\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        print(f\"Dataset '{name}' pronto para análise. Shape: {df.shape}\")\n",
    "\n",
    "    gerar_prov_outputs(doc_prov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4aefd1c-a2d5-493b-a86a-ced3102b6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal para gerar o documento de proveniência.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Inicializar proveniência\n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "    # Verificar inicialização\n",
    "    print(\"Agents:\", dict_agents.keys())\n",
    "    print(\"Activities:\", dict_activities.keys())\n",
    "    print(\"Entities:\", dict_entities.keys())\n",
    "\n",
    "    # Criar datasets para análise\n",
    "    datasets = create_eda_dataset()\n",
    "\n",
    "    # Log dos datasets carregados\n",
    "    for dataset_name, df in datasets.items():\n",
    "        print(f\"Dataset '{dataset_name}' pronto para análise. Shape: {df.shape}\")\n",
    "\n",
    "    # Serializar e gerar saída de proveniência\n",
    "    gerar_prov_outputs(doc_prov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6736787b-befa-4983-a060-fb174e84f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: dict_keys(['ag-orgbr', 'ag-anp', 'ag-ufrj', 'ag-ppgi', 'ag-greco', 'ag-author-ubirajara', 'ag-author-sergio', 'ag-author-jorge', 'ag-petrobras', 'ag-eda-ipynb'])\n",
      "Activities: dict_keys(['act-create-ds', 'act-create-ds-eda', 'act-save-ipynb'])\n",
      "Entities: dict_keys(['ent-amostras-rochas-fluidos', 'ent-setores-sirgas', 'ent-blocos-exploratorios', 'ent-campos-producao', 'ent-reservas-nacionais-hc', 'ent-pocos-perfurados-2023', 'ent-tabela-levantamentos-geoquimica', 'ent-tabela-dados-geoquimica', 'ent-levantamento-sismico-2023', 'ent-tabela-pocos-2024', 'ent-anp-dados_tec-ds', 'ent-eda-ipynb', 'ent-git-eda'])\n",
      "Carregando dataset: df_sismica_2023_orig (Fonte: levantamento_sismico_2023)\n",
      "Erro ao carregar dados de 'levantamento_sismico_2023': name 'detect_csv_header' is not defined\n",
      "Falha ao carregar o dataset 'df_sismica_2023_orig' (Fonte: levantamento_sismico_2023).\n",
      "Carregando dataset: df_pocos_orig (Fonte: tabela_pocos_2024)\n",
      "Dados carregados com sucesso para 'tabela_pocos_2024'.\n",
      "Dataset 'df_pocos_orig' carregado com sucesso. Shape: (30827, 60)\n",
      "Carregando dataset: df_lev_geoq_2022 (Fonte: tabela_levantamentos_geoquimica)\n",
      "Erro ao carregar dados de 'tabela_levantamentos_geoquimica': name 'detect_csv_header' is not defined\n",
      "Falha ao carregar o dataset 'df_lev_geoq_2022' (Fonte: tabela_levantamentos_geoquimica).\n",
      "Carregando dataset: df_geoq_2021 (Fonte: tabela_dados_geoquimica)\n",
      "Dados carregados com sucesso para 'tabela_dados_geoquimica'.\n",
      "Dataset 'df_geoq_2021' carregado com sucesso. Shape: (4665, 38)\n",
      "Carregando dataset: df_reservas (Fonte: reservas_nacionais_hc)\n",
      "Fonte 'reservas_nacionais_hc' não encontrada ou não é do tipo CSV.\n",
      "Falha ao carregar o dataset 'df_reservas' (Fonte: reservas_nacionais_hc).\n",
      "Activities after data load: dict_keys(['act-create-ds', 'act-create-ds-eda', 'act-save-ipynb', 'act-carga-tabela_pocos_2024', 'act-carga-tabela_dados_geoquimica'])\n",
      "Entities after data load: dict_keys(['ent-amostras-rochas-fluidos', 'ent-setores-sirgas', 'ent-blocos-exploratorios', 'ent-campos-producao', 'ent-reservas-nacionais-hc', 'ent-pocos-perfurados-2023', 'ent-tabela-levantamentos-geoquimica', 'ent-tabela-dados-geoquimica', 'ent-levantamento-sismico-2023', 'ent-tabela-pocos-2024', 'ent-anp-dados_tec-ds', 'ent-eda-ipynb', 'ent-git-eda', 'ent-tabela_pocos_2024', 'ent-tabela_dados_geoquimica'])\n",
      "Dataset creation tracked in provenance.\n",
      "Dataset 'df_pocos_orig' pronto para análise. Shape: (30827, 60)\n",
      "Dataset 'df_geoq_2021' pronto para análise. Shape: (4665, 38)\n",
      "Provenance graph generated successfully: EDA-PROV.png\n",
      "Provenance serialized as XML and TTL.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93858b-b3f6-41c8-84c9-cdce067cdee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad9e56-1162-48cd-a4c6-cf45db109160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b022390-096b-4496-82f2-163c3a35d74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f99d8-55e6-4241-b281-c15887ad2f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "373b39d0-6baf-47de-84d3-1e3e15fdd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_eda_dataset():\n",
    "    \"\"\"\n",
    "    Lógica para criar o dataset EDA.\n",
    "    Adicione toda a lógica para manipulação de dados e associação de proveniência.\n",
    "    \"\"\"\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "    activity_key = \"act-create-ds-eda\"\n",
    "    exec_start = datetime.datetime.now()\n",
    "\n",
    "\n",
    "    # Carregar datasets\n",
    "    df_sismica_2023_orig = load_data_from_source_csv(\"levantamento_sismico_2023\", data_sources)\n",
    "    df_pocos_orig = load_data_from_source_csv(\"tabela_pocos_2024\", data_sources)\n",
    "    df_lev_geoq_2022 = load_data_from_source_csv(\"tabela_levantamentos_geoquimica\", data_sources)\n",
    "    df_geoq_2021 = load_data_from_source_csv(\"tabela_dados_geoquimica\", data_sources)\n",
    "    df_reservas = load_data_from_source_csv(\"reservas_nacionais_hc\", data_sources)\n",
    "\n",
    "    # Debugging provenance dictionaries\n",
    "    print(\"Activities after data load:\", dict_activities.keys())\n",
    "    print(\"Entities after data load:\", dict_entities.keys())\n",
    "    \n",
    "    exec_end = datetime.datetime.now()\n",
    "\n",
    "    # Atualiza atividade no documento de proveniência\n",
    "    dict_activities[activity_key] = doc_prov.activity(\n",
    "        \"ufrj:create-ds-eda\", exec_start, exec_end,\n",
    "        {\"prov:label\": escape_label(\"Criação de datasets para EDA\")}\n",
    "    )\n",
    "    doc_prov.wasAssociatedWith(dict_activities[activity_key], dict_agents[\"ag-eda-ipynb\"])\n",
    "\n",
    "    print(\"Dataset creation tracked in provenance.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "281c37a4-6a78-4b70-ac95-3aed03f21d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: dict_keys(['ag-orgbr', 'ag-anp', 'ag-ufrj', 'ag-ppgi', 'ag-greco', 'ag-author-ubirajara', 'ag-author-sergio', 'ag-author-jorge', 'ag-petrobras', 'ag-eda-ipynb'])\n",
      "Activities: dict_keys(['act-create-ds', 'act-create-ds-eda', 'act-save-ipynb'])\n",
      "Entities: dict_keys(['ent-amostras-rochas-fluidos', 'ent-setores-sirgas', 'ent-blocos-exploratorios', 'ent-campos-producao', 'ent-reservas-nacionais-hc', 'ent-pocos-perfurados-2023', 'ent-tabela-levantamentos-geoquimica', 'ent-tabela-dados-geoquimica', 'ent-levantamento-sismico-2023', 'ent-tabela-pocos-2024', 'ent-anp-dados_tec-ds', 'ent-eda-ipynb', 'ent-git-eda'])\n",
      "Erro ao carregar dados de 'levantamento_sismico_2023': name 'detect_csv_header' is not defined\n",
      "Dados carregados com sucesso para 'tabela_pocos_2024'.\n",
      "Erro ao carregar dados de 'tabela_levantamentos_geoquimica': name 'detect_csv_header' is not defined\n",
      "Dados carregados com sucesso para 'tabela_dados_geoquimica'.\n",
      "Fonte 'reservas_nacionais_hc' não encontrada ou não é do tipo CSV.\n",
      "Activities after data load: dict_keys(['act-create-ds', 'act-create-ds-eda', 'act-save-ipynb', 'act-carga-tabela_pocos_2024', 'act-carga-tabela_dados_geoquimica'])\n",
      "Entities after data load: dict_keys(['ent-amostras-rochas-fluidos', 'ent-setores-sirgas', 'ent-blocos-exploratorios', 'ent-campos-producao', 'ent-reservas-nacionais-hc', 'ent-pocos-perfurados-2023', 'ent-tabela-levantamentos-geoquimica', 'ent-tabela-dados-geoquimica', 'ent-levantamento-sismico-2023', 'ent-tabela-pocos-2024', 'ent-anp-dados_tec-ds', 'ent-eda-ipynb', 'ent-git-eda', 'ent-tabela_pocos_2024', 'ent-tabela_dados_geoquimica'])\n",
      "Dataset creation tracked in provenance.\n",
      "Provenance graph generated successfully: EDA-PROV.png\n",
      "Provenance serialized as XML and TTL.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #Função principal para gerar o documento de proveniência.\n",
    "    \n",
    "    # Initialize provenance objects\n",
    "    global doc_prov, dict_agents, dict_activities, dict_entities\n",
    "\n",
    "    # Adicionar inicialização de proveniência, agentes e atividades\n",
    "    doc_prov, dict_agents, dict_activities, dict_entities = initProvenance()\n",
    "\n",
    "\n",
    "    # Check initializationAssociatedWith(tracking_activ\n",
    "    print(\"Agents:\", dict_agents.keys())\n",
    "    print(\"Activities:\", dict_activities.keys())\n",
    "    print(\"Entities:\", dict_entities.keys())\n",
    "\n",
    "    # Executa a lógica de criação do dataset\n",
    "    create_eda_dataset()\n",
    "\n",
    "    # Serializa e gera saídas\n",
    "    gerar_prov_outputs(doc_prov)\n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd642769-e0a8-46d6-be83-39ea12671be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a17b0-533c-4b78-8f2f-f14902322878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d97b7-c25b-4d4c-b602-bb9049fccd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a1fac-9ef9-4fa7-9b3a-4fa0e3256199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56345402-fcaf-4913-9861-2dc9efcfbf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6c990-9abc-4fc4-a1ae-68b420c3b49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2ee99-f1a9-47cb-82ea-52aa76d2a37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec94a2-98e4-427e-929d-081f710d6677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f506e37-92db-4a88-ab2b-203f80583d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c143ab9-8012-4232-b168-77a432c3bcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92298a26-a92d-4000-9b52-34e1f1c7a3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf1381-e1c6-407c-896f-4232e02f8e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55abba-9f5a-45a9-997a-d31ee4b51883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda23c4-7eaf-426c-aaec-04eacd62a1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8cebd-3dda-47a4-bf8a-eee945a0e105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
